{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install pre-requisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a65285e868d7809"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets ragatouille ratelimit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:06.356680Z",
     "start_time": "2024-03-29T09:04:02.516158Z"
    }
   },
   "id": "3c7ba6dbea18e2ef",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:06.370484Z",
     "start_time": "2024-03-29T09:04:06.358365Z"
    }
   },
   "id": "cd3959f3217bfa17",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model preparations\n",
    "\n",
    "To go through the evaluation process, we need following models:\n",
    "\n",
    "1. Document model: Embedding model to generate document embeddings which will persisted in vector index. \n",
    "2. Reader model: A text completion model to answer the final question with augmented context.\n",
    "3. Evaluator model: A chat completion model that will give final verdict about RAG output. As this model will affect scoring considerably, stronger model should be used. \n",
    "\n",
    "As the choice of different models is not subject of this article and won't impact the comparison between RAG frameworks, we are determined to use completed local solution for this experiment for better speed and lower cost. \n",
    "\n",
    "To be more precise, following models that are already optimized in Ollama are used:\n",
    "\n",
    "* [Gemma 2B](https://huggingface.co/google/gemma-2b) as both `Document model` and `Reader model`.\n",
    "* [Mixtral-8x7B](https://ollama.com/library/mixtral) for `Evaluator model`\n",
    "   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2c4b6af65d58cdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "READER_MODEL_NAME = \"gemma:2b\"\n",
    "EMBEDDING_NAME = \"all-minilm\"\n",
    "\n",
    "\n",
    "# OLLAMA_BASE_URL = \"http://192.168.0.29:11434\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=EMBEDDING_NAME, base_url = OLLAMA_BASE_URL)\n",
    "READER_LLM = Ollama(model=READER_MODEL_NAME, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "EVALUATOR_NAME = \"mixtral:8x7b\"\n",
    "EVAL_MODEL = ChatOllama(model=EVALUATOR_NAME, base_url=OLLAMA_BASE_URL)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:06.944889Z",
     "start_time": "2024-03-29T09:04:06.371135Z"
    }
   },
   "id": "b4dfdc109f0daffc",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:07.563720Z",
     "start_time": "2024-03-29T09:04:06.945676Z"
    }
   },
   "id": "cdd427a5684551d2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:13.703571Z",
     "start_time": "2024-03-29T09:04:07.566157Z"
    }
   },
   "id": "194526c689413e8d",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2647 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a647346fe8cb4adda67a4a6cb27d6f0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:14.216133Z",
     "start_time": "2024-03-29T09:04:13.707272Z"
    }
   },
   "id": "43c0568919a65e8c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:14.230494Z",
     "start_time": "2024-03-29T09:04:14.216866Z"
    }
   },
   "id": "3a855d46ab06e6ec",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:14.244633Z",
     "start_time": "2024-03-29T09:04:14.231283Z"
    }
   },
   "id": "4e60ad11cd96e14c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:22.238919Z",
     "start_time": "2024-03-29T09:04:14.245260Z"
    }
   },
   "id": "921990aa1cb355f0",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2647 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b105ff08946f47c3bf7d85c31536066e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:22.329662Z",
     "start_time": "2024-03-29T09:04:22.239932Z"
    }
   },
   "id": "11ea5ea55c0da346",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=\"gpt-4\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        disallowed_special=[],\n",
    "        allowed_special=\"all\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:23.262647Z",
     "start_time": "2024-03-29T09:04:22.330920Z"
    }
   },
   "id": "40aa9cb71de4a46a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model: Embeddings\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs\n",
    "        )\n",
    "        print(f\"Index not found, generating it... {len(docs_processed)} docs in total\")\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:23.288408Z",
     "start_time": "2024-03-29T09:04:23.263369Z"
    }
   },
   "id": "13dc9ec746ddadd2",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:23.306956Z",
     "start_time": "2024-03-29T09:04:23.289169Z"
    }
   },
   "id": "851eb803b3f0a34f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:25.652398Z",
     "start_time": "2024-03-29T09:04:23.309745Z"
    }
   },
   "id": "283a72f828eb8ec3",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        \n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:25.688135Z",
     "start_time": "2024-03-29T09:04:25.653215Z"
    }
   },
   "id": "913cbbdae2722e2d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:25.724228Z",
     "start_time": "2024-03-29T09:04:25.688883Z"
    }
   },
   "id": "c193a75198dabe56",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ratelimit import limits,sleep_and_retry\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=6, period=60)\n",
    "def throttled_invoke(eval_chat_model, eval_prompt):\n",
    "    return eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    "    throttled:bool = True\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "        \n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        if throttled:\n",
    "            eval_result = throttled_invoke(eval_chat_model, eval_prompt)\n",
    "        else:\n",
    "            eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        \n",
    "        splits = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        if len(splits) != 2:\n",
    "            print(splits)\n",
    "            experiment[f\"eval_score_{evaluator_name}\"] = \"\"\n",
    "            experiment[f\"eval_feedback_{evaluator_name}\"] = \"\"\n",
    "            continue\n",
    "        feedback, score = splits \n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:04:25.755231Z",
     "start_time": "2024-03-29T09:04:25.725126Z"
    }
   },
   "id": "31856cff24422023",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_rerank:False_reader-model:gemma:2b:\n",
      "Loading knowledge base embeddings...\n",
      "Index not found, generating it... 33666 docs in total\n",
      "Running RAG...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/67 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdc47da279cb4d939820a88935f9b837"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/67 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59947403de604a3aa62ab58729e01f98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Is the response correct, accurate, and factual based on the reference answer?]\\nScore 4: The response is mostly correct, accurate, and factual. The response states that the logs can be accessed in the \"Logs\" tab of your Endpoint in Hugging Face Endpoints. The reference answer also mentions the same location. However, the reference answer is more specific, mentioning the UI as well.']\n",
      "['[Is the response correct, accurate, and factual based on the reference answer?] The response is completely correct, accurate, and factual.']\n",
      "['Feedback: [Score 5] The response is completely correct, accurate, and factual based on the reference answer.']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "\n",
    "chunk_size = 200\n",
    "rerank = False\n",
    "\n",
    "settings_name = f\"chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "print(\"Loading knowledge base embeddings...\")\n",
    "knowledge_index = load_embeddings(\n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    chunk_size=chunk_size,\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    ")\n",
    "\n",
    "print(\"Running RAG...\")\n",
    "reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "run_rag_tests(\n",
    "    eval_dataset=eval_dataset,\n",
    "    llm=READER_LLM,\n",
    "    knowledge_index=knowledge_index,\n",
    "    output_file=output_file_name,\n",
    "    reranker=reranker,\n",
    "    verbose=False,\n",
    "    test_settings=settings_name,\n",
    ")\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "evaluate_answers(\n",
    "    output_file_name,\n",
    "    EVAL_MODEL,\n",
    "    EVAL_MODEL.name,\n",
    "    evaluation_prompt_template,\n",
    "    True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:30:36.811359Z",
     "start_time": "2024-03-29T09:04:25.756155Z"
    }
   },
   "id": "83e219af8e9a45c9",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:30:37.101641Z",
     "start_time": "2024-03-29T09:30:36.812997Z"
    }
   },
   "id": "b157cef7583830b2",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                               question  \\\n0                                           What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n   \n1                                                                    What is the purpose of the BLIP-Diffusion model?\\n   \n2                                                 How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n3                                        What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n   \n4                                    What is the default context window size for Local Attention in the LongT5 model?\\n   \n..                                                                                                                  ...   \n62  What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n   \n63               What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\\n   \n64          What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n   \n65                            What parameter is used to ensure that elements in a row have the same height in Gradio?\\n   \n66                                What is the command to install the latest version of Optimum with OpenVINO support?\\n   \n\n                                                                                                                                                         true_answer  \\\n0                                                                                                                                          x86_64-unknown-linux-musl   \n1                                                                        The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.   \n2   By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n3                                                                                                                                          Ensure the app is running   \n4                                                                                                                                                         127 tokens   \n..                                                                                                                                                               ...   \n62                                                                                                                                                              10GB   \n63                                                                          To track their machine learning experiments at every stage, from training to production.   \n64                                                                                                                                                           Optimum   \n65                                                                                                                                                      equal_height   \n66                                                                                                          pip install --upgrade-strategy eager optimum[\"openvino\"]   \n\n                                                                                            source_doc  \\\n0                          huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md   \n1                       huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md   \n2                                               huggingface/hub-docs/blob/main/docs/hub/paper-pages.md   \n3                                         huggingface/datasets-server/blob/main/services/api/README.md   \n4                                huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md   \n..                                                                                                 ...   \n62                                     huggingface/transformers/blob/main/docs/source/en/big_models.md   \n63  gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md   \n64                                                                 huggingface/blog/blob/main/intel.md   \n65              gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md   \n66                            huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md   \n\n                                                                                                                                                                                                                                       generated_answer  \\\n0   The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\\n\\nThe context provides information about the different versions and architectures of the `tokenizers` binary, specifically focusing on the Linux operating system.   \n1                                                                                                        The context does not provide any information about the purpose of the BLIP-Diffusion model, so I cannot answer this question from the context.   \n2                                                                                                                  The user can claim authorship of a paper on the Hugging Face Hub by clicking on the \"claim authorship\" link in their paper settings.   \n3                                                                             The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running smoothly and to provide information about the cache and queue.   \n4                                                                               The context window size for Local Attention in the LongT5 model is not explicitly mentioned in the context, so I cannot answer this question from the provided context.   \n..                                                                                                                                                                                                                                                  ...   \n62                                     The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is not explicitly specified in the context, so I cannot answer this question from the provided context.   \n63                                                                The passage does not mention the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists, so I cannot answer this question from the provided context.   \n64           The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Transformers**.\\n\\nThe context does not specify the name of the library, so I cannot answer this question from the provided context.   \n65                                                                 The parameter used to ensure that elements in a row have the same height in Gradio is the `scale` parameter. It allows you to specify the relative width of each element in the row.   \n66                                                                   According to the context, the command to install the latest version of Optimum with OpenVINO support is:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              retrieved_docs  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\">\\n    <br>\\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/>\\n    <br>\\n<p>\\n<p align=\"center\">\\n    <img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\">\\n    <a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://docs.rs/tokenizers/\">\\n        <img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\">\\n    </a>\\n</p>\\n<br>\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`, `tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`]   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, ## How does Stable Diffusion work?\\n\\nHaving seen the high-quality images that stable diffusion can produce, let's try to understand \\na bit better how the model functions.\\n\\nStable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\\n\\nGenerally speaking, diffusion models are machine learning systems that are trained to *denoise* random Gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)., --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., We've gone from the basic use of Stable Diffusion using  Hugging Face Diffusers to more advanced uses of the library, and we tried to introduce all the pieces in a modern diffusion system. If you liked this topic and want to learn more, we recommend the following resources:\\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\\n- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, that gives a broader overview on Diffusion systems.\\n- The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post., # Stable Diffusion 2\\n\\nStable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).]   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\\n</div>\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., The Hub will attempt to automatically match paper to users based on their email. \\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\\n</div>\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n> Looking for a good first issue to work on?\\n> Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n> Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, ### Would you like to integrate your library to the Hub?\\n\\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\\n\\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) .\\n\\nThank you fastlearners and hugging learners ., ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hubs Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Faces models on the Hugging Face Hub have an associated model card on the Hub[^8]., --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\")]   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [Datasets server API - rows endpoint\\n\\n> /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /search: get a slice of a search result over a dataset split\\n- /filter: filter rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined., The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&config={config}`\\n  - `split`: `?dataset={dataset}&config={config}&split={split}`, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset]   \n4                                                                                                                                               [## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., ## Local attention\\n\\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., -->\\n\\n# LongT5\\n\\n## Overview\\n\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention.\\n\\n\\nThe abstract from the paper is the following:, However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]]]   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n62                                                                 [## Sharded checkpoints\\n\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\\n\\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\\n\\n```py\\nfrom transformers import AutoModel\\n\\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\\n```\\n\\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:, ```py\\n>>> index[\"weight_map\"]\\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\\n ...\\n```\\n\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\\n\\n```py\\n>>> from transformers.modeling_utils import load_sharded_checkpoint\\n\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     load_sharded_checkpoint(model, tmp_dir)\\n```\\n\\n## Low memory loading, <Tip>\\n\\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \\n\\n</Tip>\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\\n\\n## Sharded checkpoints, ## Low memory loading\\n\\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\\n\\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading), ## Checkpoint Naming\\n\\n* CodeGen model [checkpoints](https://huggingface.co/models?other=codegen) are available on different pre-training data with variable sizes.\\n* The format is: `Salesforce/codegen-{size}-{data}`, where\\n  * `size`: `350M`, `2B`, `6B`, `16B`\\n  * `data`: \\n    * `nl`: Pre-trained on the Pile\\n    * `multi`: Initialized with `nl`, then further pre-trained on multiple programming languages data\\n    * `mono`: Initialized with `multi`, then further pre-trained on Python data\\n* For example, `Salesforce/codegen-350M-mono` offers a 350 million-parameter checkpoint pre-trained sequentially on the Pile, multiple programming languages, and Python.\\n\\n## Usage example\\n\\n```python\\n>>> from transformers import AutoModelForCausalLM, AutoTokenizer, performing masked image modeling. You can find it [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).\\n- As the BEiT models expect each image to be of the same size (resolution), one can use\\n  [`BeitImageProcessor`] to resize (or rescale) and normalize images for the model.\\n- Both the patch resolution and image resolution used during pre-training or fine-tuning are reflected in the name of\\n  each checkpoint. For example, `microsoft/beit-base-patch16-224` refers to a base-sized architecture with patch\\n  resolution of 16x16 and fine-tuning resolution of 224x224. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit)., In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to  Transformers.\\n\\nThe following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n\\n1.  [ ] (Optional) Understood theoretical aspects\\n\\n2.  [ ] Prepared transformers dev environment\\n\\n3.  [ ] Set up debugging environment of the original repository\\n\\n4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers]   \n63  [In order to better understand and address these risks, ML researchers and developers have started studying _machine bias_ or _algorithmic bias_, mechanisms that might lead systems to, for example, encode **negative stereotypes or associations** or to have **disparate performance** for different population groups in their deployment context., _This blog post from the [Ethics and Society regulars @](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our communitys efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If youve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\\nsection!_, In the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.\\n\\n\\n## Addressing Bias throughout the ML Development Cycle\\n\\nReady for some practical advice yet? Here we go \\n\\nThere is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc), the modality (text, tabular data, images, etc), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Lets have a look at how bias handling may differ across these various stages., ## Putting Bias in Context\\n\\nThe first and maybe most important concept to consider when dealing with machine bias is **context**. In their foundational work on [bias in NLP](https://aclanthology.org/2020.acl-main.485.pdf), Su Lin Blodgett et al. point out that: _[T]he majority of [academic works on machine bias] fail to engage critically with what constitutes bias in the first place_, including by building their work on top of _unstated assumptions about what kinds of system behaviors are harmful, in what ways, to whom, and why_., #### Task definition: recommendations\\n\\nThere are as many ways for the ML task definition and deployment to affect the risk of bias-related harms as there are applications for ML systems. As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\\n\\n\\n\\n* Investigate:\\n    * Reports of bias in the field pre-ML\\n    * At-risk demographic categories for your specific use case\\n* Examine:\\n    * The impact of your optimization objective on reinforcing biases\\n    * Alternative objectives that favor diversity and positive long-term impacts\\n\\n\\n### I am <span style=\"text-decoration:underline;\">curating/picking a dataset</span> for my ML system, how can I address bias?, So, whos on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system., Conceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:\\n\\n\\n\\n1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and\\n2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).]   \n64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [Last July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \\n, 1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, ukasz Kaiser, Anselm Levskaya., The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem  [ Transformers](https://github.com/huggingface/transformers), [ Datasets](https://github.com/huggingface/datasets), [ Tokenizers](https://github.com/huggingface/tokenizers), and [ Accelerate](https://github.com/huggingface/accelerate)  as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\\n\\n##  Languages and translations, This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\\n\\n## Usage tips, Also if you want to learn more about accelerating transformers you should also check out Hugging Face [optimum](https://github.com/huggingface/optimum). \\n\\n---\\n\\nThanks for reading! If you have any questions, feel free to contact me, through[Github](https://github.com/huggingface/transformers), or on the[forum](https://discuss.huggingface.co/c/sagemaker/17). You can also connect with me on[Twitter](https://twitter.com/_philschmid)or[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/)., --\\ntitle: \"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\"\\nthumbnail: /blog/assets/144_hf_hub_glam_guide/thumbnail.png\\nauthors:\\n- user: davanstrien\\n---\\n\\n## The Hugging Face Hub for Galleries, Libraries, Archives and Museums \\n\\n\\n\\n### What is the Hugging Face Hub?\\n\\nHugging Face aims to make high-quality machine learning accessible to everyone. This goal is pursued in various ways, including developing open-source code libraries such as the widely-used Transformers library, offering [free courses](https://huggingface.co/learn), and providing the Hugging Face Hub., --\\ntitle: \"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\"\\nthumbnail: /blog/assets/77_graphcore-update/graphcore_update.png\\nauthors:\\n- user: sallydoherty\\n  guest: true\\n---\\n\\n# Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\\n\\n\\n[Graphcore](https://huggingface.co/hardware/graphcore/) and Hugging Face have significantly expanded the range of Machine Learning modalities and tasks available in [Hugging Face Optimum](https://github.com/huggingface/optimum), an open-source library for Transformers performance optimization. Developers now have convenient access to a wide range of off-the-shelf Hugging Face Transformer models, optimised to deliver the best possible performance on Graphcores IPU.]   \n65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [- Updated docs to reflect that the `equal_height` parameter should be passed to the `.style()` method of `gr.Row()` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3125](https://github.com/gradio-app/gradio/pull/3125), $code_rows_and_columns\\n$demo_rows_and_columns\\n\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\\n\\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\\n\\n# Dimensions\\n\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\\n\\nBelow is an example illustrating the use of viewport width (vw):\\n\\n```python\\nimport gradio as gr, - Remove footers and min-height the correct way by [@aliabd](https://github.com/aliabd) in [PR 2860](https://github.com/gradio-app/gradio/pull/2860), - [#5188](https://github.com/gradio-app/gradio/pull/5188) [`b22e1888`](https://github.com/gradio-app/gradio/commit/b22e1888fcf0843520525c1e4b7e1fe73fdeb948) - Fix the images in the theme builder to use permanent URI.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n- [#5221](https://github.com/gradio-app/gradio/pull/5221) [`f344592a`](https://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7) - Allows setting a height to `gr.File` and improves the UI of the component.  Thanks [@abidlabs](https://github.com/abidlabs)!, ### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces](https://pypi.org/project/spaces/) library in order to make them work with specific hardware. It will have no effect on standalone Gradio apps or regular Gradio Spaces and can be globally deactivated as follows : `import spaces; spaces.disable_gradio_auto_wrap()` by [@cbensimon](https://github.com/cbensimon) in [PR 4389](https://github.com/gradio-app/gradio/pull/4389).\\n- Deprecated `.style` parameter and moved arguments to constructor. Added support for `.update()` to all arguments initially in style. Added `scale` and `min_width` support to every Component. By [@aliabid94](https://github.com/aliabid94) in [PR 4374](https://github.com/gradio-app/gradio/pull/4374)\\n\\n### Breaking Changes:, - [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!\\n- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!, ```python\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        btn0 = gr.Button(\"Button 0\", scale=0)\\n        btn1 = gr.Button(\"Button 1\", scale=1)\\n        btn2 = gr.Button(\"Button 2\", scale=2)\\n```\\n\\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\\n\\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\\n\\n## Columns and Nesting\\n\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n\\n$code_rows_and_columns\\n$demo_rows_and_columns]   \n66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [--\\ntitle: \"Accelerate your models with  Optimum Intel and OpenVINO\"\\nthumbnail: /blog/assets/113_openvino/thumbnail.png\\nauthors:\\n- user: echarlaix\\n- user: juliensimon\\n---\\n\\n# Accelerate your models with  Optimum Intel and OpenVINO\\n\\n\\n![image](assets/113_openvino/thumbnail.png), # OpenVINO\\n\\n [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\\n\\nYou'll need to install  Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\n\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\\n\\n## Stable Diffusion, First, let's create a virtual environment and install all dependencies.\\n\\n```bash\\nvirtualenv openvino\\nsource openvino/bin/activate\\npip install pip --upgrade\\npip install optimum[openvino,nncf] torchvision evaluate\\n```, ## Now it's your turn\\n\\nAs you can see, it's pretty easy to accelerate your models with  Optimum Intel and OpenVINO. If you'd like to get started, please visit the [Optimum Intel](https://github.com/huggingface/optimum-intel) repository, and don't forget to give it a star . You'll also find additional examples [there](https://huggingface.co/docs/optimum/intel/optimization_ov). If you'd like to dive deeper into OpenVINO, the Intel [documentation](https://docs.openvino.ai/latest/index.html) has you covered., ### OpenVINO\\n\\nBefore you begin, make sure you have all the necessary libraries installed :\\n\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```\\n\\nIt is possible to export  Transformers and Diffusers models to the OpenVINO format easily:\\n\\n```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\\n```\\n\\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov)., | [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|, ## Optimum Intel\\n\\n### OpenVINO]   \n\n                                   test_settings eval_score_None  \\\n0   chunk:200_rerank:False_reader-model:gemma:2b               3   \n1   chunk:200_rerank:False_reader-model:gemma:2b               1   \n2   chunk:200_rerank:False_reader-model:gemma:2b               3   \n3   chunk:200_rerank:False_reader-model:gemma:2b               4   \n4   chunk:200_rerank:False_reader-model:gemma:2b               1   \n..                                           ...             ...   \n62  chunk:200_rerank:False_reader-model:gemma:2b               5   \n63  chunk:200_rerank:False_reader-model:gemma:2b               4   \n64  chunk:200_rerank:False_reader-model:gemma:2b               1   \n65  chunk:200_rerank:False_reader-model:gemma:2b               1   \n66  chunk:200_rerank:False_reader-model:gemma:2b             NaN   \n\n                                                                                                                                                                                                                                                                                                                 eval_feedback_None  \\\n0                                                                                                                                                                                                                                                                Feedback: The response is somewhat correct, accurate, and factual.   \n1                                                                                                     Feedback: The response is not correct, accurate, and factual based on the reference answer. The reference answer clearly states the purpose of the BLIP-Diffusion model, while the response does not provide any information.   \n2   Feedback: The response is somewhat correct, accurate, and factual. It mentions that the user can claim authorship by clicking on the \"claim authorship\" link in their paper settings, but it does not mention the additional step of clicking their name on the corresponding Paper page and the admin team validation process.   \n3                                                                                                                                                                                                                                                                  Feedback: The response is mostly correct, accurate, and factual.   \n4                                                             Feedback: The response is completely incorrect, inaccurate, and not factual based on the reference answer. The correct answer is 127 tokens, but the response does not provide any information about the context window size for Local Attention in the LongT5 model.   \n..                                                                                                                                                                                                                                                                                                                              ...   \n62                                                                                                                                                                                                                                                             Feedback: The response is completely correct, accurate, and factual.   \n63                                                                                                                                                                                                                                                                 Feedback: The response is mostly correct, accurate, and factual.   \n64                                                                                                                                                                                                                                      Feedback: The response is not correct, accurate, and factual based on the reference answer.   \n65                                                                                                                                                                       Feedback: The response is completely incorrect, inaccurate, and/or not factual. The reference answer is `equal_height`, but the response given is `scale`.   \n66                                                                                                                                                                                                                                                                                                                              NaN   \n\n                                                          settings  \n0   ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n1   ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n2   ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n3   ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n4   ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n..                                                             ...  \n62  ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n63  ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n64  ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n65  ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n66  ./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json  \n\n[67 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>true_answer</th>\n      <th>source_doc</th>\n      <th>generated_answer</th>\n      <th>retrieved_docs</th>\n      <th>test_settings</th>\n      <th>eval_score_None</th>\n      <th>eval_feedback_None</th>\n      <th>settings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n</td>\n      <td>x86_64-unknown-linux-musl</td>\n      <td>huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md</td>\n      <td>The `tokenizers-linux-x64-musl` binary is designed for the x86_64 architecture.\\n\\nThe context provides information about the different versions and architectures of the `tokenizers` binary, specifically focusing on the Linux operating system.</td>\n      <td>[`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\"&gt;\\n    &lt;br&gt;\\n    &lt;img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/&gt;\\n    &lt;br&gt;\\n&lt;p&gt;\\n&lt;p align=\"center\"&gt;\\n    &lt;img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\"&gt;\\n    &lt;a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\"&gt;\\n        &lt;img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\"&gt;\\n    &lt;/a&gt;\\n    &lt;a href=\"https://docs.rs/tokenizers/\"&gt;\\n        &lt;img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\"&gt;\\n    &lt;/a&gt;\\n&lt;/p&gt;\\n&lt;br&gt;\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`, `tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>3</td>\n      <td>Feedback: The response is somewhat correct, accurate, and factual.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the purpose of the BLIP-Diffusion model?\\n</td>\n      <td>The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.</td>\n      <td>huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md</td>\n      <td>The context does not provide any information about the purpose of the BLIP-Diffusion model, so I cannot answer this question from the context.</td>\n      <td>[Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, ## How does Stable Diffusion work?\\n\\nHaving seen the high-quality images that stable diffusion can produce, let's try to understand \\na bit better how the model functions.\\n\\nStable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\\n\\nGenerally speaking, diffusion models are machine learning systems that are trained to *denoise* random Gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)., --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., We've gone from the basic use of Stable Diffusion using  Hugging Face Diffusers to more advanced uses of the library, and we tried to introduce all the pieces in a modern diffusion system. If you liked this topic and want to learn more, we recommend the following resources:\\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\\n- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, that gives a broader overview on Diffusion systems.\\n- The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post., # Stable Diffusion 2\\n\\nStable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>1</td>\n      <td>Feedback: The response is not correct, accurate, and factual based on the reference answer. The reference answer clearly states the purpose of the BLIP-Diffusion model, while the response does not provide any information.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n      <td>huggingface/hub-docs/blob/main/docs/hub/paper-pages.md</td>\n      <td>The user can claim authorship of a paper on the Hugging Face Hub by clicking on the \"claim authorship\" link in their paper settings.</td>\n      <td>[* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., The Hub will attempt to automatically match paper to users based on their email. \\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n&gt; Looking for a good first issue to work on?\\n&gt; Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n&gt; Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, ### Would you like to integrate your library to the Hub?\\n\\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\\n\\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) .\\n\\nThank you fastlearners and hugging learners ., ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hubs Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Faces models on the Hugging Face Hub have an associated model card on the Hub[^8]., --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\")]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>3</td>\n      <td>Feedback: The response is somewhat correct, accurate, and factual. It mentions that the user can claim authorship by clicking on the \"claim authorship\" link in their paper settings, but it does not mention the additional step of clicking their name on the corresponding Paper page and the admin team validation process.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n</td>\n      <td>Ensure the app is running</td>\n      <td>huggingface/datasets-server/blob/main/services/api/README.md</td>\n      <td>The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the app is running smoothly and to provide information about the cache and queue.</td>\n      <td>[Datasets server API - rows endpoint\\n\\n&gt; /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /search: get a slice of a search result over a dataset split\\n- /filter: filter rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined., The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&amp;config={config}`\\n  - `split`: `?dataset={dataset}&amp;config={config}&amp;split={split}`, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>4</td>\n      <td>Feedback: The response is mostly correct, accurate, and factual.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the default context window size for Local Attention in the LongT5 model?\\n</td>\n      <td>127 tokens</td>\n      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md</td>\n      <td>The context window size for Local Attention in the LongT5 model is not explicitly mentioned in the context, so I cannot answer this question from the provided context.</td>\n      <td>[## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., ## Local attention\\n\\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., --&gt;\\n\\n# LongT5\\n\\n## Overview\\n\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention.\\n\\n\\nThe abstract from the paper is the following:, However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]]]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>1</td>\n      <td>Feedback: The response is completely incorrect, inaccurate, and not factual based on the reference answer. The correct answer is 127 tokens, but the response does not provide any information about the context window size for Local Attention in the LongT5 model.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n</td>\n      <td>10GB</td>\n      <td>huggingface/transformers/blob/main/docs/source/en/big_models.md</td>\n      <td>The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is not explicitly specified in the context, so I cannot answer this question from the provided context.</td>\n      <td>[## Sharded checkpoints\\n\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size &lt; 10GB) and an index that maps parameter names to the files they are stored in.\\n\\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\\n\\n```py\\nfrom transformers import AutoModel\\n\\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\\n```\\n\\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:, ```py\\n&gt;&gt;&gt; index[\"weight_map\"]\\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\\n ...\\n```\\n\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\\n\\n```py\\n&gt;&gt;&gt; from transformers.modeling_utils import load_sharded_checkpoint\\n\\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     load_sharded_checkpoint(model, tmp_dir)\\n```\\n\\n## Low memory loading, &lt;Tip&gt;\\n\\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \\n\\n&lt;/Tip&gt;\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\\n\\n## Sharded checkpoints, ## Low memory loading\\n\\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\\n\\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading), ## Checkpoint Naming\\n\\n* CodeGen model [checkpoints](https://huggingface.co/models?other=codegen) are available on different pre-training data with variable sizes.\\n* The format is: `Salesforce/codegen-{size}-{data}`, where\\n  * `size`: `350M`, `2B`, `6B`, `16B`\\n  * `data`: \\n    * `nl`: Pre-trained on the Pile\\n    * `multi`: Initialized with `nl`, then further pre-trained on multiple programming languages data\\n    * `mono`: Initialized with `multi`, then further pre-trained on Python data\\n* For example, `Salesforce/codegen-350M-mono` offers a 350 million-parameter checkpoint pre-trained sequentially on the Pile, multiple programming languages, and Python.\\n\\n## Usage example\\n\\n```python\\n&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer, performing masked image modeling. You can find it [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).\\n- As the BEiT models expect each image to be of the same size (resolution), one can use\\n  [`BeitImageProcessor`] to resize (or rescale) and normalize images for the model.\\n- Both the patch resolution and image resolution used during pre-training or fine-tuning are reflected in the name of\\n  each checkpoint. For example, `microsoft/beit-base-patch16-224` refers to a base-sized architecture with patch\\n  resolution of 16x16 and fine-tuning resolution of 224x224. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit)., In the following, we try to give you a general recipe that we found most\\nuseful when porting a model to  Transformers.\\n\\nThe following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n\\n1.  [ ] (Optional) Understood theoretical aspects\\n\\n2.  [ ] Prepared transformers dev environment\\n\\n3.  [ ] Set up debugging environment of the original repository\\n\\n4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>5</td>\n      <td>Feedback: The response is completely correct, accurate, and factual.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>What is the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists?\\n</td>\n      <td>To track their machine learning experiments at every stage, from training to production.</td>\n      <td>gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md</td>\n      <td>The passage does not mention the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists, so I cannot answer this question from the provided context.</td>\n      <td>[In order to better understand and address these risks, ML researchers and developers have started studying _machine bias_ or _algorithmic bias_, mechanisms that might lead systems to, for example, encode **negative stereotypes or associations** or to have **disparate performance** for different population groups in their deployment context., _This blog post from the [Ethics and Society regulars @](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our communitys efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If youve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\\nsection!_, In the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.\\n\\n\\n## Addressing Bias throughout the ML Development Cycle\\n\\nReady for some practical advice yet? Here we go \\n\\nThere is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc), the modality (text, tabular data, images, etc), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Lets have a look at how bias handling may differ across these various stages., ## Putting Bias in Context\\n\\nThe first and maybe most important concept to consider when dealing with machine bias is **context**. In their foundational work on [bias in NLP](https://aclanthology.org/2020.acl-main.485.pdf), Su Lin Blodgett et al. point out that: _[T]he majority of [academic works on machine bias] fail to engage critically with what constitutes bias in the first place_, including by building their work on top of _unstated assumptions about what kinds of system behaviors are harmful, in what ways, to whom, and why_., #### Task definition: recommendations\\n\\nThere are as many ways for the ML task definition and deployment to affect the risk of bias-related harms as there are applications for ML systems. As in the examples above, some common steps that may help decide whether and how to apply ML in a way that minimizes bias-related risk include:\\n\\n\\n\\n* Investigate:\\n    * Reports of bias in the field pre-ML\\n    * At-risk demographic categories for your specific use case\\n* Examine:\\n    * The impact of your optimization objective on reinforcing biases\\n    * Alternative objectives that favor diversity and positive long-term impacts\\n\\n\\n### I am &lt;span style=\"text-decoration:underline;\"&gt;curating/picking a dataset&lt;/span&gt; for my ML system, how can I address bias?, So, whos on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system., Conceptualizing bias as a risk factor then allows us to better understand the **shared responsibility** for machine biases between developers at all stages. Bias can never be fully removed, not least because the definitions of social biases and the power dynamics that tie them to discrimination vary vastly across social contexts. However:\\n\\n\\n\\n1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and\\n2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>4</td>\n      <td>Feedback: The response is mostly correct, accurate, and factual.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n</td>\n      <td>Optimum</td>\n      <td>huggingface/blog/blob/main/intel.md</td>\n      <td>The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Transformers**.\\n\\nThe context does not specify the name of the library, so I cannot answer this question from the provided context.</td>\n      <td>[Last July, we [announced](https://huggingface.co/blog/intel) that Intel and Hugging Face would collaborate on building state-of-the-art yet simple hardware acceleration tools for Transformer models. \\n, 1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, ukasz Kaiser, Anselm Levskaya., The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem  [ Transformers](https://github.com/huggingface/transformers), [ Datasets](https://github.com/huggingface/datasets), [ Tokenizers](https://github.com/huggingface/tokenizers), and [ Accelerate](https://github.com/huggingface/accelerate)  as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\\n\\n##  Languages and translations, This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/kimiyoung/transformer-xl).\\n\\n## Usage tips, Also if you want to learn more about accelerating transformers you should also check out Hugging Face [optimum](https://github.com/huggingface/optimum). \\n\\n---\\n\\nThanks for reading! If you have any questions, feel free to contact me, through[Github](https://github.com/huggingface/transformers), or on the[forum](https://discuss.huggingface.co/c/sagemaker/17). You can also connect with me on[Twitter](https://twitter.com/_philschmid)or[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/)., --\\ntitle: \"The Hugging Face Hub for Galleries, Libraries, Archives and Museums\"\\nthumbnail: /blog/assets/144_hf_hub_glam_guide/thumbnail.png\\nauthors:\\n- user: davanstrien\\n---\\n\\n## The Hugging Face Hub for Galleries, Libraries, Archives and Museums \\n\\n\\n\\n### What is the Hugging Face Hub?\\n\\nHugging Face aims to make high-quality machine learning accessible to everyone. This goal is pursued in various ways, including developing open-source code libraries such as the widely-used Transformers library, offering [free courses](https://huggingface.co/learn), and providing the Hugging Face Hub., --\\ntitle: \"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\"\\nthumbnail: /blog/assets/77_graphcore-update/graphcore_update.png\\nauthors:\\n- user: sallydoherty\\n  guest: true\\n---\\n\\n# Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\\n\\n\\n[Graphcore](https://huggingface.co/hardware/graphcore/) and Hugging Face have significantly expanded the range of Machine Learning modalities and tasks available in [Hugging Face Optimum](https://github.com/huggingface/optimum), an open-source library for Transformers performance optimization. Developers now have convenient access to a wide range of off-the-shelf Hugging Face Transformer models, optimised to deliver the best possible performance on Graphcores IPU.]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>1</td>\n      <td>Feedback: The response is not correct, accurate, and factual based on the reference answer.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>What parameter is used to ensure that elements in a row have the same height in Gradio?\\n</td>\n      <td>equal_height</td>\n      <td>gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md</td>\n      <td>The parameter used to ensure that elements in a row have the same height in Gradio is the `scale` parameter. It allows you to specify the relative width of each element in the row.</td>\n      <td>[- Updated docs to reflect that the `equal_height` parameter should be passed to the `.style()` method of `gr.Row()` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3125](https://github.com/gradio-app/gradio/pull/3125), $code_rows_and_columns\\n$demo_rows_and_columns\\n\\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\\n\\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\\n\\n# Dimensions\\n\\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\\n\\nBelow is an example illustrating the use of viewport width (vw):\\n\\n```python\\nimport gradio as gr, - Remove footers and min-height the correct way by [@aliabd](https://github.com/aliabd) in [PR 2860](https://github.com/gradio-app/gradio/pull/2860), - [#5188](https://github.com/gradio-app/gradio/pull/5188) [`b22e1888`](https://github.com/gradio-app/gradio/commit/b22e1888fcf0843520525c1e4b7e1fe73fdeb948) - Fix the images in the theme builder to use permanent URI.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n- [#5221](https://github.com/gradio-app/gradio/pull/5221) [`f344592a`](https://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7) - Allows setting a height to `gr.File` and improves the UI of the component.  Thanks [@abidlabs](https://github.com/abidlabs)!, ### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces](https://pypi.org/project/spaces/) library in order to make them work with specific hardware. It will have no effect on standalone Gradio apps or regular Gradio Spaces and can be globally deactivated as follows : `import spaces; spaces.disable_gradio_auto_wrap()` by [@cbensimon](https://github.com/cbensimon) in [PR 4389](https://github.com/gradio-app/gradio/pull/4389).\\n- Deprecated `.style` parameter and moved arguments to constructor. Added support for `.update()` to all arguments initially in style. Added `scale` and `min_width` support to every Component. By [@aliabid94](https://github.com/aliabid94) in [PR 4374](https://github.com/gradio-app/gradio/pull/4374)\\n\\n### Breaking Changes:, - [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!\\n- [#5283](https://github.com/gradio-app/gradio/pull/5283) [`a7460557`](https://github.com/gradio-app/gradio/commit/a74605572dd0d6bb41df6b38b120d656370dd67d) - Add height parameter and scrolling to `gr.Dataframe`. Thanks [@abidlabs](https://github.com/abidlabs)!, ```python\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        btn0 = gr.Button(\"Button 0\", scale=0)\\n        btn1 = gr.Button(\"Button 1\", scale=1)\\n        btn2 = gr.Button(\"Button 2\", scale=2)\\n```\\n\\n- `min_width` will set the minimum width the element will take. The Row will wrap if there isn't sufficient space to satisfy all `min_width` values.\\n\\nLearn more about Rows in the [docs](https://gradio.app/docs/#row).\\n\\n## Columns and Nesting\\n\\nComponents within a Column will be placed vertically atop each other. Since the vertical layout is the default layout for Blocks apps anyway, to be useful, Columns are usually nested within Rows. For example:\\n\\n$code_rows_and_columns\\n$demo_rows_and_columns]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>1</td>\n      <td>Feedback: The response is completely incorrect, inaccurate, and/or not factual. The reference answer is `equal_height`, but the response given is `scale`.</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>What is the command to install the latest version of Optimum with OpenVINO support?\\n</td>\n      <td>pip install --upgrade-strategy eager optimum[\"openvino\"]</td>\n      <td>huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md</td>\n      <td>According to the context, the command to install the latest version of Optimum with OpenVINO support is:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```</td>\n      <td>[--\\ntitle: \"Accelerate your models with  Optimum Intel and OpenVINO\"\\nthumbnail: /blog/assets/113_openvino/thumbnail.png\\nauthors:\\n- user: echarlaix\\n- user: juliensimon\\n---\\n\\n# Accelerate your models with  Optimum Intel and OpenVINO\\n\\n\\n![image](assets/113_openvino/thumbnail.png), # OpenVINO\\n\\n [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\\n\\nYou'll need to install  Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\n\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\\n\\n## Stable Diffusion, First, let's create a virtual environment and install all dependencies.\\n\\n```bash\\nvirtualenv openvino\\nsource openvino/bin/activate\\npip install pip --upgrade\\npip install optimum[openvino,nncf] torchvision evaluate\\n```, ## Now it's your turn\\n\\nAs you can see, it's pretty easy to accelerate your models with  Optimum Intel and OpenVINO. If you'd like to get started, please visit the [Optimum Intel](https://github.com/huggingface/optimum-intel) repository, and don't forget to give it a star . You'll also find additional examples [there](https://huggingface.co/docs/optimum/intel/optimization_ov). If you'd like to dive deeper into OpenVINO, the Intel [documentation](https://docs.openvino.ai/latest/index.html) has you covered., ### OpenVINO\\n\\nBefore you begin, make sure you have all the necessary libraries installed :\\n\\n```bash\\npip install --upgrade-strategy eager optimum[openvino,nncf]\\n```\\n\\nIt is possible to export  Transformers and Diffusers models to the OpenVINO format easily:\\n\\n```bash\\noptimum-cli export openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov\\n```\\n\\nIf you add `--int8`, the weights will be quantized to INT8. Static quantization can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf), more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov)., | [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|, ## Optimum Intel\\n\\n### OpenVINO]</td>\n      <td>chunk:200_rerank:False_reader-model:gemma:2b</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>./output/rag_chunk:200_rerank:False_reader-model:gemma:2b.json</td>\n    </tr>\n  </tbody>\n</table>\n<p>67 rows  9 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:30:37.140366Z",
     "start_time": "2024-03-29T09:30:37.102587Z"
    }
   },
   "id": "2660a544e59e81d",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mint\u001B[39m(x) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      2\u001B[0m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[1;32m      3\u001B[0m average_scores \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msettings\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmean()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/pandas/core/series.py:4915\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4780\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4781\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4782\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4787\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4788\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4790\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4791\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4906\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4907\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4908\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SeriesApply(\n\u001B[1;32m   4909\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4910\u001B[0m         func,\n\u001B[1;32m   4911\u001B[0m         convert_dtype\u001B[38;5;241m=\u001B[39mconvert_dtype,\n\u001B[1;32m   4912\u001B[0m         by_row\u001B[38;5;241m=\u001B[39mby_row,\n\u001B[1;32m   4913\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   4914\u001B[0m         kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[0;32m-> 4915\u001B[0m     )\u001B[38;5;241m.\u001B[39mapply()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_standard()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_map_values(\n\u001B[1;32m   1508\u001B[0m     mapper\u001B[38;5;241m=\u001B[39mcurried, na_action\u001B[38;5;241m=\u001B[39maction, convert\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_dtype\n\u001B[1;32m   1509\u001B[0m )\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m algorithms\u001B[38;5;241m.\u001B[39mmap_array(arr, mapper, na_action\u001B[38;5;241m=\u001B[39mna_action, convert\u001B[38;5;241m=\u001B[39mconvert)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer(values, mapper, convert\u001B[38;5;241m=\u001B[39mconvert)\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mint\u001B[39m(x) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      2\u001B[0m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m (result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[1;32m      3\u001B[0m average_scores \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msettings\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_None\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmean()\n",
      "\u001B[0;31mValueError\u001B[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "result[\"eval_score_None\"] = result[\"eval_score_None\"].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "result[\"eval_score_None\"] = (result[\"eval_score_None\"] - 1) / 4\n",
    "average_scores = result.groupby(\"settings\")[\"eval_score_None\"].mean()\n",
    "average_scores.sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:30:37.743698Z",
     "start_time": "2024-03-29T09:30:37.141088Z"
    }
   },
   "id": "b9a50771a6f6daa9",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T09:30:37.744636Z",
     "start_time": "2024-03-29T09:30:37.744585Z"
    }
   },
   "id": "3756a61f2e968f40"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
