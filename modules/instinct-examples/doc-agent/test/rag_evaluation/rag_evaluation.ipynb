{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install pre-requisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a65285e868d7809"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets ragatouille ratelimit retry"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:08.739906Z",
     "start_time": "2024-04-03T06:16:05.745829Z"
    }
   },
   "id": "3c7ba6dbea18e2ef",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:08.759104Z",
     "start_time": "2024-04-03T06:16:08.741302Z"
    }
   },
   "id": "cd3959f3217bfa17",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model preparations\n",
    "\n",
    "To go through the evaluation process, we need following models:\n",
    "\n",
    "1. Document model: Embedding model to generate document embeddings which will persisted in vector index. \n",
    "2. Reader model: A text completion model to answer the final question with augmented context.\n",
    "3. Evaluator model: A chat completion model that will give final verdict about RAG output. As this model will affect scoring considerably, stronger model should be used. \n",
    "\n",
    "As the choice of different models is not subject of this article and won't impact the comparison between RAG frameworks, we are determined to use completed local solution for this experiment for better speed and lower cost. \n",
    "\n",
    "To be more precise, following models that are already optimized in Ollama are used:\n",
    "\n",
    "* [Gemma 2B](https://huggingface.co/google/gemma-2b) as both `Document model` and `Reader model`.\n",
    "* [Mixtral-8x7B](https://ollama.com/library/mixtral) for `Evaluator model`\n",
    "   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2c4b6af65d58cdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama, MiniMaxChat\n",
    "import os\n",
    "\n",
    "\n",
    "READER_MODEL_NAME = \"gemma:2b\"\n",
    "EMBEDDING_NAME = \"all-minilm\"\n",
    "\n",
    "# OLLAMA_BASE_URL = \"http://192.168.0.29:11434\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=EMBEDDING_NAME, base_url = OLLAMA_BASE_URL)\n",
    "READER_LLM = Ollama(model=READER_MODEL_NAME, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "EVALUATOR_NAME = \"mixtral:latest\"\n",
    "EVAL_MODEL = ChatOllama(model=EVALUATOR_NAME, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "\n",
    "# EVALUATOR_NAME = \"abab6-chat\"\n",
    "# EVAL_MODEL = MiniMaxChat(\n",
    "#     model_name=EVALUATOR_NAME, \n",
    "#     minimax_api_key=os.getenv(\"MINIMAX_API_KEY\"),\n",
    "#     minimax_group_id=os.getenv(\"MINIMAX_GROUP_ID\")\n",
    "# )\n",
    "\n",
    "\n",
    "LANGCHAIN_DATA_ROOT = \"./data/langchain\"\n",
    "INSTINCT_DOC_AGENT_DATA_ROOT = \"./data/doc_agent\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:09.220275Z",
     "start_time": "2024-04-03T06:16:08.759753Z"
    }
   },
   "id": "b4dfdc109f0daffc",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build RAG pipeline using `langchain` \n",
    "\n",
    "1. transform training data in `m-ric/huggingface_doc` to `langchain`'s document objects\n",
    "2. Load into faiss index if index file is absent\n",
    "3. prompt with eval data `m-ric/huggingface_doc` using `READER_MODEL` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bdb8cc674412799"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Knowledge base preparations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881763874245235f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:09.773031Z",
     "start_time": "2024-04-03T06:16:09.221613Z"
    }
   },
   "id": "cdd427a5684551d2",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:09.786473Z",
     "start_time": "2024-04-03T06:16:09.773583Z"
    }
   },
   "id": "3a855d46ab06e6ec",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:09.800988Z",
     "start_time": "2024-04-03T06:16:09.787012Z"
    }
   },
   "id": "4e60ad11cd96e14c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EVAL_DATASET = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:31.354078Z",
     "start_time": "2024-04-03T06:16:09.801707Z"
    }
   },
   "id": "921990aa1cb355f0",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2647 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "703dd19dd2cd4be2bd216585342815b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\"))\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:43.735799Z",
     "start_time": "2024-04-03T06:16:31.356587Z"
    }
   },
   "id": "11ea5ea55c0da346",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=\"gpt-4\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        disallowed_special=[],\n",
    "        allowed_special=\"all\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:43.757106Z",
     "start_time": "2024-04-03T06:16:43.736726Z"
    }
   },
   "id": "40aa9cb71de4a46a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model: Embeddings,\n",
    "    embedding_model_name: str\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model: the embedding\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "         \n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name}\"\n",
    "    index_folder_path = os.path.join(LANGCHAIN_DATA_ROOT, index_name)\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs\n",
    "        )\n",
    "        print(f\"Index not found, generating it... {len(docs_processed)} docs in total\")\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:43.781759Z",
     "start_time": "2024-04-03T06:16:43.759414Z"
    }
   },
   "id": "13dc9ec746ddadd2",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QA chain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74338ffcd9971f99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:43.796717Z",
     "start_time": "2024-04-03T06:16:43.782564Z"
    }
   },
   "id": "851eb803b3f0a34f",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:46.980628Z",
     "start_time": "2024-04-03T06:16:43.797553Z"
    }
   },
   "id": "283a72f828eb8ec3",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating answers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeda09db98f634b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test function with langchain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "110434a4f3c3b6d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_langchain_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:47.012379Z",
     "start_time": "2024-04-03T06:16:46.981401Z"
    }
   },
   "id": "913cbbdae2722e2d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_langchain_test_all() -> str:\n",
    "    \"\"\"\n",
    "    Build index and run langchain test with fixed parameter and model selections\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"langchain_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "    \n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        embedding_model_name=EMBEDDING_NAME\n",
    "    )\n",
    "    \n",
    "    print(\"Running RAG...\")\n",
    "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "    run_langchain_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        llm=READER_LLM,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=True,\n",
    "        test_settings=settings_name,\n",
    "    )\n",
    "    \n",
    "    return output_file_name "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:16:47.043142Z",
     "start_time": "2024-04-03T06:16:47.013090Z"
    }
   },
   "id": "78de170f745561d",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm:\n",
      "Loading knowledge base embeddings...\n",
      "Index not found, generating it... 33666 docs in total\n",
      "Running RAG...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/67 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9075a022d6af4402873cd3b925260de4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer: The `tokenizers-linux-x64-musl` binary is designed for the x86 architecture.\n",
      "\n",
      "The context provides a list of binary names for different architectures, including `x86_64-unknown-linux-musl`, `aarch64-unknown-linux-musl`, `x86_64-unknown-linux-gnu`, `aarch64-unknown-linux-gnu`, `x86_64-pc-windows-msvc` and `aarch64-pc-windows-msvc`.\n",
      "True answer: x86_64-unknown-linux-musl\n",
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer: The context does not provide any information about the purpose of the BLIP-Diffusion model, so I cannot answer this question from the provided context.\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer: The user can claim authorship of a paper on the Hugging Face Hub by clicking in their name in the corresponding Paper page and clicking \"claim authorship\". This will automatically re-direct to their paper settings where they can confirm the request.\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer: The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure the app is running properly. It sends a JSON response containing information about the cache, queue, and any pending jobs for the dataset.\n",
      "True answer: Ensure the app is running\n",
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer: The context window size for Local Attention in the LongT5 model is not specified in the context, so the question cannot be answered from the context.\n",
      "True answer: 127 tokens\n",
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer: The `from_pretrained()` method is used to load a checkpoint for a task using `AutoPipeline`. This method allows you to specify the path to the checkpoint file and the model architecture to load.\n",
      "True answer: from_pretrained()\n",
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer: The purpose of the Diffusers library is to provide an easy-to-use and efficient framework for building natural language processing (NLP) models. It is designed to be lightweight and performant, while still offering a wide range of features for model customization and optimization.\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer: The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.\n",
      "True answer: Ancestral sampling with Euler method steps.\n",
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer: The name of the large multimodal model that can solve image-text tasks and is based on Flamingo is IDEFICS.\n",
      "True answer: IDEFICS\n",
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer: The purpose of the `gradio.Blocks` API is to allow developers to have full control over the data flows and layout of their application. It allows developers to build very complex, multi-step applications.\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer: The purpose of the two-stage model proposed in the paper is to generate images with high-frequency details. It first generates the latents in a very compressed latent space and then decompresses them into a bigger latent space of a VQGAN. These latents can then be decoded by Stage A, which is a VQGAN, into the pixel-space. Stage B & Stage A are both encapsulated in the `decoder_pipeline`.\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using ðŸ¤— Transformers?\n",
      "\n",
      "Answer: The command to install the requirements for a research project using ðŸ¤— Transformers is:\n",
      "\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "True answer: pip install -r requirements.txt\n",
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer: The `roberta-large-mnli` checkpoint performs the task of fine-tuning a pre-trained RoBERTa model on the Multi Language Information in the Large (MNLI) dataset.\n",
      "True answer: Text classification\n",
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer: The context does not provide information about the service replacing the Paid tier of the Inference API at Hugging Face, so I cannot answer this question from the context.\n",
      "True answer: Inference Endpoints\n",
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer: SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.\n",
      "True answer: Grouped convolutions\n",
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer: The HuggingFace Team's software is distributed under the Apache License, Version 2.0 (the \"License\").\n",
      "True answer: Apache License, Version 2.0\n",
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer: Sure, the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:\n",
      "\n",
      "- Splitting the embedding matrix into two smaller matrices.\n",
      "- Using repeating layers split among groups.\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?\n",
      "\n",
      "Answer: Sure, here are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library:\n",
      "\n",
      "1. Load a dataset from the Hugging Face Hub.\n",
      "2. Preprocess the data with `Dataset.map()`.\n",
      "3. Load and compute metrics.\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer: The passage states that Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n",
      "True answer: +800%\n",
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer: The command to upload a spaCy pipeline to the Hugging Face Hub is:\n",
      "```\n",
      "pip install --upgrade spacy-huggingface-hub\n",
      "```\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "=======================================================\n",
      "Question: What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?\n",
      "\n",
      "Answer: The time and memory complexity of the NystrÃ¶mformer's approximation of self-attention is O(n), where n is the length of the input sequence. The model uses a linear time and memory complexity by sampling landmarks (or NystrÃ¶m points) from queries and keys, which can be used to construct matrices that approximate the self-attention matrix.\n",
      "True answer: O(n)\n",
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer: The goal of the Named Entity Recognition (NER) task in token classification is to find the entities in a piece of text, such as person, location, or organization.\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer: The resolution of images used by the CLIPSeg model is not specified in the context, so I cannot answer this question from the context.\n",
      "True answer: 352 x 352 pixels\n",
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer: The context does not provide any information about what Gradio can be used for, so I cannot answer this question from the provided context.\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer: The TensorFlow API function used to load a saved tensor file is `safe_open`.\n",
      "True answer: safetensors.tensorflow.load_file\n",
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer: According to the context, you can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the â€œLogsâ€ tab of your Endpoint.\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n",
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer: The latest task added to Hugging Face AutoTrain for Computer Vision is image classification.\n",
      "True answer: Image Classification\n",
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer: The default repository type created by the `create_repo` function on Hugging Face Hub is the `dataset` type.\n",
      "True answer: model\n",
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer: The \"duorc\" dataset has 3 splits. They are named `train`, `validation` and `test`.\n",
      "True answer: Six\n",
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer: The purpose of Fully Sharded Data Parallel (FSDP) is to enable distributed training of large models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes. FSDP achieves memory efficiency by replicating the model across data parallel workers, reducing memory usage. Additionally, it allows for CPU offloading of all those tensors, enabling training of larger models.\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer: The file format used to save and store PyTorch model weights more securely than `.bin` files is the `safetensor` format. `safetensors` is a secure alternative to `pickle`, making it ideal for sharing model weights.\n",
      "True answer: `.safetensors`\n",
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer: According to the context, Hugging Face is [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning they provide security certification to their customers and actively monitor and patch any security weaknesses.\n",
      "True answer: SOC2 Type 2 certified\n",
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer: RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer: The library that MarkupLMFeatureExtractor uses to extract data from HTML and XML files is Beautiful Soup.\n",
      "True answer: Beautiful Soup\n",
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer: The file size limit for syncing to HF Spaces without using Git-LFS is not mentioned in the context, so I cannot answer this question from the provided context.\n",
      "True answer: 10MB\n",
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer: The title of the paper introducing the ByT5 model is [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626).\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer: The dimension of the feature vector for the base BERT model is 768.\n",
      "True answer: 768\n",
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer: The WordPiece model uses the `##` prefix to identify tokens that are part of a word (ie not starting a word).\n",
      "True answer: ##\n",
      "=======================================================\n",
      "Question: What is the purpose of the ðŸ§¨ Diffusers tutorials?\n",
      "\n",
      "Answer: The purpose of the ðŸ§¨ Diffusers tutorials is not explicitly stated in the context, so I cannot answer this question from the context.\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer: The default setting for the `allow_flagging` parameter in Gradio's `Interface` is **False**.\n",
      "True answer: \"manual\"\n",
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer: The full code for the Stable Diffusion demo can be found at:\n",
      "- [hf.co/spaces/stabilityai/stable-diffusion/tree/mainDocument](https://hf.co/apple/coreml-stable-diffusion-2-1-base)\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer: The FNet model replaces the self-attention layer in a BERT model with a fourier transform.\n",
      "True answer: Fourier transform\n",
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer: According to the context, a bug fix should typically accompany tests wherever reasonably possible. This ensures that the bug fix has been properly tested and does not introduce new issues.\n",
      "True answer: Dynamic code test\n",
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?\n",
      "\n",
      "Answer: The passage does not specify how to force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate, so I cannot answer this question from the context.\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n",
      "=======================================================\n",
      "Question: What is the range of parameters for the LLaMA models?\n",
      "\n",
      "Answer: The range of parameters for the LLaMA models is from 7B to 65B.\n",
      "\n",
      "The context states:\n",
      "\n",
      "> \"trainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\"\n",
      "True answer: 7B to 65B parameters\n",
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer: The purpose of tokenizers in the NLP pipeline is to break down the raw text into smaller units called tokens. This allows the model to understand the meaning of the text at a more granular level.\n",
      "\n",
      "The purpose of tokenizers is to handle entities that span over several tokens, as they need to be recognized by the model in order to perform tasks such as token classification or sentiment analysis.\n",
      "True answer: To translate text into data that can be processed by the model.\n",
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer: Sure, here is the answer to the question:\n",
      "\n",
      "The purpose of the Safety Checker in the Diffusers library is to check and flag inappropriate content generated during inference. This component helps model creators to ensure responsible and ethical use of the models, preventing the generation of harmful and NSFW content.\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer: The `InferenceClient` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub.\n",
      "True answer: HfApi\n",
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer: The context does not provide the name of the new library introduced by Hugging Face for hosting scikit-learn models, so I cannot answer this question from the provided context.\n",
      "True answer: Skops\n",
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer: The purpose of Textual Inversion is to personalize models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer: The context does not provide information about the recommended multiple of batch size for fp16 data type on an A100 GPU, so I cannot answer this question from the context.\n",
      "True answer: 64\n",
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer: Sure, here's the answer to the question:\n",
      "\n",
      "To run a Gradio Blocks app in reload mode using a Python IDE, simply pass the name of the Gradio Blocks/Interface demo in the code as the 2nd parameter to the `gr.Blocks()` function. For example, the code in the question would be:\n",
      "\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "with gr.Blocks(\"my_demo\") as demo:\n",
      "    # Your code goes here\n",
      "```\n",
      "\n",
      "This will allow you to make changes to the layout, inputs, and outputs of the Gradio app and have Gradio reload the app automatically when you make changes.\n",
      "True answer: Run `gradio run.py` in the terminal.\n",
      "=======================================================\n",
      "Question: What command is used to install the development version of the ðŸ¤— Transformers library in a Python virtual environment?\n",
      "\n",
      "Answer: The command used to install the development version of the ðŸ¤— Transformers library in a Python virtual environment is:\n",
      "\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "True answer: pip install \"transformers[sentencepiece]\"\n",
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer: Sure, here's how you can install the Hugging Face Unity API in your Unity project:\n",
      "\n",
      "1. Open your Unity project\n",
      "2. Go to `Window` -> `Package Manager`\n",
      "3. Click `+` and select `Add Package from git URL`\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer: The pretraining objective of the Wav2Vec2 context network is to find the most similar context vector and quantized speech unit (the target label).\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer: The default checkpoint used by the sentiment analysis pipeline in the Transformers library is distilbert base uncased finetuned sst2 english.\n",
      "True answer: distilbert base uncased finetuned sst2 english\n",
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer: The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to provide instructions on how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer: Sure, the `torchrun` module allows you to run a script on multiple GPUs.\n",
      "\n",
      "The command would be:\n",
      "\n",
      "```bash\n",
      "torchrun --nproc_per_node=2 --nnodes=1 example_script.py\n",
      "```\n",
      "True answer: torchrun\n",
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer: The context does not specify the most popular vision transformer model on the Hugging Face Model Hub for image classification, so I cannot answer this question from the context.\n",
      "True answer: google/vit-base-patch16-224\n",
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer: The context does not provide the command to upload an ESPnet model to a Hugging Face repository, so I cannot answer this question from the context.\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer: The file containing the Python dependencies should be added to the model repository on the Hugging Face Hub.\n",
      "True answer: requirements.txt\n",
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer: The context does not specify how many images are needed to teach new concepts to Stable Diffusion using Textual Inversion, so I cannot answer this question from the provided context.\n",
      "True answer: 3-5 images\n",
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer: The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is not specified in the context, so I cannot answer this question from the context.\n",
      "True answer: 10GB\n",
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer: The purpose of Weights and Biases (W&B) is not mentioned in the context, so I cannot answer this question from the provided context.\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n",
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer: The name of the open-source library created by Hugging Face to simplify Transformer acceleration is **Transformers**.\n",
      "True answer: Optimum\n",
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer: The parameter used to ensure that elements in a row have the same height in Gradio is the `scale` parameter.\n",
      "True answer: equal_height\n",
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer: The command to install the latest version of Optimum with OpenVINO support is:\n",
      "\n",
      "```bash\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "```\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    }
   ],
   "source": [
    "# execute test for langchain\n",
    "LANGCHAIN_TEST_OUTPUT = run_langchain_test_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:57.764824Z",
     "start_time": "2024-04-03T06:16:47.044089Z"
    }
   },
   "id": "96d9c29a10db3e84",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test function with doc-agent in instinct.cpp\n",
    "\n",
    "You have to manually start `doc-agent` locally.\n",
    "\n",
    "To build knowledge index with same knowledge base data from HF:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=./data/instinct/index.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  build \\\n",
    "  --force \\\n",
    "  --file=https://huggingface.co/api/datasets/m-ric/huggingface_doc/parquet/default/train/0.parquet \\\n",
    "  --type=PARQUET \\\n",
    "  --parquet_mapping=0:txt,1:metadata:source:varchar \\\n",
    "  serve \\\n",
    "  --port=9090 \n",
    "```\n",
    "\n",
    "To start http server for query:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=/tmp/rag_eval.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  serve \\\n",
    "  --port=9090 \n",
    "```\n",
    "\n",
    "Next, we will begin QA tests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4580fc87b583de8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def answer_with_doc_agent(question: str):\n",
    "    import requests\n",
    "    res = requests.post(\"http://localhost:9090/v1/chat/completions\", json={\"messages\": [{\"content\": question, \"role\": \"human\"}], \"stream\": False})\n",
    "    assert res.status_code == 200\n",
    "    body = res.json()\n",
    "    return body[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "\n",
    "def run_doc_agent_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = answer_with_doc_agent(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:57.848753Z",
     "start_time": "2024-04-03T06:32:57.765576Z"
    }
   },
   "id": "4afb09c4ab37a98f",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_doc_agent_test_all():\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"doc_agent_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "    print(f\"Running evaluation for {settings_name}:\")\n",
    "    run_doc_agent_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        output_file=output_file_name,\n",
    "        test_settings=settings_name\n",
    "    )\n",
    "    \n",
    "    return output_file_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:57.878599Z",
     "start_time": "2024-04-03T06:32:57.849412Z"
    }
   },
   "id": "4cceb41a4f1cbe75",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm:\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/67 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "158d21ec8c9a4698acd33d82677e43ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOC_AGENT_TEST_OUTPUT = run_doc_agent_test_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:57.916222Z",
     "start_time": "2024-04-03T06:32:57.879361Z"
    }
   },
   "id": "66a1467f5bfb50f",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Runner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdc11c2c8edc69d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ratelimit import limits,sleep_and_retry\n",
    "from retry import retry\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=6, period=60)\n",
    "def throttled_invoke(eval_chat_model, eval_prompt):\n",
    "    return eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "\n",
    "\n",
    "@retry(exceptions=Exception, tries=6)\n",
    "def evaluate_single_answer(\n",
    "        evaluation_prompt_template: ChatPromptTemplate,\n",
    "        experiment: dict,\n",
    "        throttled:bool,\n",
    "        eval_chat_model: BaseChatModel\n",
    "):\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "    if throttled:\n",
    "        eval_result = throttled_invoke(eval_chat_model, eval_prompt)\n",
    "    else:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    splits = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    assert len(splits) == 2\n",
    "    assert 1 <= int(splits[1]) <= 5\n",
    "    return splits\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    "    throttled:bool = True\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment and experiment[f\"eval_score_{evaluator_name}\"]:\n",
    "            continue\n",
    "        \n",
    "        splits = evaluate_single_answer(evaluation_prompt_template, experiment, throttled, eval_chat_model)\n",
    "        \n",
    "        if len(splits) != 2:\n",
    "            print(splits)\n",
    "            # experiment[f\"eval_score_{evaluator_name}\"] = \"\"\n",
    "            # experiment[f\"eval_feedback_{evaluator_name}\"] = \"\"\n",
    "            continue\n",
    "        feedback, score = splits \n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:57.950320Z",
     "start_time": "2024-04-03T06:32:57.916951Z"
    }
   },
   "id": "31856cff24422023",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:57.988289Z",
     "start_time": "2024-04-03T06:32:57.951271Z"
    }
   },
   "id": "c193a75198dabe56",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run evaluations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "292b22f2978e2d2c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/67 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75684c5bf1484566ac8f795efc57fd16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/67 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0381ba9f62e74f7292ebfa6d611da06c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_eval_results():\n",
    "    import glob\n",
    "    for output_file_name in glob.glob(\"./output/*.json\"):\n",
    "        print(f\"Evaluating {output_file_name}\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            EVAL_MODEL,\n",
    "            EVALUATOR_NAME,\n",
    "            EVALUATION_PROMPT_TEMPLATE,\n",
    "            # throttling is not needed for local model\n",
    "            False\n",
    "        )\n",
    "\n",
    "generate_eval_results()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:39:19.044785Z",
     "start_time": "2024-04-03T06:32:57.988996Z"
    }
   },
   "id": "83e219af8e9a45c9",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                               question  \\\n0                                           What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n   \n1                                                                    What is the purpose of the BLIP-Diffusion model?\\n   \n2                                                 How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n3                                        What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n   \n4                                    What is the default context window size for Local Attention in the LongT5 model?\\n   \n..                                                                                                                  ...   \n62  What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n   \n63               What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\\n   \n64          What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n   \n65                            What parameter is used to ensure that elements in a row have the same height in Gradio?\\n   \n66                                What is the command to install the latest version of Optimum with OpenVINO support?\\n   \n\n                                                                                                                                                         true_answer  \\\n0                                                                                                                                          x86_64-unknown-linux-musl   \n1                                                                        The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.   \n2   By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n3                                                                                                                                          Ensure the app is running   \n4                                                                                                                                                         127 tokens   \n..                                                                                                                                                               ...   \n62                                                                                                                                                              10GB   \n63                                                                          To track their machine learning experiments at every stage, from training to production.   \n64                                                                                                                                                           Optimum   \n65                                                                                                                                                      equal_height   \n66                                                                                                          pip install --upgrade-strategy eager optimum[\"openvino\"]   \n\n                                                                                            source_doc  \\\n0                          huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md   \n1                       huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md   \n2                                               huggingface/hub-docs/blob/main/docs/hub/paper-pages.md   \n3                                         huggingface/datasets-server/blob/main/services/api/README.md   \n4                                huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md   \n..                                                                                                 ...   \n62                                     huggingface/transformers/blob/main/docs/source/en/big_models.md   \n63  gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md   \n64                                                                 huggingface/blog/blob/main/intel.md   \n65              gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md   \n66                            huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        generated_answer  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The `tokenizers-linux-x64-musl` binary is designed for the x86 architecture.\\n\\nThe context provides a list of binary names for different architectures, including `x86_64-unknown-linux-musl`, `aarch64-unknown-linux-musl`, `x86_64-unknown-linux-gnu`, `aarch64-unknown-linux-gnu`, `x86_64-pc-windows-msvc` and `aarch64-pc-windows-msvc`.   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The context does not provide any information about the purpose of the BLIP-Diffusion model, so I cannot answer this question from the provided context.   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The user can claim authorship of a paper on the Hugging Face Hub by clicking in their name in the corresponding Paper page and clicking \"claim authorship\". This will automatically re-direct to their paper settings where they can confirm the request.   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure the app is running properly. It sends a JSON response containing information about the cache, queue, and any pending jobs for the dataset.   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The context window size for Local Attention in the LongT5 model is not specified in the context, so the question cannot be answered from the context.   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...   \n62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The maximum size of a model checkpoint before it is automatically sharded in the Transformers version 4.18.0 is not specified in the documentation.   \n63  Sure, here are the specific roles and responsibilities of Weights and Biases (W&B) in data science and machine learning:\\n\\n> W&B play a crucial role in data science and machine learning by helping to model and analyze complex relationships between data inputs and outputs. They are used in various data science tasks such as image processing, natural language processing, and predictive modeling.\\n\\n> W&B are responsible for learning the patterns and relationships from the data by performing various operations such as training, backpropagation, and optimization. They help to extract these patterns and relationships to make predictions on new data inputs.\\n\\n> W&B are also responsible for setting the initial weights and biases of neural networks to determine their initial state. They are used in the training process to adjust the weights and biases to achieve better performance on the data.   \n64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The name of the open-source library created by Hugging Face to simplify Transformer acceleration is not mentioned in the conversation, so I cannot answer this question from the provided context.   \n65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Sure, here is the standalone question in its original language:\\n\\n> What is the parameter used to ensure that elements in a row have the same height in Gradio?   \n66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The standalone question is about OpenVINO support for installation. It is not explicitly mentioned in the context, so I cannot answer this question from the provided context.   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  retrieved_docs  \\\n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\">\\n    <br>\\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/>\\n    <br>\\n<p>\\n<p align=\"center\">\\n    <img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\">\\n    <a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://docs.rs/tokenizers/\">\\n        <img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\">\\n    </a>\\n</p>\\n<br>\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`, `tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`]   \n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, ## How does Stable Diffusion work?\\n\\nHaving seen the high-quality images that stable diffusion can produce, let's try to understand \\na bit better how the model functions.\\n\\nStable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\\n\\nGenerally speaking, diffusion models are machine learning systems that are trained to *denoise* random Gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)., --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., We've gone from the basic use of Stable Diffusion using ðŸ¤— Hugging Face Diffusers to more advanced uses of the library, and we tried to introduce all the pieces in a modern diffusion system. If you liked this topic and want to learn more, we recommend the following resources:\\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\\n- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, that gives a broader overview on Diffusion systems.\\n- The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post., # Stable Diffusion 2\\n\\nStable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).]   \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\\n</div>\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., The Hub will attempt to automatically match paper to users based on their email. \\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\\n</div>\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n> Looking for a good first issue to work on?\\n> Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n> Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, ### Would you like to integrate your library to the Hub?\\n\\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\\n\\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) ðŸ¦™.\\n\\nThank you fastlearners and hugging learners ðŸ¤—., ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hubâ€™s Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Faceâ€™s models on the Hugging Face Hub have an associated model card on the Hub[^8]., --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\")]   \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [Datasets server API - rows endpoint\\n\\n> /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /search: get a slice of a search result over a dataset split\\n- /filter: filter rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined., The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&config={config}`\\n  - `split`: `?dataset={dataset}&config={config}&split={split}`, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset]   \n4   [## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., ## Local attention\\n\\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., -->\\n\\n# LongT5\\n\\n## Overview\\n\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention.\\n\\n\\nThe abstract from the paper is the following:, However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]]]   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...   \n62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           NaN   \n\n                                                                        test_settings  \\\n0   langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n1   langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n2   langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n3   langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n4   langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n..                                                                                ...   \n62  doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n63  doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n64  doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n65  doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n66  doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm   \n\n   eval_score_mixtral:latest  \\\n0                          3   \n1                          3   \n2                          4   \n3                          5   \n4                          3   \n..                       ...   \n62                         3   \n63                         1   \n64                         3   \n65                         5   \n66                         2   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               eval_feedback_mixtral:latest  \\\n0                                                                                                                                                                                                                               While the response correctly identifies that the `tokenizers-linux-x64-musl` binary is designed for the x86 architecture, it fails to provide the complete and exact architecture name as specified in the reference answer. The reference answer includes additional details (`x86_64-unknown-linux-musl`) that are crucial for a fully accurate response.   \n1                                                                                                                                                                                                   The response correctly acknowledges that the context does not provide sufficient information to answer the question, which is a valid observation. However, it fails to provide any additional information about the purpose of the BLIP-Diffusion model as outlined in the reference answer. Therefore, the response lacks the necessary factual information to be considered correct.   \n2                                                                                                                                                                                                                                                                                                             The response is largely correct and accurately describes the process of claiming authorship on the Hugging Face Hub. However, it omits the crucial step of waiting for admin team validation, which is an essential part of the process as mentioned in the reference answer.   \n3                                                                                                                                                                                                                 The response accurately explains the purpose of the `/healthcheck` endpoint in the Datasets server API, stating that it ensures the app is running properly and sends a JSON response with information about the cache, queue, and any pending jobs for the dataset. This aligns perfectly with the reference answer, making the response correct, accurate, and factual.   \n4   The response correctly indicates that the context does not provide enough information to answer the question, which aligns with the instruction's requirement for a factual answer based on the context. However, since the reference answer provides a specific value for the default context window size, the response lacks the necessary detail to be evaluated as correct or fully accurate.\\nFeedback: The response accurately states that the information is not available in the provided context; however, it does not offer a factual answer based on the reference material.   \n..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n62          The response correctly states that the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is not specified in the documentation. However, the reference answer provides a specific value (10GB) which is the actual maximum size. Therefore, the response is not directly addressing the given instruction and lacks accuracy.\\n\\nFeedback: [The response correctly indicates that the maximum size is not specified in the documentation but fails to provide the expected factual value as stated in the reference answer.]   \n63                                                                                                                                     The response does not correctly address the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. While some parts of the response touch on relevant topics such as training, backpropagation, and optimization, they are not presented in a way that accurately describes W&B's role. The response does not mention experiment tracking at all, which is a key purpose of W&B according to the reference answer.   \n64                                                                                                                                                                                                                                                              The response correctly states that the name of the library cannot be answered from the provided context, which aligns with the score rubric's criteria for accuracy and factuality. However, it does not provide an actual answer or reference to the Optimum library as in the reference answer, leading to a lower score.   \n65                                                                                                                                                                                                                                                                                                                                                                                                               The response is correct and aligns with the reference answer. The parameter used to ensure that elements in a row have the same height in Gradio is indeed \"equal_height\".   \n66                                                                                                                                                                                                                               The response accurately acknowledges that the context does not provide enough information to answer the question. However, it fails to provide a solution to install the latest version of Optimum with OpenVINO support as required by the task. The reference answer shows the correct command for this operation, which is not present in the response.   \n\n                                                                                               settings  \n0   ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n1   ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n2   ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n3   ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n4   ./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n..                                                                                                  ...  \n62  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n63  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n64  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n65  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n66  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json  \n\n[134 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>true_answer</th>\n      <th>source_doc</th>\n      <th>generated_answer</th>\n      <th>retrieved_docs</th>\n      <th>test_settings</th>\n      <th>eval_score_mixtral:latest</th>\n      <th>eval_feedback_mixtral:latest</th>\n      <th>settings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n</td>\n      <td>x86_64-unknown-linux-musl</td>\n      <td>huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md</td>\n      <td>The `tokenizers-linux-x64-musl` binary is designed for the x86 architecture.\\n\\nThe context provides a list of binary names for different architectures, including `x86_64-unknown-linux-musl`, `aarch64-unknown-linux-musl`, `x86_64-unknown-linux-gnu`, `aarch64-unknown-linux-gnu`, `x86_64-pc-windows-msvc` and `aarch64-pc-windows-msvc`.</td>\n      <td>[`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\"&gt;\\n    &lt;br&gt;\\n    &lt;img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/&gt;\\n    &lt;br&gt;\\n&lt;p&gt;\\n&lt;p align=\"center\"&gt;\\n    &lt;img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\"&gt;\\n    &lt;a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\"&gt;\\n        &lt;img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\"&gt;\\n    &lt;/a&gt;\\n    &lt;a href=\"https://docs.rs/tokenizers/\"&gt;\\n        &lt;img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\"&gt;\\n    &lt;/a&gt;\\n&lt;/p&gt;\\n&lt;br&gt;\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`, `tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`]</td>\n      <td>langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>3</td>\n      <td>While the response correctly identifies that the `tokenizers-linux-x64-musl` binary is designed for the x86 architecture, it fails to provide the complete and exact architecture name as specified in the reference answer. The reference answer includes additional details (`x86_64-unknown-linux-musl`) that are crucial for a fully accurate response.</td>\n      <td>./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is the purpose of the BLIP-Diffusion model?\\n</td>\n      <td>The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.</td>\n      <td>huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md</td>\n      <td>The context does not provide any information about the purpose of the BLIP-Diffusion model, so I cannot answer this question from the provided context.</td>\n      <td>[Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, ## How does Stable Diffusion work?\\n\\nHaving seen the high-quality images that stable diffusion can produce, let's try to understand \\na bit better how the model functions.\\n\\nStable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\\n\\nGenerally speaking, diffusion models are machine learning systems that are trained to *denoise* random Gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)., --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., We've gone from the basic use of Stable Diffusion using ðŸ¤— Hugging Face Diffusers to more advanced uses of the library, and we tried to introduce all the pieces in a modern diffusion system. If you liked this topic and want to learn more, we recommend the following resources:\\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\\n- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, that gives a broader overview on Diffusion systems.\\n- The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post., # Stable Diffusion 2\\n\\nStable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).]</td>\n      <td>langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>3</td>\n      <td>The response correctly acknowledges that the context does not provide sufficient information to answer the question, which is a valid observation. However, it fails to provide any additional information about the purpose of the BLIP-Diffusion model as outlined in the reference answer. Therefore, the response lacks the necessary factual information to be considered correct.</td>\n      <td>./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n      <td>huggingface/hub-docs/blob/main/docs/hub/paper-pages.md</td>\n      <td>The user can claim authorship of a paper on the Hugging Face Hub by clicking in their name in the corresponding Paper page and clicking \"claim authorship\". This will automatically re-direct to their paper settings where they can confirm the request.</td>\n      <td>[* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., The Hub will attempt to automatically match paper to users based on their email. \\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n&gt; Looking for a good first issue to work on?\\n&gt; Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n&gt; Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, ### Would you like to integrate your library to the Hub?\\n\\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\\n\\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) ðŸ¦™.\\n\\nThank you fastlearners and hugging learners ðŸ¤—., ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hubâ€™s Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Faceâ€™s models on the Hugging Face Hub have an associated model card on the Hub[^8]., --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\")]</td>\n      <td>langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>4</td>\n      <td>The response is largely correct and accurately describes the process of claiming authorship on the Hugging Face Hub. However, it omits the crucial step of waiting for admin team validation, which is an essential part of the process as mentioned in the reference answer.</td>\n      <td>./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n</td>\n      <td>Ensure the app is running</td>\n      <td>huggingface/datasets-server/blob/main/services/api/README.md</td>\n      <td>The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure the app is running properly. It sends a JSON response containing information about the cache, queue, and any pending jobs for the dataset.</td>\n      <td>[Datasets server API - rows endpoint\\n\\n&gt; /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /search: get a slice of a search result over a dataset split\\n- /filter: filter rows of a dataset split, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /hub-cache: Return a dataset information as a Server-Sent Event (SSE) when a dataset is updated. If `?all=true` is passed in the parameters, and if the cache already has some entries, one SSE per cached dataset is sent to the client. Then, a SSE is sent when a dataset is inserted, modified or deleted. The event data is a JSON with the following structure. The `hub_cache` field is null for deleted entries, or when the response is an error. The `num_rows` value is `0` if it could not be determined., The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&amp;config={config}`\\n  - `split`: `?dataset={dataset}&amp;config={config}&amp;split={split}`, ## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset]</td>\n      <td>langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>5</td>\n      <td>The response accurately explains the purpose of the `/healthcheck` endpoint in the Datasets server API, stating that it ensures the app is running properly and sends a JSON response with information about the cache, queue, and any pending jobs for the dataset. This aligns perfectly with the reference answer, making the response correct, accurate, and factual.</td>\n      <td>./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the default context window size for Local Attention in the LongT5 model?\\n</td>\n      <td>127 tokens</td>\n      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md</td>\n      <td>The context window size for Local Attention in the LongT5 model is not specified in the context, so the question cannot be answered from the context.</td>\n      <td>[## Longformer Self Attention\\n\\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., ## Local attention\\n\\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., --&gt;\\n\\n# LongT5\\n\\n## Overview\\n\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention.\\n\\n\\nThe abstract from the paper is the following:, However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]]]</td>\n      <td>langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>3</td>\n      <td>The response correctly indicates that the context does not provide enough information to answer the question, which aligns with the instruction's requirement for a factual answer based on the context. However, since the reference answer provides a specific value for the default context window size, the response lacks the necessary detail to be evaluated as correct or fully accurate.\\nFeedback: The response accurately states that the information is not available in the provided context; however, it does not offer a factual answer based on the reference material.</td>\n      <td>./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n</td>\n      <td>10GB</td>\n      <td>huggingface/transformers/blob/main/docs/source/en/big_models.md</td>\n      <td>The maximum size of a model checkpoint before it is automatically sharded in the Transformers version 4.18.0 is not specified in the documentation.</td>\n      <td>NaN</td>\n      <td>doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>3</td>\n      <td>The response correctly states that the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is not specified in the documentation. However, the reference answer provides a specific value (10GB) which is the actual maximum size. Therefore, the response is not directly addressing the given instruction and lacks accuracy.\\n\\nFeedback: [The response correctly indicates that the maximum size is not specified in the documentation but fails to provide the expected factual value as stated in the reference answer.]</td>\n      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>What is the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists?\\n</td>\n      <td>To track their machine learning experiments at every stage, from training to production.</td>\n      <td>gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md</td>\n      <td>Sure, here are the specific roles and responsibilities of Weights and Biases (W&amp;B) in data science and machine learning:\\n\\n&gt; W&amp;B play a crucial role in data science and machine learning by helping to model and analyze complex relationships between data inputs and outputs. They are used in various data science tasks such as image processing, natural language processing, and predictive modeling.\\n\\n&gt; W&amp;B are responsible for learning the patterns and relationships from the data by performing various operations such as training, backpropagation, and optimization. They help to extract these patterns and relationships to make predictions on new data inputs.\\n\\n&gt; W&amp;B are also responsible for setting the initial weights and biases of neural networks to determine their initial state. They are used in the training process to adjust the weights and biases to achieve better performance on the data.</td>\n      <td>NaN</td>\n      <td>doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>1</td>\n      <td>The response does not correctly address the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists. While some parts of the response touch on relevant topics such as training, backpropagation, and optimization, they are not presented in a way that accurately describes W&amp;B's role. The response does not mention experiment tracking at all, which is a key purpose of W&amp;B according to the reference answer.</td>\n      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n</td>\n      <td>Optimum</td>\n      <td>huggingface/blog/blob/main/intel.md</td>\n      <td>The name of the open-source library created by Hugging Face to simplify Transformer acceleration is not mentioned in the conversation, so I cannot answer this question from the provided context.</td>\n      <td>NaN</td>\n      <td>doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>3</td>\n      <td>The response correctly states that the name of the library cannot be answered from the provided context, which aligns with the score rubric's criteria for accuracy and factuality. However, it does not provide an actual answer or reference to the Optimum library as in the reference answer, leading to a lower score.</td>\n      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>What parameter is used to ensure that elements in a row have the same height in Gradio?\\n</td>\n      <td>equal_height</td>\n      <td>gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md</td>\n      <td>Sure, here is the standalone question in its original language:\\n\\n&gt; What is the parameter used to ensure that elements in a row have the same height in Gradio?</td>\n      <td>NaN</td>\n      <td>doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>5</td>\n      <td>The response is correct and aligns with the reference answer. The parameter used to ensure that elements in a row have the same height in Gradio is indeed \"equal_height\".</td>\n      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>What is the command to install the latest version of Optimum with OpenVINO support?\\n</td>\n      <td>pip install --upgrade-strategy eager optimum[\"openvino\"]</td>\n      <td>huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md</td>\n      <td>The standalone question is about OpenVINO support for installation. It is not explicitly mentioned in the context, so I cannot answer this question from the provided context.</td>\n      <td>NaN</td>\n      <td>doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm</td>\n      <td>2</td>\n      <td>The response accurately acknowledges that the context does not provide enough information to answer the question. However, it fails to provide a solution to install the latest version of Optimum with OpenVINO support as required by the task. The reference answer shows the correct command for this operation, which is not present in the response.</td>\n      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json</td>\n    </tr>\n  </tbody>\n</table>\n<p>134 rows Ã— 9 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_eval_results():\n",
    "    import glob\n",
    "    outputs = []\n",
    "    for file in glob.glob(\"./output/*.json\"):\n",
    "        output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "        output[\"settings\"] = file\n",
    "        outputs.append(output)\n",
    "    return pd.concat(outputs)\n",
    "\n",
    "EVAL_RESULTS = load_eval_results()\n",
    "display(EVAL_RESULTS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:39:19.094571Z",
     "start_time": "2024-04-03T06:39:19.045731Z"
    }
   },
   "id": "b157cef7583830b2",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scoring evaluation results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddfe8937f08d290a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "settings\n./output/rag_doc_agent_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json    0.492537\n./output/rag_langchain_chunk:200_rerank:False_reader-model:gemma:2b_embedding-model:all-minilm.json    0.641791\nName: eval_score_mixtral:latest, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scoring_output(eval_result: pd.DataFrame, evaluator_name: str):\n",
    "    score_field = f\"eval_score_{evaluator_name}\"\n",
    "    result = eval_result.loc[:, [score_field, \"settings\"]].copy()\n",
    "    \n",
    "    result[score_field] = result[score_field].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "    \n",
    "    result[score_field] = (result[score_field] - 1) / 4    \n",
    "    average_scores = result.groupby(\"settings\")[score_field].mean()\n",
    "\n",
    "    average_scores.sort_values()\n",
    "    return average_scores\n",
    "\n",
    "scores = scoring_output(EVAL_RESULTS, EVALUATOR_NAME)\n",
    "display(scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:39:19.130763Z",
     "start_time": "2024-04-03T06:39:19.095342Z"
    }
   },
   "id": "b9a50771a6f6daa9",
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
