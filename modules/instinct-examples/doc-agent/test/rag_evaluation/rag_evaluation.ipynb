{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a65285e868d7809",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c7ba6dbea18e2ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:45.825193Z",
     "start_time": "2024-04-17T09:44:43.031782Z"
    }
   },
   "source": "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets ragatouille ratelimit retry duckdb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "cd3959f3217bfa17",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:45.845738Z",
     "start_time": "2024-04-17T09:44:45.826447Z"
    }
   },
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext dotenv\n",
    "%dotenv"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d2c4b6af65d58cdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model preparations\n",
    "\n",
    "To go through the evaluation process, we need following models:\n",
    "\n",
    "1. Document model: Embedding model to generate document embeddings which will persisted in vector index. \n",
    "2. Reader model: A text completion model to answer the final question with augmented context.\n",
    "3. Evaluator model: A chat completion model that will give final verdict about RAG output. As this model will affect scoring considerably, stronger model should be used. \n",
    "\n",
    "As the choice of different models is not subject of this article and won't impact the comparison between RAG frameworks, we are determined to use completed local solution for this experiment for better speed and lower cost. \n",
    "\n",
    "To be more precise, following models that are already optimized in Ollama are used:\n",
    "\n",
    "* [all-minilm](https://huggingface.co/google/gemma-2b) for `Embedding model`;\n",
    "* [Mixtral-8x7B](https://ollama.com/library/mixtral) for `Evaluator model` and `Reader model`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "id": "b4dfdc109f0daffc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:46.654415Z",
     "start_time": "2024-04-17T09:44:45.846425Z"
    }
   },
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama, MiniMaxChat\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import os\n",
    "\n",
    "# points to a vLLM server\n",
    "MIXTRAL_ENDPOINT = \"http://192.168.0.134:30253\"\n",
    "\n",
    "# points to a ollama server\n",
    "MINILM_ENDPOINT = \"http://192.168.0.29:11434\"\n",
    "\n",
    "READER_MODEL_NAME = \"mixtral:instruct\"\n",
    "EMBEDDING_NAME = \"all-minilm\"\n",
    "EVALUATOR_NAME = \"mixtral:instruct\"\n",
    "\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=EMBEDDING_NAME, base_url = MINILM_ENDPOINT)\n",
    "READER_LLM = Ollama(model=READER_MODEL_NAME, base_url = MIXTRAL_ENDPOINT)\n",
    "EVAL_MODEL = ChatOllama(model=EVALUATOR_NAME, base_url = MIXTRAL_ENDPOINT)\n",
    "\n",
    "LANGCHAIN_DATA_ROOT = \"./data/langchain\"\n",
    "INSTINCT_DOC_AGENT_DATA_ROOT = \"./data/doc_agent\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8088b64de81ed222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:48.152634Z",
     "start_time": "2024-04-17T09:44:46.655645Z"
    }
   },
   "source": [
    "# Test all these models\n",
    "\n",
    "EMBEDDING_MODEL.embed_query(\"hello\")\n",
    "\n",
    "READER_LLM.invoke(\"hello\")\n",
    "\n",
    "EVAL_MODEL.invoke(\"hello\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Hello! It's nice to meet you. Is there something you would like to ask or talk about? I'm here to help with information and answer questions to the best of my ability.\", response_metadata={'model': 'mixtral:instruct', 'created_at': '2024-04-17T09:44:49.408726113Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 583661542, 'load_duration': 520790, 'prompt_eval_duration': 14299000, 'eval_count': 41, 'eval_duration': 568214000}, id='run-83d09bb5-b90f-4e62-beac-30fbe418e19c-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "3bdb8cc674412799",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Build RAG pipeline using `langchain` \n",
    "\n",
    "1. transform training data in `m-ric/huggingface_doc` to `langchain`'s document objects\n",
    "2. Load into faiss index if index file is absent\n",
    "3. prompt with eval data `m-ric/huggingface_doc` using `READER_MODEL` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881763874245235f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge base preparations"
   ]
  },
  {
   "cell_type": "code",
   "id": "cdd427a5684551d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:48.672828Z",
     "start_time": "2024-04-17T09:44:48.153927Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3a855d46ab06e6ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:48.687448Z",
     "start_time": "2024-04-17T09:44:48.673480Z"
    }
   },
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "4e60ad11cd96e14c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:44:48.702435Z",
     "start_time": "2024-04-17T09:44:48.688147Z"
    }
   },
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "921990aa1cb355f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:01.681468Z",
     "start_time": "2024-04-17T09:44:48.703219Z"
    }
   },
   "source": [
    "EVAL_DATASET = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since m-ric/huggingface_doc_qa_eval couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/robinqu/.cache/huggingface/datasets/m-ric___huggingface_doc_qa_eval/default/0.0.0/72d15fdd245839652aa30a5f8717b3b79f106c2a (last modified on Thu Mar 28 20:55:20 2024).\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "11ea5ea55c0da346",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:14.447837Z",
     "start_time": "2024-04-17T09:45:01.682373Z"
    }
   },
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\"))\n",
    "]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since m-ric/huggingface_doc couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/robinqu/.cache/huggingface/datasets/m-ric___huggingface_doc/default/0.0.0/1b83935099b148190b6a9a9874b7e62a17fea889 (last modified on Thu Mar 28 20:10:51 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76137fdc910d423e9ae611a364eedc5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "40aa9cb71de4a46a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:14.478017Z",
     "start_time": "2024-04-17T09:45:14.451272Z"
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=\"gpt-4\",\n",
    "        chunk_size=chunk_size,\n",
    "        # chunk_overlap=int(chunk_size / 10),\n",
    "        chunk_overlap=0,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        disallowed_special=[],\n",
    "        allowed_special=\"all\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "13dc9ec746ddadd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:14.501508Z",
     "start_time": "2024-04-17T09:45:14.478705Z"
    }
   },
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model: Embeddings,\n",
    "    embedding_model_name: str\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model: the embedding\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "         \n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name}\"\n",
    "    index_folder_path = os.path.join(LANGCHAIN_DATA_ROOT, index_name)\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs\n",
    "        )\n",
    "        print(f\"Index not found, generating it... {len(docs_processed)} docs in total\")\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "74338ffcd9971f99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## QA chain"
   ]
  },
  {
   "cell_type": "code",
   "id": "851eb803b3f0a34f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:14.516700Z",
     "start_time": "2024-04-17T09:45:14.502167Z"
    }
   },
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "283a72f828eb8ec3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:17.533836Z",
     "start_time": "2024-04-17T09:45:14.517280Z"
    }
   },
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "    \n",
    "    print(\"final prompt size:\", len(final_prompt))\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "eeda09db98f634b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generating answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110434a4f3c3b6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test function with langchain"
   ]
  },
  {
   "cell_type": "code",
   "id": "913cbbdae2722e2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:17.566406Z",
     "start_time": "2024-04-17T09:45:17.534863Z"
    }
   },
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_langchain_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        \n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "78de170f745561d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:45:17.597040Z",
     "start_time": "2024-04-17T09:45:17.567437Z"
    }
   },
   "source": [
    "def run_langchain_test_all() -> str:\n",
    "    \"\"\"\n",
    "    Build index and run langchain test with fixed parameter and model selections\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"langchain_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = os.path.join(\"./output\", f\"{settings_name}.json\")\n",
    "    \n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        embedding_model_name=EMBEDDING_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Running RAG with {settings_name}\")\n",
    "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "    run_langchain_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        llm=READER_LLM,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=True,\n",
    "        test_settings=settings_name,\n",
    "    )\n",
    "    \n",
    "    return output_file_name "
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "96d9c29a10db3e84",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:48:58.536105Z",
     "start_time": "2024-04-17T09:45:17.597616Z"
    }
   },
   "source": [
    "# execute test for langchain\n",
    "LANGCHAIN_TEST_OUTPUT = run_langchain_test_all()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge base embeddings...\n",
      "Running RAG with langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae2834cfd2f24c7485d254ffcc443334"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final prompt size: 1913\n",
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer:  The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.\n",
      "True answer: x86_64-unknown-linux-musl\n",
      "final prompt size: 3634\n",
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer:  Based on the provided context, there is no information given about the purpose of the BLIP-Diffusion model. The context mainly discusses Stable Diffusion and DDPO models.\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "final prompt size: 4301\n",
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer:  A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page, clicking on their name in the corresponding Paper page, and then clicking \"claim authorship\". This will redirect them to their paper settings where they can confirm the request. The admin team will validate this request, and once confirmed, the Paper page will show as verified.\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "final prompt size: 4199\n",
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer:  The `/healthcheck` endpoint in the Datasets server API is used to ensure that the application is running. It's a basic health check for the server's status.\n",
      "True answer: Ensure the app is running\n",
      "final prompt size: 5488\n",
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer:  The default context window size for Local Attention in the LongT5 model is defined by `config.attention_window`. It can be of type `List` to define a different window size for each layer. For more specifics on the default value, it would be best to refer to the actual implementation or documentation as it could vary depending on the specific configuration of the model.\n",
      "True answer: 127 tokens\n",
      "final prompt size: 5412\n",
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer:  The method used to load a checkpoint for a task using `AutoPipeline` is `from_pretrained()`. This method automatically detects the correct pipeline class to use, making it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n",
      "True answer: from_pretrained()\n",
      "final prompt size: 2532\n",
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer:  The purpose of the Diffusers library, as described in the provided context, is to be a lightweight and accessible package for working with diffusion models. It is designed to be a natural extension of PyTorch, prioritizing usability over performance and favoring simple, self-explainable code. By default, diffusion pipelines are instantiated on CPU with float32 precision to ensure usability across different platforms and accelerators without requiring complex installations. The library also aims to have minimal required dependencies, but offers many soft dependencies that can improve performance.\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "final prompt size: 4823\n",
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer:  The EulerAncestralDiscreteScheduler uses categorical samples across the vocabulary during response generation, as per the provided context (lm_human_preferences/language/sample.py#L43).\n",
      "True answer: Ancestral sampling with Euler method steps.\n",
      "final prompt size: 4513\n",
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer:  The name of the large multimodal model is Flamingo. It's mentioned in Document 2, which states that you can use this model to solve image-text tasks. However, it's not explicitly stated that Flamingo is based on Flamingo itself in the document. This information might be derived from other sources or documents.\n",
      "True answer: IDEFICS\n",
      "final prompt size: 2988\n",
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer:  The `gradio.Blocks` API is a new, low-level interface in Gradio 3.0 that provides full control over the data flows and layout of an application. It allows for the creation of complex, multi-step applications. This is achieved by enabling developers to build their own blocks, define their behavior, and arrange them as needed.\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "final prompt size: 4518\n",
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer:  The two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is designed to generate high-quality images based on textual descriptions. In the first stage, referred to as Stage C, the model generates latents in a very compressed latent space. These generated latents are then passed to Stage B in the second stage, which decompresses the latents into a bigger latent space of a VQGAN. Finally, these latents are decoded by Stage A, another VQGAN, into the pixel-space to produce the resulting image. This two-stage process allows for more controlled and detailed image generation based on text inputs.\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "final prompt size: 3725\n",
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using ðŸ¤— Transformers?\n",
      "\n",
      "Answer:  The command to install the requirements for a research project using ðŸ¤— Transformers is \"pip install -r requirements.txt\". This command should be run inside the specific research project folder.\n",
      "True answer: pip install -r requirements.txt\n",
      "final prompt size: 4223\n",
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer:  Based on the information provided in Document 0, the specific task of the `roberta-large-mnli` checkpoint is not directly mentioned. The document only asks the user to find the `roberta-large-mnli` checkpoint in the Hugging Face Hub and does not provide any details about the task it performs.\n",
      "True answer: Text classification\n",
      "final prompt size: 4598\n",
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer:  The Inference Endpoints service is replacing the Paid tier of the Inference API at Hugging Face, as announced in their new pricing blog post.\n",
      "True answer: Inference Endpoints\n",
      "final prompt size: 4471\n",
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer:  SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. This information can be found in the first document, \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\".\n",
      "True answer: Grouped convolutions\n",
      "final prompt size: 4600\n",
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer:  The HuggingFace Team's software is distributed under the Apache License, Version 2.0. This information can be found in the headers of several documents, including Document 2, Document 4, and Document 6.\n",
      "True answer: Apache License, Version 2.0\n",
      "final prompt size: 5354\n",
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer:  The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are 1) splitting the embedding matrix into two smaller matrices, and 2) using repeating layers split among groups.\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "final prompt size: 4524\n",
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?\n",
      "\n",
      "Answer:  The three main steps for fine-tuning a model with the ðŸ¤— Datasets library are:\n",
      "\n",
      "1. Loading a dataset from the Hugging Face Hub.\n",
      "2. Preprocessing the data using `Dataset.map()`.\n",
      "3. Loading and computing metrics.\n",
      "\n",
      "This is mentioned in Chapter 3 of the course you're taking on the ðŸ¤— Datasets library.\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "final prompt size: 5597\n",
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer:  The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is 800%. This information can be found in Document 0.\n",
      "True answer: +800%\n",
      "final prompt size: 4957\n",
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer:  The command to upload a spaCy pipeline to the Hugging Face Hub is not explicitly stated in the provided context. However, it is mentioned that the `spacy-huggingface-hub` library extends `spaCy`'s native CLI, and you can use the `huggingface-hub push` command to push models. You would need to install this library using `pip install spacy-huggingface-hub` and then use the command as mentioned in the context. Please refer to Document 5 for more details.\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "final prompt size: 4477\n",
      "=======================================================\n",
      "Question: What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?\n",
      "\n",
      "Answer:  The NystrÃ¶mformer model approximates standard self-attention with O(n) complexity, which is a significant improvement over the O(n^2) time and memory complexity of the standard self-attention mechanism. This efficiency is achieved without relying on sparsity or low-rankness priors of the self-attention matrix, making it applicable to various modalities like speech and protein sequence modeling.\n",
      "True answer: O(n)\n",
      "final prompt size: 4196\n",
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer:  The goal of the Named Entity Recognition (NER) task in token classification is to find and classify entities in a piece of text, such as persons, locations, or organizations. Each token (word or subword) in the text is assigned a label from a predefined set of classes indicating the type of entity it belongs to. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "final prompt size: 3980\n",
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer:  The resolution of images used by the CLIPSeg model is 352 x 352 pixels. This information can be found in document 5, where it's mentioned that \"the model uses images of 352 x 352 pixels\".\n",
      "True answer: 352 x 352 pixels\n",
      "final prompt size: 1244\n",
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer:  Based on the context provided, Gradio is a library built on top of other open-source libraries. It allows you to try out all its components and learn more about it from the provided link. However, the specific use cases or features of Gradio are not detailed in the provided documents. Therefore, I cannot provide a comprehensive answer to your question without additional context.\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "final prompt size: 4512\n",
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer:  The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load`. This information can be found in Document 0.\n",
      "True answer: safetensors.tensorflow.load_file\n",
      "final prompt size: 3943\n",
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer:  You can access the logs of your Endpoints in Hugging Face Endpoints through the \"Logs\" tab of your Endpoint in the UI. The Container Logs are only available when your Endpoint is in the \"Running\" state. If your Endpoint creation fails, you can check the Build Logs to see the reason, such as a wrong version of a dependency.\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n",
      "final prompt size: 4245\n",
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer:  The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.\n",
      "True answer: Image Classification\n",
      "final prompt size: 4767\n",
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer:  The default repository type created by the `create_repo` function on Hugging Face Hub is a 'model' repository. If you want to create a 'dataset' or a 'space', you can specify it using the `repo_type` argument.\n",
      "True answer: model\n",
      "final prompt size: 4883\n",
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer:  The \"duorc\" dataset has at least three splits: \"train\", \"validation\", and \"test\". This information can be found in Document 4, where the \"duorc\" dataset is mentioned with these three split names. There might be more splits, but based on this document, at least these three are confirmed.\n",
      "True answer: Six\n",
      "final prompt size: 5813\n",
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer:  The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard a model's parameters, gradients, and optimizer states across multiple GPUs. This reduces memory usage, improves GPU memory efficiency, and allows for the training of much larger models on fewer GPUs. FSDP also supports CPU offloading of sharded model parameters, enabling the training of large models that exceed the available GPU memory. It is designed for distributed training of large pretrained models up to 1T parameters.\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "final prompt size: 5508\n",
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer:  The file format used to save and store PyTorch model weights more securely than `.bin` files is `safetensors`. It is a secure alternative to `pickle`, which is not secure and may contain malicious code that can be executed.\n",
      "True answer: `.safetensors`\n",
      "final prompt size: 4553\n",
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer:  Hugging Face has SOC2 Type 2 security certification. This information can be found in Document 2.\n",
      "True answer: SOC2 Type 2 certified\n",
      "final prompt size: 4785\n",
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer:  RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks. This information is provided in Document 0 and Document 4.\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "final prompt size: 6171\n",
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer:  MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files, to extract data from HTML and XML files. This is mentioned in Document 0.\n",
      "True answer: Beautiful Soup\n",
      "final prompt size: 4616\n",
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer:  The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. This is specified in Document 0 where the `sync-to-hub` job has a step to check files larger than 10485760 bytes (10MB) with the `filesizelimit` parameter.\n",
      "True answer: 10MB\n",
      "final prompt size: 3274\n",
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer:  The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\". This information can be found in Document 2, 3, 4, and 5, which all refer to the same paper.\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "final prompt size: 4797\n",
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer:  The dimension of the feature vector for the base BERT model is 768. This information is provided in Document 0, where it's stated that \"Each of these vectors is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768.\"\n",
      "True answer: 768\n",
      "final prompt size: 5689\n",
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer:  The WordPiece model uses the \"##\" prefix to identify continuing subwords. This is used to indicate that the following characters are part of a word rather than the beginning of it.\n",
      "True answer: ##\n",
      "final prompt size: 2212\n",
      "=======================================================\n",
      "Question: What is the purpose of the ðŸ§¨ Diffusers tutorials?\n",
      "\n",
      "Answer:  The purpose of the ðŸ§¨ Diffusers tutorials is to provide practical guidance on how to use the ðŸ¤— Diffusers library, which is designed with a focus on usability and simplicity. The tutorials help users understand how to effectively utilize Diffusers, which is a natural extension of PyTorch, by providing clear and easy-to-follow instructions.\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "final prompt size: 4403\n",
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer:  The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that the flag button is not automatically displayed; it has to be added explicitly using the **Flag** button or through code.\n",
      "True answer: \"manual\"\n",
      "final prompt size: 4108\n",
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer:  The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. This information is provided in Document 0 of your context.\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "final prompt size: 3833\n",
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer:  The FNet model uses a fourier transform to replace the self-attention layer in a BERT model, specifically returning only the real parts of the transform.\n",
      "True answer: Fourier transform\n",
      "final prompt size: 4669\n",
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer:  The type of test that should typically accompany a bug fix in Gradio's testing strategy is a unit test or an integration test, depending on the complexity and scope of the bug. The goal is to isolate the behavior of the fixed code and ensure that the bug does not reoccur. This aligns with point 3 of the objectives mentioned in Document 0, which states \"Bug fixes should be accompanied by tests wherever is reasonably possible.\"\n",
      "True answer: Dynamic code test\n",
      "final prompt size: 5122\n",
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?\n",
      "\n",
      "Answer:  You can force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate by setting the `fp16` parameter to `True`. Here's an example:\n",
      "\n",
      "```py\n",
      "from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator(fp16=True)\n",
      "```\n",
      "\n",
      "This will enable mixed precision training, which can help reduce memory usage and speed up computation during model training.\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n",
      "final prompt size: 4036\n",
      "=======================================================\n",
      "Question: What is the range of parameters for the LLaMA models?\n",
      "\n",
      "Answer:  The LLaMA models range in parameters from 7B to 65B. This information can be found in Document 5, which provides a reference to the paper \"LLaMA: Open and Efficient Foundation Language Models\" where the models are originally discussed.\n",
      "True answer: 7B to 65B parameters\n",
      "final prompt size: 2048\n",
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer:  The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. This is crucial because raw text needs to be converted into a format that models can understand and process.\n",
      "True answer: To translate text into data that can be processed by the model.\n",
      "final prompt size: 4362\n",
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer:  The Safety Checker in the Diffusers library is a component that checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. It is designed to flag inappropriate or harmful content, helping users interact with generative models responsibly and ethically. The Safety Checker is a crucial part of the Diffusers library, ensuring that the generated content adheres to ethical guidelines.\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "final prompt size: 4843\n",
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer:  The `HfApi` class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub. This is mentioned in document 2 of the context.\n",
      "True answer: HfApi\n",
      "final prompt size: 5317\n",
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer:  The name of the new library introduced by Hugging Face for hosting scikit-learn models is not explicitly mentioned in the provided context. However, it is mentioned that the `huggingface_hub` library can be used to load a Scikit-learn model with a few lines of code.\n",
      "True answer: Skops\n",
      "final prompt size: 3906\n",
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer:  The purpose of Textual Inversion, as described in the provided context, is a training method for personalizing models by learning new text embeddings from a few example images. The resulting file is very small and the new embeddings can be loaded into the text encoder.\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "final prompt size: 3806\n",
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer:  The recommended multiple of batch size for fp16 data type on an A100 GPU is not explicitly stated in the provided context. However, it's mentioned that the Gaudi runs were performed in bfloat16 precision and the A100 runs in fp16 precision, both in single-device runs. The document also suggests experimenting with the `chunks` value to find the most efficient GPU utilization for batch sizes. Therefore, it's advisable to perform tests with different batch size multiples to determine the most efficient one for your specific use case on an A100 GPU.\n",
      "True answer: 64\n",
      "final prompt size: 4854\n",
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer:  To run a Gradio Blocks app in reload mode using a Python IDE, you need to use the `gradio` command followed by the name of your Python script and the name of your demo. If your `run.py` file contained a demo named `my_demo`, you would launch it in reload mode like this: `gradio run.py my_demo`. This will allow Gradio to automatically reload the app whenever changes are made to the file.\n",
      "True answer: Run `gradio run.py` in the terminal.\n",
      "final prompt size: 3770\n",
      "=======================================================\n",
      "Question: What command is used to install the development version of the ðŸ¤— Transformers library in a Python virtual environment?\n",
      "\n",
      "Answer:  To install the development version of the ðŸ¤— Transformers library in a Python virtual environment, you can use the following commands:\n",
      "\n",
      "```bash\n",
      "# Create and activate a virtual environment\n",
      "python -m venv .env\n",
      "source .env/bin/activate # On Linux and MacOS\n",
      ".env/Scripts/activate  # On Windows\n",
      "\n",
      "# Clone the repository and install the library in editable mode\n",
      "git clone https://github.com/huggingface/transformers.git\n",
      "cd transformers\n",
      "pip install -e .\n",
      "```\n",
      "\n",
      "This will clone the ðŸ¤— Transformers repository, switch to the desired version (in this case v3.5.1), and install the library in editable mode within the Python virtual environment. The `-e` flag in `pip install -e .` is used for an editable install, meaning any changes made to the source code will be reflected immediately without reinstalling the package.\n",
      "True answer: pip install \"transformers[sentencepiece]\"\n",
      "final prompt size: 3873\n",
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer:  To install the Hugging Face Unity API in your Unity project, follow these steps:\n",
      "\n",
      "1. Open your Unity project.\n",
      "2. Go to `Window` -> `Package Manager`.\n",
      "3. Click `+` and select `Add Package from git URL`.\n",
      "4. Enter `https://github.com/huggingface/unity-api.git`.\n",
      "5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`. \n",
      "\n",
      "This information can be found in Document 3.\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "final prompt size: 5519\n",
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer:  The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones. This encourages the model to find the most similar context vector and quantized speech unit, which is the target label.\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "final prompt size: 5501\n",
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer:  The default checkpoint used by the sentiment analysis pipeline in the Transformers library is a particular pretrained model that has been fine-tuned for sentiment analysis in English. It's downloaded and cached when you create the `classifier` object (source document 0).\n",
      "True answer: distilbert base uncased finetuned sst2 english\n",
      "final prompt size: 4505\n",
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer:  The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train/fine-tune a 1.6B-parameter GPT2-XL model for causal language modeling on Habana Gaudi.\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "final prompt size: 5596\n",
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer:  The command line module that PyTorch provides to run a script on multiple GPUs is `torchrun`. It allows you to specify the number of GPUs to use and the script to run. Here is an example:\n",
      "\n",
      "```bash\n",
      "torchrun --nproc_per_node=2 --nnodes=1 example_script.py\n",
      "```\n",
      "\n",
      "This command will run the training script on two GPUs that live on a single machine.\n",
      "True answer: torchrun\n",
      "final prompt size: 4936\n",
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer:  The information about the most popular vision transformer model for image classification on the Hugging Face Model Hub is not provided directly in the context. The context includes information about various models and resources related to computer vision and Hugging Face, but it does not contain specific data about the popularity of different models. Therefore, I cannot provide an answer to this question based on the provided context.\n",
      "True answer: google/vit-base-patch16-224\n",
      "final prompt size: 5773\n",
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer:  The command to upload an ESPnet model to a Hugging Face repository using the `run.sh` script is:\n",
      "\n",
      "```bash\n",
      "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "```\n",
      "Replace `username` and `model_repo` with your desired username and repository name.\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "final prompt size: 5224\n",
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer:  A `requirements.txt` file should be added to a model repository to install custom Python dependencies for Inference Endpoints. This file should contain the Python dependencies that you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt` file and installs the dependencies listed within.\n",
      "True answer: requirements.txt\n",
      "final prompt size: 4276\n",
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer:  According to Document 4, you need 3-5 images to teach new concepts to Stable Diffusion using Textual Inversion.\n",
      "True answer: 3-5 images\n",
      "final prompt size: 4719\n",
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer:  The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB.\n",
      "True answer: 10GB\n",
      "final prompt size: 5622\n",
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer:  The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is not explicitly mentioned in the provided context. However, W&B is a tool often used for tracking and visualizing machine learning experiments, which can include monitoring model performance, understanding the impact of hyperparameters, and comparing runs. It can help manage the ML development cycle and potentially contribute to addressing bias by providing transparency and accountability in the modeling process.\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n",
      "final prompt size: 3792\n",
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer:  The name of the open-source library created by Hugging Face to simplify Transformer acceleration is [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate). This information can be found in Document 2.\n",
      "True answer: Optimum\n",
      "final prompt size: 4529\n",
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer:  The `equal_height` parameter is used to ensure that elements in a row have the same height in Gradio. This parameter should be passed to the `.style()` method of `gr.Row()`. This information was updated in PR 3125 by freddyaboulton.\n",
      "True answer: equal_height\n",
      "final prompt size: 4236\n",
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer:  The command to install the latest version of Optimum with OpenVINO support is:\n",
      "```bash\n",
      "pip install --upgrade-strategy eager optimum[\"openvino\"]\n",
      "```\n",
      "This command will ensure that `optimum-intel` is using the latest version.\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "e4580fc87b583de8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test function with doc-agent in instinct.cpp\n",
    "\n",
    "You have to manually start `doc-agent` locally.\n",
    "\n",
    "To build knowledge index with same knowledge base data from HF:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=./data/instinct/index.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  build \\\n",
    "  --force \\\n",
    "  --file=https://huggingface.co/api/datasets/m-ric/huggingface_doc/parquet/default/train/0.parquet \\\n",
    "  --type=PARQUET \\\n",
    "  --parquet_mapping=0:txt,1:metadata:source:varchar\n",
    "```\n",
    "\n",
    "To start http server for query:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=/tmp/rag_eval.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  serve \\\n",
    "  --port=9090 \n",
    "```\n",
    "\n",
    "Next, we will begin QA tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b91ee34890b32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "4afb09c4ab37a98f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:48:58.575745Z",
     "start_time": "2024-04-17T09:48:58.537582Z"
    }
   },
   "source": [
    "def answer_with_doc_agent(question: str):\n",
    "    import requests\n",
    "    res = requests.post(\"http://localhost:9090/v1/chat/completions\", json={\"messages\": [{\"content\": question, \"role\": \"human\"}], \"stream\": False})\n",
    "    assert res.status_code == 200\n",
    "    body = res.json()\n",
    "    return body[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "\n",
    "def run_doc_agent_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = answer_with_doc_agent(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "4cceb41a4f1cbe75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T09:48:58.605035Z",
     "start_time": "2024-04-17T09:48:58.576780Z"
    }
   },
   "source": [
    "def run_doc_agent_test_all():\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"doc_agent_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "    print(f\"Running RAG with settings {settings_name}\")\n",
    "    run_doc_agent_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        output_file=output_file_name,\n",
    "        test_settings=settings_name\n",
    "    )\n",
    "    \n",
    "    return output_file_name"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "66a1467f5bfb50f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T11:22:14.119879Z",
     "start_time": "2024-04-17T11:13:07.835296Z"
    }
   },
   "source": [
    "DOC_AGENT_TEST_OUTPUT = run_doc_agent_test_all()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG with settings doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a871e87545d54e5281ba84545a24bb3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Question: What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n",
      "\n",
      "Answer:  The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. This refers to a 64-bit Intel x86 architecture running on a Linux system using musl as the C library implementation.\n",
      "True answer: x86_64-unknown-linux-musl\n",
      "=======================================================\n",
      "Question: What is the purpose of the BLIP-Diffusion model?\n",
      "\n",
      "Answer:  The BLIP-Diffusion model is a multimodal model that combines the strengths of two existing models, BLIP and Stable Diffusion. It uses the BLIP model to generate captions for images, and then uses the Stable Diffusion model to generate new images based on those captions. This allows for more creative and diverse image generation, as well as improved alignment between the generated images and their corresponding captions. The model can be used for a variety of applications, including text-to-image synthesis, image editing, and image captioning.\n",
      "True answer: The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.\n",
      "=======================================================\n",
      "Question: How can a user claim authorship of a paper on the Hugging Face Hub?\n",
      "\n",
      "Answer:  A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page and clicking on their name. They will then be redirected to the paper settings where they can confirm the request. The admin team will validate the request, and once confirmed, the Paper page will show as verified.\n",
      "True answer: By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.\n",
      "=======================================================\n",
      "Question: What is the purpose of the /healthcheck endpoint in the Datasets server API?\n",
      "\n",
      "Answer:  The `/healthcheck` endpoint in the Datasets server API checks whether the application is running or not. It returns an HTTP 200 status code if the app is up and running, and an error message otherwise. This endpoint is useful for monitoring the service's availability and ensuring it is functioning correctly.\n",
      "\n",
      "To use this endpoint, you can send a GET request to `https://datasets-server.huggingface.co/healthcheck`:\n",
      "\n",
      "<inferencesnippet>\n",
      "<python>\n",
      "```python\n",
      "import requests\n",
      "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
      "API_URL = \"https://datasets-server.huggingface.co/healthcheck\"\n",
      "def query():\n",
      "    response = requests.get(API_URL, headers=headers)\n",
      "    if response.status_code == 200:\n",
      "        print(\"The app is running!\")\n",
      "    else:\n",
      "        print(\"Error:\", response.text)\n",
      "data = query()\n",
      "```\n",
      "</python>\n",
      "<js>\n",
      "```js\n",
      "import fetch from \"node-fetch\";\n",
      "async function query(data) {\n",
      "    const response = await fetch(\n",
      "        \"https://datasets-server.huggingface.co/healthcheck\",\n",
      "        {\n",
      "            headers: { Authorization: `Bearer ${API_TOKEN}` },\n",
      "            method: \"GET\"\n",
      "        }\n",
      "    );\n",
      "    if (response.status === 200) {\n",
      "        console.log(\"The app is running!\");\n",
      "    } else {\n",
      "        const errorMessage = await response.text();\n",
      "        console.error(\"Error:\", errorMessage);\n",
      "    }\n",
      "}\n",
      "query().then((response) => {});\n",
      "```\n",
      "</js>\n",
      "<curl>\n",
      "```curl\n",
      "curl https://datasets-server.huggingface.co/healthcheck \\\n",
      "        -X GET \\\n",
      "        -H \"Authorization: Bearer ${API_TOKEN}\"\n",
      "```\n",
      "</curl>\n",
      "</inferencesnippet>\n",
      "\n",
      "If the app is running, you will see the message \"The app is running!\" in your console. If there's an issue with the service, you will receive an error message instead.\n",
      "True answer: Ensure the app is running\n",
      "=======================================================\n",
      "Question: What is the default context window size for Local Attention in the LongT5 model?\n",
      "\n",
      "Answer:  The default context window size for Local Attention in the LongT5 model is 512.\n",
      "True answer: 127 tokens\n",
      "=======================================================\n",
      "Question: What method is used to load a checkpoint for a task using `AutoPipeline`?\n",
      "\n",
      "Answer:  You can load a checkpoint for a task using the `from_pretrained()` method of the `AutoPipeline` class. This method takes as input the name or path to the pretrained weights and returns an instance of the relevant pipeline class. The `torch_dtype` parameter allows you to specify the data type used in the model, while the `use_safetensors` parameter enables the use of safetensors for loading the checkpoint.\n",
      "\n",
      "Here's an example:\n",
      "```py\n",
      "from diffusers import AutoPipelineForText2Image\n",
      "import torch\n",
      "\n",
      "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
      "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
      ").to(\"cuda\")\n",
      "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
      "\n",
      "image = pipeline(prompt, num_inference_steps=25).images[0]\n",
      "```\n",
      "True answer: from_pretrained()\n",
      "=======================================================\n",
      "Question: What is the purpose of Diffusers library?\n",
      "\n",
      "Answer:  The Diffusers library is a Python library for creating and using diffusion models, which are a type of generative model used to generate images, audio, and other types of data. The library provides a range of tools for working with diffusion models, including pre-trained models, training scripts, and utilities for visualizing and analyzing the generated data.\n",
      "\n",
      "Diffusion models work by gradually adding noise to a sample from a dataset and then learning to remove that noise in order to recover the original sample. This process is repeated many times, with the model gradually learning to denoise the samples more effectively over time. The Diffusers library provides a range of tools for working with diffusion models, including pre-trained models, training scripts, and utilities for visualizing and analyzing the generated data.\n",
      "\n",
      "One of the key benefits of the Diffusers library is its support for a wide range of diffusion models, including both classical and state-of-the-art models. The library includes models for generating images, audio, and other types of data, as well as tools for customizing and extending these models to suit your specific needs.\n",
      "\n",
      "In addition to its pre-trained models, the Diffusers library also provides a range of utilities for working with diffusion models, including tools for visualizing and analyzing the generated data, as well as scripts for training and fine-tuning the models. These utilities make it easy to get started with diffusion models and to integrate them into your own projects.\n",
      "\n",
      "Overall, the Diffusers library is a powerful tool for working with diffusion models, providing a range of pre-trained models, training scripts, and utilities for visualizing and analyzing the generated data. Whether you're new to diffusion models or an experienced user, the Diffusers library has something to offer, making it a valuable resource for anyone working with generative models.\n",
      "True answer: To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.\n",
      "=======================================================\n",
      "Question: What method does the EulerAncestralDiscreteScheduler use for sampling?\n",
      "\n",
      "Answer:  The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps.\n",
      "True answer: Ancestral sampling with Euler method steps.\n",
      "=======================================================\n",
      "Question: What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n",
      "\n",
      "Answer:  AlignVisionModel\n",
      "True answer: IDEFICS\n",
      "=======================================================\n",
      "Question: What is the purpose of the `gradio.Blocks` API?\n",
      "\n",
      "Answer:  The `gradio.Blocks` API is a high-level interface for creating user interfaces (UIs) for machine learning models in Python. It allows developers to create custom UIs with minimal code, and includes features such as support for different input and output types, real-time updates, and the ability to share UIs via links or embed them in other websites. The `Blocks` API is part of the Gradio library, which provides a range of tools for building and deploying machine learning applications.\n",
      "True answer: The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.\n",
      "=======================================================\n",
      "Question: What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n",
      "\n",
      "Answer:  The two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" involves using a pretrained text encoder to extract features from the input prompt, and then using these features to condition a diffusion model for image generation. This approach allows for more accurate and detailed image generation compared to traditional methods that directly use the prompt as input to the diffusion model. The two-stage model is used in the `pipeline_clip_latents` pipeline provided in this repository.\n",
      "True answer: The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.\n",
      "=======================================================\n",
      "Question: What command is used to install the requirements for a research project using ðŸ¤— Transformers?\n",
      "\n",
      "Answer:  The command used to install the requirements for a research project utilizing ðŸ¤— Transformers is `pip install -r requirements.txt`, which should be run inside the folder of the chosen research project.\n",
      "True answer: pip install -r requirements.txt\n",
      "=======================================================\n",
      "Question: What task does the `roberta-large-mnli` checkpoint perform?\n",
      "\n",
      "Answer:  The `roberta-large-mnli` checkpoint performs natural language inference (NLI), which involves determining whether a given hypothesis is entailed by, contradicts, or is neutral with respect to a given premise. This specific checkpoint was fine-tuned on the Multi-Genre Natural Language Inference (MNLI) dataset, which consists of pairs of sentences and corresponding NLI labels.\n",
      "True answer: Text classification\n",
      "=======================================================\n",
      "Question: What service is replacing the Paid tier of the Inference API at Hugging Face?\n",
      "\n",
      "Answer:  The Paid tier of the Hugging Face Inference API has been replaced by the Inference Endpoints, which offer a more flexible and customizable solution for running inference on Hugging Face models.\n",
      "True answer: Inference Endpoints\n",
      "=======================================================\n",
      "Question: What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n",
      "\n",
      "Answer:  SqueezeBERT uses depthwise separable convolutions in place of fully-connected layers for the Q, K, V, and FNN layers. This reduces the number of parameters and computations required while maintaining similar performance to traditional fully-connected layers.\n",
      "True answer: Grouped convolutions\n",
      "=======================================================\n",
      "Question: What type of license is the HuggingFace Team's software distributed under?\n",
      "\n",
      "Answer:  The Hugging Face team's software is licensed under the Apache License, Version 2.0.\n",
      "True answer: Apache License, Version 2.0\n",
      "=======================================================\n",
      "Question: What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n",
      "\n",
      "Answer:  The two parameter-reduction techniques proposed in the ALBERT model are cross-layer parameter sharing and factorized embedding parameterization. Cross-layer parameter sharing reduces the number of unique parameters by sharing them across multiple layers, while factorized embedding parameterization reduces the embedding size by decomposing it into smaller matrices.\n",
      "True answer: Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.\n",
      "=======================================================\n",
      "Question: What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?\n",
      "\n",
      "Answer:  The three main steps for fine-tuning a model using the ðŸ¤— (Hugging Face) Datasets library are:\n",
      "\n",
      "1. Preprocessing the data: This involves loading and preprocessing the dataset, which includes tokenization, padding, and batching.\n",
      "2. Fine-tuning the model: This involves defining the training parameters, such as the learning rate, number of epochs, and batch size, and then fine-tuning the model on the preprocessed data.\n",
      "3. Evaluating the model: This involves evaluating the performance of the fine-tuned model on a validation dataset.\n",
      "\n",
      "Here's an example notebook that demonstrates these steps using the ðŸ¤— Transformers and Datasets libraries to fine-tune a BERT model for text classification: [How to fine-tune BERT for text classification with ðŸ¤— Transformers and ðŸ¤— Datasets](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_bert.ipynb).\n",
      "True answer: 1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.\n",
      "=======================================================\n",
      "Question: What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n",
      "\n",
      "Answer:  The maximum improvement in throughput was 800%.\n",
      "True answer: +800%\n",
      "=======================================================\n",
      "Question: What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n",
      "\n",
      "Answer:  To upload a spaCy pipeline to the Hugging Face Hub, you can follow these steps:\n",
      "\n",
      "1. First, make sure you have created an account on the Hugging Face website and installed the `hf` command line interface (CLI) tool. If you haven't done so already, you can install the CLI by running `pip install huggingface_hub`.\n",
      "2. Next, save your spaCy pipeline to a directory using the `spacy.save()` method:\n",
      "```python\n",
      "import spacy\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")  # load the spaCy model you want to upload\n",
      "spacy.save(nlp, \"my_pipeline\")  # save the pipeline to a directory\n",
      "```\n",
      "3. Once you have saved your pipeline, navigate to the directory containing the saved pipeline using the command line:\n",
      "```bash\n",
      "cd my_pipeline\n",
      "```\n",
      "4. Use the `hf` CLI tool to create a new model card for your pipeline by running the following command:\n",
      "```bash\n",
      "hf models create --tag spacy\n",
      "```\n",
      "This will create a new model card in your Hugging Face account, which you can use to describe your pipeline and provide additional information.\n",
      "5. Finally, use the `hf` CLI tool to upload your pipeline to the Hugging Face Hub by running the following command:\n",
      "```bash\n",
      "hf upload --tag spacy .  # upload the contents of the current directory (i.e., the saved pipeline)\n",
      "```\n",
      "This will upload your pipeline to the Hugging Face Hub, where it can be downloaded and used by others. Note that you may need to authenticate with the CLI tool using your Hugging Face account credentials before you can upload a model.\n",
      "True answer: python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n",
      "=======================================================\n",
      "Question: What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?\n",
      "\n",
      "Answer:  The time and memory complexity of the NystrÃ¶mformer's approximation of self-attention is O(n), where n is the sequence length.\n",
      "True answer: O(n)\n",
      "=======================================================\n",
      "Question: What is the goal of the Named Entity Recognition task in token classification?\n",
      "\n",
      "Answer:  The goal of the Named Entity Recognition (NER) task in token classification is to identify and categorize named entities present in text into predefined classes such as person names, organization names, location names, medical codes, time expressions, quantities, monetary values, percentages, etc. This information can be used for various natural language processing applications like question answering, machine translation, and information extraction.\n",
      "True answer: The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.\n",
      "=======================================================\n",
      "Question: What is the resolution of images used by the CLIPSeg model?\n",
      "\n",
      "Answer:  The default resolution for images used in the CLIPSeg model is 256x256 pixels. However, this can be changed during initialization of the `CLIPSegModel` or `CLIPSegForImageSegmentation` class by providing a different value for the `image_resolution` argument.\n",
      "True answer: 352 x 352 pixels\n",
      "=======================================================\n",
      "Question: What can you use Gradio for?\n",
      "\n",
      "Answer:  Gradio can be used for creating user interfaces (UIs) for machine learning models. It allows developers to quickly build web-based demos, share their models with others, and gather user feedback in an accessible and interactive way. This makes it a great tool for showcasing the capabilities of your models.\n",
      "True answer: Create a demo for your machine learning model, share your machine learning model with others, and debug your model.\n",
      "=======================================================\n",
      "Question: What TensorFlow API function is used to load a saved tensor file?\n",
      "\n",
      "Answer:  The TensorFlow API function used to load a saved tensor file is `tf.saved_model.load`. It loads a SavedModel format, which contains the model's architecture and trained weights. This allows you to restore a previously saved TensorFlow model for further use or inference.\n",
      "True answer: safetensors.tensorflow.load_file\n",
      "=======================================================\n",
      "Question: Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n",
      "\n",
      "Answer:  To access the logs of your Endpoints in Hugging Face Endpoints, you can go to the \"Logs\" tab on the endpoint's page on the Hugging Face Hub. There, you will find a list of all the requests made to your endpoint, along with their status and any error messages that were returned. You can also filter the logs by time range, request method, and status code.\n",
      "True answer: In the \"Logs\" tab of your Endpoint through the UI.\n",
      "=======================================================\n",
      "Question: What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n",
      "\n",
      "Answer:  The most recent task added to Hugging Face's AutoTrain for Computer Vision is Image Classification.\n",
      "True answer: Image Classification\n",
      "=======================================================\n",
      "Question: What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n",
      "\n",
      "Answer:  The default repository type created by the `create_repo` function when used on the Hugging Face Hub is a model card.\n",
      "True answer: model\n",
      "=======================================================\n",
      "Question: How many splits does the \"duorc\" dataset have?\n",
      "\n",
      "Answer:  The \"duorc\" dataset has 6 splits.\n",
      "True answer: Six\n",
      "=======================================================\n",
      "Question: What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n",
      "\n",
      "Answer:  Fully Sharded Data Parallel (FSDP) is a technique used in distributed training to shard the model parameters, optimizer and gradient states, and even offload them to the CPU when they're inactive. This reduces the high cost of large-scale training by allowing you to train really large models on multiple GPUs or TPUs. FSDP can be a powerful tool for reducing memory usage and increasing throughput in distributed training.\n",
      "True answer: FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.\n",
      "=======================================================\n",
      "Question: What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n",
      "\n",
      "Answer:  The TorchScript (`.pt`) format is a more secure way to save and store PyTorch model weights compared to the `.bin` format. It provides better protection against unauthorized access and use by encrypting the saved file with a password, making it harder for someone to steal or tamper with the model. The TorchScript format also includes additional metadata about the model, such as its name, version, and description, which can be useful for tracking and managing different versions of the same model.\n",
      "\n",
      "To save a PyTorch model in the TorchScript format, you can use the `torch.jit.save` function:\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "checkpoint = \"facebook/opt-13b\"\n",
      "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16)\n",
      "\n",
      "# Save the model in TorchScript format with a password\n",
      "torch.jit.save(model, \"my_model.pt\", password=\"secret\")\n",
      "```\n",
      "To load a PyTorch model from the TorchScript format, you can use the `torch.jit.load` function:\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load the model from the TorchScript format with a password\n",
      "model = torch.jit.load(\"my_model.pt\", map_location=\"cpu\", password=\"secret\")\n",
      "```\n",
      "Note that loading a PyTorch model from the TorchScript format requires the use of a CPU device, since the TorchScript format does not support GPU devices or offloading to disk. If you want to run the model on a GPU or offload its weights to disk, you will need to convert it to the `.bin` format using the `torch.jit.script` function:\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Convert the model to TorchScript format with a password\n",
      "scripted_model = torch.jit.script(model, password=\"secret\")\n",
      "\n",
      "# Save the model in TorchScript format with a password\n",
      "torch.jit.save(scripted_model, \"my_model.pt\", password=\"secret\")\n",
      "```\n",
      "Then you can load and run the model as usual:\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load the model from the TorchScript format with a password\n",
      "scripted_model = torch.jit.load(\"my_model.pt\", map_location=\"cpu\", password=\"secret\")\n",
      "model = AutoModelForCausalLM.from_pretrained(scripted_model, device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch.float16)\n",
      "```\n",
      "True answer: `.safetensors`\n",
      "=======================================================\n",
      "Question: What type of security certification does Hugging Face have?\n",
      "\n",
      "Answer:  Hugging Face is SOC2 Type 2 certified. This means that they provide security certification to their customers and actively monitor and patch any security weaknesses.\n",
      "True answer: SOC2 Type 2 certified\n",
      "=======================================================\n",
      "Question: What do RAG models combine to generate outputs?\n",
      "\n",
      "Answer:  RAG models combine two main components: a retriever and a generator. The retriever is responsible for retrieving relevant documents from a knowledge source (such as a corpus of text) based on a given input query. The generator then uses the retrieved documents, along with the input query, to generate an output sequence. This allows RAG models to generate more accurate and informative outputs by leveraging external knowledge beyond what is contained in their training data.\n",
      "True answer: Pretrained dense retrieval (DPR) and sequence-to-sequence models.\n",
      "=======================================================\n",
      "Question: What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n",
      "\n",
      "Answer:  Beautiful Soup is used by MarkupLMFeatureExtractor to extract data from HTML and XML files.\n",
      "True answer: Beautiful Soup\n",
      "=======================================================\n",
      "Question: What is the file size limit for syncing to HF Spaces without using Git-LFS?\n",
      "\n",
      "Answer:  The file size limit for syncing files to HF Spaces without utilizing Git-LFS is 10MB.\n",
      "True answer: 10MB\n",
      "=======================================================\n",
      "Question: What is the title of the paper introducing the ByT5 model?\n",
      "\n",
      "Answer:  The title of the paper that introduces the ByT5 model is \"ByT5: Towards a Unified Text-to-Text Transformer\".\n",
      "True answer: ByT5: Towards a token-free future with pre-trained byte-to-byte models\n",
      "=======================================================\n",
      "Question: What is the dimension of the feature vector for the base BERT model?\n",
      "\n",
      "Answer:  The dimension of the feature vector for the base BERT model is 768.\n",
      "True answer: 768\n",
      "=======================================================\n",
      "Question: What special identifier does the WordPiece Model use for continuing subwords?\n",
      "\n",
      "Answer:  The special identifier that the WordPiece model uses for continuing subwords is `##`. This symbol is added at the beginning of each subword token that is not the first one in a word, to indicate that it is a continuation of the previous subword. For example, if a WordPiece tokenizer breaks down the word \"unexpectedly\" into the subwords \"un\", \"##expect\", and \"##edly\", then the encoded form of this word would be `un unexpected ##edy`.\n",
      "True answer: ##\n",
      "=======================================================\n",
      "Question: What is the purpose of the ðŸ§¨ Diffusers tutorials?\n",
      "\n",
      "Answer:  The ðŸ§¨ Diffusers tutorials aim to provide clear, concise, and easy-to-follow examples that demonstrate how to use Hugging Face's Diffusers library for diffusion models in various applications. These tutorials cover a wide range of topics, including text generation, image generation, and audio processing, among others. The tutorials are designed to help users quickly get started with the library by providing practical examples that can be easily adapted to their specific use cases.\n",
      "\n",
      "The Diffusers tutorials also aim to showcase the versatility and flexibility of diffusion models, which have gained popularity in recent years for their ability to generate high-quality samples from complex data distributions. By providing a wide range of examples, the tutorials hope to inspire users to explore new applications and use cases for diffusion models, and to encourage further research and development in this exciting field.\n",
      "\n",
      "Overall, the ðŸ§¨ Diffusers tutorials aim to provide a valuable resource for anyone interested in learning about diffusion models and how they can be used in practice. Whether you are a seasoned machine learning practitioner or just starting out, these tutorials offer a wealth of information and practical examples that can help you get up and running with diffusion models in no time.\n",
      "True answer: To provide a gentle introduction to diffusion models and help understand the library fundamentals.\n",
      "=======================================================\n",
      "Question: What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n",
      "\n",
      "Answer:  The default value for the `allow_flagging` parameter in a Gradio `Interface` is `False`.\n",
      "True answer: \"manual\"\n",
      "=======================================================\n",
      "Question: Where can the full code for the Stable Diffusion demo be found?\n",
      "\n",
      "Answer:  The full code for the Stable Diffusion demo is available in this Colab notebook: <https://colab.research.google.com/github/huggingface/diffusers/blob/main/notebooks/StableDiffusion_Demo.ipynb>\n",
      "\n",
      "This notebook includes examples of how to use the Stable Diffusion model for text-to-image generation, image editing, and more. It also covers various techniques for optimizing inference time and reducing memory usage.\n",
      "True answer: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n",
      "=======================================================\n",
      "Question: What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n",
      "\n",
      "Answer:  The FNet model uses a Fourier Transform that returns only the real parts of the transform to replace the self-attention layer in a BERT model.\n",
      "True answer: Fourier transform\n",
      "=======================================================\n",
      "Question: What type of test should typically accompany a bug fix in Gradio's testing strategy?\n",
      "\n",
      "Answer:  Unit tests are typically recommended to accompany a bug fix in Gradio's testing strategy. These tests focus on individual components or functions, making it easier to identify the root cause of any issues that arise. Additionally, unit tests can be run quickly and efficiently, allowing developers to catch and address bugs early in the development process.\n",
      "True answer: Dynamic code test\n",
      "=======================================================\n",
      "Question: How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?\n",
      "\n",
      "Answer:  To force mixed precision training when initializing the Accelerator in Hugging Face's Accelerate library, you can pass `mixed_precision=\"fp16\"` as an argument to the `Accelerator` constructor. Here is an example:\n",
      "```python\n",
      "from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
      "```\n",
      "This will enable mixed precision training using the FP16 data type, which can help reduce memory usage and improve training speed. Note that you may need to make additional changes to your code to ensure it is compatible with mixed precision training. For more information, please refer to the [Accelerate documentation](https://huggingface.co/docs/accelerate/main/en/usage_guides/training#mixed-precision).\n",
      "True answer: By passing `fp16=True` to the Accelerator init.\n",
      "=======================================================\n",
      "Question: What is the range of parameters for the LLaMA models?\n",
      "\n",
      "Answer:  The LLaMA models have a range of parameters from 7B to 65B.\n",
      "True answer: 7B to 65B parameters\n",
      "=======================================================\n",
      "Question: What is the purpose of tokenizers in the NLP pipeline?\n",
      "\n",
      "Answer:  Tokenizers are used to break up text into smaller units called tokens, which can be words, subwords, or even individual characters. This process is a crucial step in NLP pipelines because it allows models to work with discrete units of meaning that can be more easily processed and analyzed. By breaking text down into tokens, tokenizers help to simplify the input data for NLP models, making it easier for them to learn patterns and relationships between words and phrases.\n",
      "\n",
      "Tokenization is also an important step in preparing text data for use in machine learning algorithms, as it allows for more efficient processing of large datasets. By breaking up text into smaller units, tokenizers can help reduce the dimensionality of the input data, making it easier to train models on large datasets without running out of memory or taking too long to process.\n",
      "\n",
      "Overall, tokenization is a critical step in NLP pipelines that helps to simplify and streamline the processing of text data for use in machine learning algorithms. By breaking up text into smaller units, tokenizers enable models to more easily learn patterns and relationships between words and phrases, making it possible to build powerful NLP applications that can understand and analyze human language with remarkable accuracy.\n",
      "True answer: To translate text into data that can be processed by the model.\n",
      "=======================================================\n",
      "Question: What is the purpose of the Safety Checker in the Diffusers library?\n",
      "\n",
      "Answer:  The Safety Checker in the Diffusers library is used to ensure that the generated images are safe and appropriate for all audiences. It checks for any explicit or harmful content in the generated images and prevents them from being saved or displayed. This feature helps to maintain a safe and respectful environment for users of the library.\n",
      "True answer: The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.\n",
      "=======================================================\n",
      "Question: What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n",
      "\n",
      "Answer:  The `HfApi` class, which is part of the `huggingface_hub` library, can be used to interact with the Hugging Face Hub. To retrieve Discussions and Pull Requests from a specific repository, you can use the `get_discussion` and `list_pulls` methods respectively, both of which are available in the `HfApi` class.\n",
      "\n",
      "Here's an example:\n",
      "\n",
      "```python\n",
      "from huggingface_hub import HfApi\n",
      "\n",
      "api = HfApi()\n",
      "repo_id = \"huggingface/transformers\"  # replace with your desired repository ID\n",
      "\n",
      "# Retrieve a specific discussion by its ID\n",
      "discussion_id = \"1738095264\"\n",
      "discussion = api.get_discussion(repo_id, discussion_id)\n",
      "print(discussion.title)\n",
      "\n",
      "# List all pull requests for the repository\n",
      "pulls = api.list_pulls(repo_id)\n",
      "for pr in pulls:\n",
      "    print(pr.title)\n",
      "```\n",
      "\n",
      "In this example, replace `huggingface/transformers` with your desired repository ID and modify the discussion and pull request IDs as needed.\n",
      "True answer: HfApi\n",
      "=======================================================\n",
      "Question: What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n",
      "\n",
      "Answer:  skops\n",
      "True answer: Skops\n",
      "=======================================================\n",
      "Question: What is the purpose of Textual Inversion?\n",
      "\n",
      "Answer:  Textual Inversion is a method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n",
      "True answer: Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.\n",
      "=======================================================\n",
      "Question: What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n",
      "\n",
      "Answer:  The recommended multiple of batch size for fp16 data type on an A100 GPU is 2. This means that if you have a batch size of 4 for fp32, you should use a batch size of 8 for fp16 to fully utilize the available VRAM.\n",
      "True answer: 64\n",
      "=======================================================\n",
      "Question: How do you run a Gradio Blocks app in reload mode using a Python IDE?\n",
      "\n",
      "Answer:  To run a Gradio Blocks app in reload mode using a Python IDE, you can use the `gradio_client` library to connect to your local Gradio server and then use the `reload` function from the `IPython` library to automatically reload your code when it changes. Here's an example:\n",
      "\n",
      "1. Start your Gradio Blocks app in reload mode by running the following command in a terminal:\n",
      "```python\n",
      "import gradio as gr\n",
      "\n",
      "def greet(name):\n",
      "    return \"Hello \" + name + \"!\"\n",
      "\n",
      "blocks = gr.Blocks()\n",
      "blocks.greet_button, blocks.greet_textbox = blocks.buttons(\"Greet\", \"Click to greet\"), blocks.textbox(\"\")\n",
      "blocks.greet_button.click(greet, blocks.greet_textbox)\n",
      "blocks.launch(reload=True)  # Start the server in reload mode\n",
      "```\n",
      "2. In your Python IDE, run the following code to connect to your local Gradio server and automatically reload your code when it changes:\n",
      "```python\n",
      "import gradio_client as grc\n",
      "import ipykernel\n",
      "from traitlets import TraitSet\n",
      "\n",
      "# Connect to the local Gradio server\n",
      "app = grc.connect('http://localhost:7865')\n",
      "\n",
      "# Set up reloading of code when it changes\n",
      "code = \"\"\"\n",
      "import gradio_client as grc\n",
      "\n",
      "def on_reload():\n",
      "    app.reload()\n",
      "\n",
      "ipykernel.getfigid().on_reload(on_reload)\n",
      "\"\"\"\n",
      "\n",
      "# Run the code in a separate kernel to avoid conflicts with your main kernel\n",
      "ipykernel.connect(code=code, fname='gradio_reload.py')\n",
      "```\n",
      "This will automatically reload your Gradio Blocks app whenever you make changes to your code. Note that this approach only works when running the Gradio server locally on your machine. If you're hosting your Gradio app on a remote server or using a different URL, you'll need to modify the `connect` function accordingly.\n",
      "True answer: Run `gradio run.py` in the terminal.\n",
      "=======================================================\n",
      "Question: What command is used to install the development version of the ðŸ¤— Transformers library in a Python virtual environment?\n",
      "\n",
      "Answer:  To install the development version of the ðŸ¤— Transformers library in a Python virtual environment, you can use the `pip` package manager with the following command:\n",
      "```\n",
      "pip install \"transformers[sentencepiece]\"\n",
      "```\n",
      "This will install the development version of the library along with all the required dependencies.\n",
      "True answer: pip install \"transformers[sentencepiece]\"\n",
      "=======================================================\n",
      "Question: How can you install the Hugging Face Unity API in your Unity project?\n",
      "\n",
      "Answer:  To install the Hugging Face Unity API in your Unity project, follow these steps:\n",
      "1. Open your Unity project.\n",
      "2. Go to `Window` > `Package Manager`.\n",
      "3. Click on the `+` button and select `Add Package from git URL`.\n",
      "4. Enter the following URL: `https://github.com/huggingface/unity-api.git`.\n",
      "5. Once installed, open the Hugging Face API Wizard by going to `Window` > `Hugging Face API Wizard`.\n",
      "6. Enter your API key (create one in your [Hugging Face account settings](https://huggingface.co/settings/tokens)).\n",
      "7. Test the API key and configure advanced settings if needed.\n",
      "8. Optionally, install examples for reference.\n",
      "\n",
      "Now you can use the Hugging Face Unity API to access and use Hugging Face AI models in your Unity project.\n",
      "True answer: To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.\n",
      "=======================================================\n",
      "Question: What is the pretraining objective of the Wav2Vec2 context network?\n",
      "\n",
      "Answer:  The pretraining objective of the Wav2Vec2 context network is contrastive predictive coding (CPC). CPC involves training a neural network to predict future audio samples based on past audio samples. Specifically, the network is trained to distinguish between true future audio samples and negative samples that are generated by randomly sampling from other parts of the input audio. This helps the network learn meaningful representations of the audio data that can be used for downstream tasks such as automatic speech recognition (ASR).\n",
      "True answer: The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.\n",
      "=======================================================\n",
      "Question: What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n",
      "\n",
      "Answer:  The default checkpoint for the sentiment analysis pipeline in the Transformers library is \"finiteautomata/bertweet-base-sentiment-analysis\".\n",
      "True answer: distilbert base uncased finetuned sst2 english\n",
      "=======================================================\n",
      "Question: What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n",
      "\n",
      "Answer:  The purpose of the \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" notebook is to demonstrate how to use DeepSpeed, a distributed deep learning training framework, to train large models with billions of parameters on Habana Gaudi. This notebook provides an example of how to use DeepSpeed with Hugging Face's Transformers library and Habana's SynapseAI software development kit (SDK) to fine-tune the T5-3B text summarization model on the CNN DailyMail dataset. The results show that Gaudi2 is 2.44x faster than A100 80GB for this task.\n",
      "True answer: To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.\n",
      "=======================================================\n",
      "Question: What command line module does PyTorch provide to run a script on multiple GPUs?\n",
      "\n",
      "Answer:  `torch.distributed.launch`\n",
      "True answer: torchrun\n",
      "=======================================================\n",
      "Question: What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n",
      "\n",
      "Answer:  The most popular vision transformer model for image classification on the Hugging Face Model Hub is `google/vit-base-patch16-224`. It has been starred over 3,000 times and has been used in numerous projects. This model is based on the Vision Transformer (ViT) architecture proposed in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" by Alexey Dosovitskiy et al. It has a base configuration with 12 transformer layers, a hidden size of 768, and 12 attention heads. The model takes patches of size 16x16 from the input image and linearly embeds them into a sequence of vectors that are fed into the transformer encoder.\n",
      "True answer: google/vit-base-patch16-224\n",
      "=======================================================\n",
      "Question: What is the command to upload an ESPnet model to a Hugging Face repository?\n",
      "\n",
      "Answer:  To upload an ESPnet model to a Hugging Face repository, you can follow these steps:\n",
      "\n",
      "1. First, create a new repository on the Hugging Face Hub for your model. You can do this by going to the Hugging Face Hub website and clicking on the \"New Model\" button. Give your repository a name and a short description of what it contains.\n",
      "2. Next, clone the repository to your local machine using Git. This will allow you to make changes to the repository and push them back to the Hugging Face Hub.\n",
      "3. Once you have cloned the repository, you can add your ESPnet model files to the repository. Make sure to include a `model.py` file that defines the model architecture, as well as any other necessary files such as tokenizer files or vocabulary files.\n",
      "4. After adding your model files to the repository, you can commit the changes and push them back to the Hugging Face Hub using Git. This will make the model available for others to use and download from the Hugging Face Hub.\n",
      "5. Finally, you can create a model card for your model by adding a `README.md` file to the repository. The model card should include information about the model, such as its architecture, training data, and performance metrics. You can use the [Hugging Face Model Card template](https://huggingface.co/docs/transformers/model_card) as a guide for what to include in your model card.\n",
      "\n",
      "Once you have uploaded your ESPnet model and created a model card, others will be able to find and use your model on the Hugging Face Hub.\n",
      "True answer: ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n",
      "=======================================================\n",
      "Question: What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n",
      "\n",
      "Answer:  A `requirements.txt` file should be added to a model repository in order to install custom Python dependencies for Inference Endpoints. This file should list all the required dependencies, one per line. When the endpoint and image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt` file and installs the dependencies listed within.\n",
      "True answer: requirements.txt\n",
      "=======================================================\n",
      "Question: How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n",
      "\n",
      "Answer:  The number of images required to teach new concepts to Stable Diffusion through the process of Textual Inversion can vary depending on the complexity and specificity of the concept. Generally, a few dozen to a few hundred images are used in the training process for each concept. However, this is not a strict rule and the number can be adjusted based on the desired results. It's important to note that more images do not always lead to better results, as the quality of the images and their relevance to the concept also play crucial roles in the learning process.\n",
      "True answer: 3-5 images\n",
      "=======================================================\n",
      "Question: What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n",
      "\n",
      "Answer:  The maximum size of a model checkpoint at which automatic sharding occurs in Transformers version 4.18.0 is 10GB.\n",
      "True answer: 10GB\n",
      "=======================================================\n",
      "Question: What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n",
      "\n",
      "Answer:  Weights and Biases (W&B) is a tool that helps data scientists and machine learning scientists track, visualize, and compare experiments and model performance. It allows users to log metrics, hyperparameters, code versions, and other relevant information about their models and datasets. W&B also provides tools for comparing different runs, visualizing the training process, and collaborating with team members. By using W&amp;B, data scientists and machine learning scientists can more easily manage and optimize their models, improve reproducibility, and share their results with others.\n",
      "True answer: To track their machine learning experiments at every stage, from training to production.\n",
      "=======================================================\n",
      "Question: What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n",
      "\n",
      "Answer:  The name of the open-source library created by Hugging Face for Transformer acceleration is `Transformers.js`.\n",
      "True answer: Optimum\n",
      "=======================================================\n",
      "Question: What parameter is used to ensure that elements in a row have the same height in Gradio?\n",
      "\n",
      "Answer:  The `elem_id` parameter can be used in Gradio to set the same height for all elements in a row. This parameter allows you to specify the ID of an HTML element, and any other elements with the same ID will have their height adjusted to match. For example:\n",
      "```\n",
      "gr.Interface(my_function, inputs=\"text\", outputs=\"text\").launch(elem_id=\"same-height\")\n",
      "```\n",
      "This will set the height of all text input elements in the interface to be the same. Note that this parameter is only available in Gradio version 3.0 and later.\n",
      "True answer: equal_height\n",
      "=======================================================\n",
      "Question: What is the command to install the latest version of Optimum with OpenVINO support?\n",
      "\n",
      "Answer:  To install the most recent version of Optimum that includes OpenVINO support, you can use the following pip command:\n",
      "```\n",
      "pip install optimum[openvino]\n",
      "```\n",
      "This will install the `optimum` package along with the necessary dependencies for OpenVINO support.\n",
      "True answer: pip install --upgrade-strategy eager optimum[\"openvino\"]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "fdc11c2c8edc69d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "id": "31856cff24422023",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T11:24:36.774530Z",
     "start_time": "2024-04-17T11:24:36.742942Z"
    }
   },
   "source": [
    "from ratelimit import limits,sleep_and_retry\n",
    "from retry import retry\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=6, period=60)\n",
    "def throttled_invoke(eval_chat_model, eval_prompt):\n",
    "    return eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "\n",
    "\n",
    "@retry(exceptions=Exception, tries=6)\n",
    "def evaluate_single_answer(\n",
    "        evaluation_prompt_template: ChatPromptTemplate,\n",
    "        experiment: dict,\n",
    "        throttled:bool,\n",
    "        eval_chat_model: BaseChatModel\n",
    "):\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "    if throttled:\n",
    "        eval_result = throttled_invoke(eval_chat_model, eval_prompt)\n",
    "    else:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    splits = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    if len(splits) != 2:\n",
    "        print(splits)\n",
    "        raise Exception(\"Evaluation did not complete successfully\")\n",
    "    assert 1 <= int(splits[1]) <= 5\n",
    "    return splits\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    "    throttled:bool = True\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment and experiment[f\"eval_score_{evaluator_name}\"]:\n",
    "            continue\n",
    "        \n",
    "        splits = evaluate_single_answer(evaluation_prompt_template, experiment, throttled, eval_chat_model)\n",
    "        \n",
    "        if len(splits) != 2:\n",
    "            print(splits)\n",
    "            # experiment[f\"eval_score_{evaluator_name}\"] = \"\"\n",
    "            # experiment[f\"eval_feedback_{evaluator_name}\"] = \"\"\n",
    "            continue\n",
    "        feedback, score = splits \n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "c193a75198dabe56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T11:24:38.222505Z",
     "start_time": "2024-04-17T11:24:38.188818Z"
    }
   },
   "source": [
    "EVALUATION_PROMPT = \"\"\" You are a fair evaluator language model.\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "292b22f2978e2d2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "id": "83e219af8e9a45c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T11:27:09.076279Z",
     "start_time": "2024-04-17T11:24:40.357430Z"
    }
   },
   "source": [
    "def generate_eval_results():\n",
    "    import glob\n",
    "    for output_file_name in glob.glob(\"./output/*.json\"):\n",
    "        print(f\"Evaluating {output_file_name}\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            EVAL_MODEL,\n",
    "            EVALUATOR_NAME,\n",
    "            EVALUATION_PROMPT_TEMPLATE,\n",
    "            # throttling is not needed for local model\n",
    "            False\n",
    "        )\n",
    "\n",
    "generate_eval_results()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "096a41839c5649b6b3ec65719906e339"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d5d486c02684a7daaed8588ad48f1a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ./output/huggingface_doc_splits.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c6813794d61418c822def6584b7b38e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 14\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m         evaluate_answers(\n\u001B[1;32m      6\u001B[0m             output_file_name,\n\u001B[1;32m      7\u001B[0m             EVAL_MODEL,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m             \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     12\u001B[0m         )\n\u001B[0;32m---> 14\u001B[0m generate_eval_results()\n",
      "Cell \u001B[0;32mIn[28], line 5\u001B[0m, in \u001B[0;36mgenerate_eval_results\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output_file_name \u001B[38;5;129;01min\u001B[39;00m glob\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./output/*.json\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m     evaluate_answers(\n\u001B[1;32m      6\u001B[0m         output_file_name,\n\u001B[1;32m      7\u001B[0m         EVAL_MODEL,\n\u001B[1;32m      8\u001B[0m         EVALUATOR_NAME,\n\u001B[1;32m      9\u001B[0m         EVALUATION_PROMPT_TEMPLATE,\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;66;03m# throttling is not needed for local model\u001B[39;00m\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     )\n",
      "Cell \u001B[0;32mIn[26], line 52\u001B[0m, in \u001B[0;36mevaluate_answers\u001B[0;34m(answer_path, eval_chat_model, evaluator_name, evaluation_prompt_template, throttled)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevaluator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m experiment \u001B[38;5;129;01mand\u001B[39;00m experiment[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval_score_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevaluator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m splits \u001B[38;5;241m=\u001B[39m evaluate_single_answer(evaluation_prompt_template, experiment, throttled, eval_chat_model)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(splits) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28mprint\u001B[39m(splits)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/decorator.py:232\u001B[0m, in \u001B[0;36mdecorate.<locals>.fun\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[1;32m    231\u001B[0m     args, kw \u001B[38;5;241m=\u001B[39m fix(args, kw, sig)\n\u001B[0;32m--> 232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m caller(func, \u001B[38;5;241m*\u001B[39m(extras \u001B[38;5;241m+\u001B[39m args), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/retry/api.py:73\u001B[0m, in \u001B[0;36mretry.<locals>.retry_decorator\u001B[0;34m(f, *fargs, **fkwargs)\u001B[0m\n\u001B[1;32m     71\u001B[0m args \u001B[38;5;241m=\u001B[39m fargs \u001B[38;5;28;01mif\u001B[39;00m fargs \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m()\n\u001B[1;32m     72\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m fkwargs \u001B[38;5;28;01mif\u001B[39;00m fkwargs \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mdict\u001B[39m()\n\u001B[0;32m---> 73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m __retry_internal(partial(f, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), exceptions, tries, delay, max_delay, backoff, jitter,\n\u001B[1;32m     74\u001B[0m                         logger)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/insintct-cpp/lib/python3.11/site-packages/retry/api.py:33\u001B[0m, in \u001B[0;36m__retry_internal\u001B[0;34m(f, exceptions, tries, delay, max_delay, backoff, jitter, logger)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m _tries:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f()\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     35\u001B[0m         _tries \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[26], line 20\u001B[0m, in \u001B[0;36mevaluate_single_answer\u001B[0;34m(evaluation_prompt_template, experiment, throttled, eval_chat_model)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;129m@retry\u001B[39m(exceptions\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mException\u001B[39;00m, tries\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_single_answer\u001B[39m(\n\u001B[1;32m     14\u001B[0m         evaluation_prompt_template: ChatPromptTemplate,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m         eval_chat_model: BaseChatModel\n\u001B[1;32m     18\u001B[0m ):\n\u001B[1;32m     19\u001B[0m     eval_prompt \u001B[38;5;241m=\u001B[39m evaluation_prompt_template\u001B[38;5;241m.\u001B[39mformat_messages(\n\u001B[0;32m---> 20\u001B[0m             instruction\u001B[38;5;241m=\u001B[39mexperiment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     21\u001B[0m             response\u001B[38;5;241m=\u001B[39mexperiment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_answer\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     22\u001B[0m             reference_answer\u001B[38;5;241m=\u001B[39mexperiment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue_answer\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     23\u001B[0m         )\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m throttled:\n\u001B[1;32m     25\u001B[0m         eval_result \u001B[38;5;241m=\u001B[39m throttled_invoke(eval_chat_model, eval_prompt)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'question'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "b157cef7583830b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T11:43:10.340092Z",
     "start_time": "2024-04-17T11:43:10.291733Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_eval_results():\n",
    "    import glob\n",
    "    outputs = []\n",
    "    for file in glob.glob(\"./output/*.json\"):\n",
    "        output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "        output[\"settings\"] = file\n",
    "        outputs.append(output)\n",
    "    return pd.concat(outputs)\n",
    "\n",
    "EVAL_RESULTS = load_eval_results()\n",
    "display(EVAL_RESULTS)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                               question  \\\n",
       "0                                           What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n   \n",
       "1                                                                    What is the purpose of the BLIP-Diffusion model?\\n   \n",
       "2                                                 How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n",
       "3                                        What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n   \n",
       "4                                    What is the default context window size for Local Attention in the LongT5 model?\\n   \n",
       "..                                                                                                                  ...   \n",
       "62  What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n   \n",
       "63               What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\\n   \n",
       "64          What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n   \n",
       "65                            What parameter is used to ensure that elements in a row have the same height in Gradio?\\n   \n",
       "66                                What is the command to install the latest version of Optimum with OpenVINO support?\\n   \n",
       "\n",
       "                                                                                                                                                         true_answer  \\\n",
       "0                                                                                                                                          x86_64-unknown-linux-musl   \n",
       "1                                                                        The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.   \n",
       "2   By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n",
       "3                                                                                                                                          Ensure the app is running   \n",
       "4                                                                                                                                                         127 tokens   \n",
       "..                                                                                                                                                               ...   \n",
       "62                                                                                                                                                              10GB   \n",
       "63                                                                          To track their machine learning experiments at every stage, from training to production.   \n",
       "64                                                                                                                                                           Optimum   \n",
       "65                                                                                                                                                      equal_height   \n",
       "66                                                                                                          pip install --upgrade-strategy eager optimum[\"openvino\"]   \n",
       "\n",
       "                                                                                            source_doc  \\\n",
       "0                          huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md   \n",
       "1                       huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md   \n",
       "2                                               huggingface/hub-docs/blob/main/docs/hub/paper-pages.md   \n",
       "3                                         huggingface/datasets-server/blob/main/services/api/README.md   \n",
       "4                                huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md   \n",
       "..                                                                                                 ...   \n",
       "62                                     huggingface/transformers/blob/main/docs/source/en/big_models.md   \n",
       "63  gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md   \n",
       "64                                                                 huggingface/blog/blob/main/intel.md   \n",
       "65              gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md   \n",
       "66                            huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  generated_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                       Based on the provided context, there is no information given about the purpose of the BLIP-Diffusion model. The context mainly discusses Stable Diffusion and DDPO models.   \n",
       "2                                                                                                                                                                                                                                A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page, clicking on their name in the corresponding Paper page, and then clicking \"claim authorship\". This will redirect them to their paper settings where they can confirm the request. The admin team will validate this request, and once confirmed, the Paper page will show as verified.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                     The `/healthcheck` endpoint in the Datasets server API is used to ensure that the application is running. It's a basic health check for the server's status.   \n",
       "4                                                                                                                                                                                                                              The default context window size for Local Attention in the LongT5 model is defined by `config.attention_window`. It can be of type `List` to define a different window size for each layer. For more specifics on the default value, it would be best to refer to the actual implementation or documentation as it could vary depending on the specific configuration of the model.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The maximum size of a model checkpoint at which automatic sharding occurs in Transformers version 4.18.0 is 10GB.   \n",
       "63   Weights and Biases (W&B) is a tool that helps data scientists and machine learning scientists track, visualize, and compare experiments and model performance. It allows users to log metrics, hyperparameters, code versions, and other relevant information about their models and datasets. W&B also provides tools for comparing different runs, visualizing the training process, and collaborating with team members. By using W&amp;B, data scientists and machine learning scientists can more easily manage and optimize their models, improve reproducibility, and share their results with others.   \n",
       "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The name of the open-source library created by Hugging Face for Transformer acceleration is `Transformers.js`.   \n",
       "65                                                                              The `elem_id` parameter can be used in Gradio to set the same height for all elements in a row. This parameter allows you to specify the ID of an HTML element, and any other elements with the same ID will have their height adjusted to match. For example:\\n```\\ngr.Interface(my_function, inputs=\"text\", outputs=\"text\").launch(elem_id=\"same-height\")\\n```\\nThis will set the height of all text input elements in the interface to be the same. Note that this parameter is only available in Gradio version 3.0 and later.   \n",
       "66                                                                                                                                                                                                                                                                                                                                              To install the most recent version of Optimum that includes OpenVINO support, you can use the following pip command:\\n```\\npip install optimum[openvino]\\n```\\nThis will install the `optimum` package along with the necessary dependencies for OpenVINO support.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              retrieved_docs  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\">\\n    <br>\\n    <img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/>\\n    <br>\\n<p>\\n<p align=\"center\">\\n    <img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\">\\n    <a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://docs.rs/tokenizers/\">\\n        <img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\">\\n    </a>\\n</p>\\n<br>\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`, `tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, Stable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/)., In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., We've gone from the basic use of Stable Diffusion using ðŸ¤— Hugging Face Diffusers to more advanced uses of the library, and we tried to introduce all the pieces in a modern diffusion system. If you liked this topic and want to learn more, we recommend the following resources:\\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\\n- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, that gives a broader overview on Diffusion systems.\\n- The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post., In order to make it easy for everyone to take advantage of these improvements, we have converted the four official Stable Diffusion models and pushed them to the [Hub](https://huggingface.co/apple). These are all the variants:]   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\\n</div>\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., <div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\\n</div>\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n> Looking for a good first issue to work on?\\n> Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n> Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hubâ€™s Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Faceâ€™s models on the Hugging Face Hub have an associated model card on the Hub[^8]., --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\"), You can also revoke a user's membership or change their role on this page.\\n\\n## Organization domain name\\n\\nUnder the **Account** tab in the Organization settings, you can set an **Organization domain name**. Specifying a domain name will allow any user with a matching email address on the Hugging Face Hub to join your organization.]   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [Datasets server API - rows endpoint\\n\\n> /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&config={config}`\\n  - `split`: `?dataset={dataset}&config={config}&split={split}`, Datasets server - worker\\n\\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus, - /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset, Datasets server API\\n\\n> API on ðŸ¤— datasets\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server]   \n",
       "4   [Longformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, [Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]], The authors use 512 latents for all image models, and set the dimensionality of the latents to 1024. Hence, the latents are a tensor of shape (batch_size, 512, 1024) - assuming we add a batch dimension. The cross-attention layer takes the queries of shape (batch_size, 512, 1024) and keys + values of shape (batch_size, 50176, 512) as input, and produces a tensor that has the same shape as the queries, so outputs a new tensor of shape (batch_size, 512, 1024). Next, a block of 6 self-attention layers is applied repeatedly (8 times), to produce final hidden states of the latents of shape (batch_size, 512, 1024)]   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NaN   \n",
       "\n",
       "                                                                                test_settings  \\\n",
       "0   langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "1   langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "2   langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "3   langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "4   langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "..                                                                                        ...   \n",
       "62  doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "63  doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "64  doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "65  doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "66  doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm   \n",
       "\n",
       "   eval_score_mixtral:instruct  \\\n",
       "0                            5   \n",
       "1                            3   \n",
       "2                            5   \n",
       "3                            5   \n",
       "4                            3   \n",
       "..                         ...   \n",
       "62                           5   \n",
       "63                           4   \n",
       "64                           1   \n",
       "65                           3   \n",
       "66                           4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       eval_feedback_mixtral:instruct  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The response is exactly the same as the reference answer, providing complete correctness, accuracy, and being factual.   \n",
       "1                                                                                                                                                                                                                                          The response accurately acknowledges that there is no information provided about the purpose of the BLIP-Diffusion model in the context given. This is a precise and honest evaluation, as the model correctly identifies that it cannot provide an answer based on the given context. However, this response does not include the reference answer's statement describing the actual purpose of the BLIP-Diffusion model.   \n",
       "2                                                                                                                                                                                                                                  The response is correctly describing the steps to claim authorship of a paper on the Hugging Face Hub. It provides slightly more detail than the reference answer by specifying that the user will be redirected to their paper settings and that the admin team needs to validate the request. However, this does not affect the accuracy or correctness of the response, so it still aligns with the score rubric for a score 5.   \n",
       "3                                                                                                                                                                                                                                                                                                                                             The response accurately explains the purpose of the `/healthcheck` endpoint in the Datasets server API, stating that it is used to ensure that the application is running and functions as a basic health check for the server's status. This aligns with the reference answer and meets the criteria for a score of 5.   \n",
       "4                                                                                                         While the response correctly identifies where to find the attention window size in the LongT5 model configuration, it does not provide a specific default value as stated in the rubric. The reference answer clearly indicates that the default context window size for Local Attention in the LongT5 model is 127 tokens. Therefore, the response lacks the specificity required to be considered fully accurate.\\n\\nFeedback: The response does not provide a specific default value for the context window size of Local Attention in the LongT5 model.   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ...   \n",
       "62                                                                                                                                                                                                                                                                                                                                                                                          The response is entirely consistent with the reference answer, providing the same information about the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0. This demonstrates accuracy and adherence to the given fact.   \n",
       "63  The response accurately captures the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists. It highlights the main functionalities of W&B such as tracking, visualizing, and comparing experiments, logging metrics, hyperparameters, code versions, and other relevant information. The response also correctly mentions that W&B facilitates collaboration among team members, improves reproducibility, and helps optimize models. However, the response does not explicitly mention that W&B is used to track machine learning experiments \"at every stage, from training to production\" as stated in the reference answer.   \n",
       "64                                                                                                                                                                                                                                                                                                                  The response is incorrect as it provides the name `Transformers.js` which is not the correct name of the open-source library created by Hugging Face for Transformer acceleration. The reference answer indicates that the correct name is `Optimum`. Therefore, the response does not meet the criteria of being correct, accurate, and factual.   \n",
       "65                                                                                                                                                             While the response correctly explains how to use the `elem_id` parameter in Gradio to set a uniform height for elements in a row, it unfortunately does not provide the correct parameter name as per the reference answer. The parameter used to ensure that elements in a row have the same height in Gradio is actually called \"equal_height\", not \"elem_id\". Therefore, the response is not accurate in this regard. However, the explanation of how to use the parameter is correct and detailed.   \n",
       "66                                                                                                                                                                                               While the provided response is correct in specifying the command to install Optimum with OpenVINO support using pip, it does not mention the '--upgrade-strategy eager' flag that is included in the reference answer. This flag ensures that the installation uses an eager strategy for upgrades, which may be important in specific use cases or environments. Since the response is missing this detail, it does not meet the full criteria of the score rubric.   \n",
       "\n",
       "                                                                                                       settings  \n",
       "0       ./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "1       ./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "2       ./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "3       ./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "4       ./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "..                                                                                                          ...  \n",
       "62  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "63  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "64  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "65  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "66  ./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json  \n",
       "\n",
       "[134 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_mixtral:instruct</th>\n",
       "      <th>eval_feedback_mixtral:instruct</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What architecture is the `tokenizers-linux-x64-musl` binary designed for?\\n</td>\n",
       "      <td>x86_64-unknown-linux-musl</td>\n",
       "      <td>huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md</td>\n",
       "      <td>The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture.</td>\n",
       "      <td>[`tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`, `tokenizers-linux-x64-gnu`\\n\\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`, `tokenizers-linux-arm64-gnu`\\n\\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`, p align=\"center\"&gt;\\n    &lt;br&gt;\\n    &lt;img src=\"https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png\" width=\"600\"/&gt;\\n    &lt;br&gt;\\n&lt;p&gt;\\n&lt;p align=\"center\"&gt;\\n    &lt;img alt=\"Build\" src=\"https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg\"&gt;\\n    &lt;a href=\"https://github.com/huggingface/tokenizers/blob/master/LICENSE\"&gt;\\n        &lt;img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue\"&gt;\\n    &lt;/a&gt;\\n    &lt;a href=\"https://docs.rs/tokenizers/\"&gt;\\n        &lt;img alt=\"Doc\" src=\"https://docs.rs/tokenizers/badge.svg\"&gt;\\n    &lt;/a&gt;\\n&lt;/p&gt;\\n&lt;br&gt;\\n\\n\\nThe core of `tokenizers`, written in Rust.\\nProvides an implementation of today's most used tokenizers, with a focus on performance and\\nversatility., `tokenizers-win32-x64-msvc`\\n\\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`, `tokenizers-win32-arm64-msvc`\\n\\nThis is the **aarch64-pc-windows-msvc** binary for `tokenizers`]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>The response is exactly the same as the reference answer, providing complete correctness, accuracy, and being factual.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the BLIP-Diffusion model?\\n</td>\n",
       "      <td>The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md</td>\n",
       "      <td>Based on the provided context, there is no information given about the purpose of the BLIP-Diffusion model. The context mainly discusses Stable Diffusion and DDPO models.</td>\n",
       "      <td>[Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:, The literature on Diffusion-based models is developing at a rapid pace which is why we partnered with [Jonathan Whitaker](https://github.com/johnowhitaker) to develop a course on it. The course is free, and you can check it out [here](https://github.com/huggingface/diffusion-models-class).\\n\\n## Support for third-party libraries, --\\ntitle: \"Finetune Stable Diffusion Models with DDPO via TRL\" \\nthumbnail: /blog/assets/166_trl_ddpo/thumbnail.png\\nauthors:\\n- user: metric-space\\n  guest: true\\n- user: sayakpaul\\n- user: kashif\\n- user: lvwerra\\n---\\n\\n# Finetune Stable Diffusion Models with DDPO via TRL\\n\\n\\n## Introduction, Stable Diffusion 2 is a text-to-image _latent diffusion_ model built upon the work of the original [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and it was led by Robin Rombach and Katherine Crowson from [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/)., In this blog post, we discuss how DDPO came to be, a brief description of how it works, and how DDPO can be incorporated into an RLHF workflow to achieve model outputs more aligned with the human aesthetics. We then quickly switch gears to talk about how you can apply DDPO to your  models with the newly integrated `DDPOTrainer` from the `trl` library and discuss our findings from running DDPO on Stable Diffusion. \\n\\n## The Advantages of DDPO\\n\\nDDPO is not the only working answer to the question of how to attempt to fine-tune diffusion models with RL. \\n\\nBefore diving in, there are two key points to remember when it comes to understanding the advantages of one RL solution over the other\\n\\n1. Computational efficiency is key. The more complicated your data distribution gets, the higher your computational costs get.\\n2. Approximations are nice, but because approximations are not the real thing, associated errors stack up., We've gone from the basic use of Stable Diffusion using ðŸ¤— Hugging Face Diffusers to more advanced uses of the library, and we tried to introduce all the pieces in a modern diffusion system. If you liked this topic and want to learn more, we recommend the following resources:\\n- Our [Colab notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\\n- The [Getting Started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, that gives a broader overview on Diffusion systems.\\n- The [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) blog post., In order to make it easy for everyone to take advantage of these improvements, we have converted the four official Stable Diffusion models and pushed them to the [Hub](https://huggingface.co/apple). These are all the variants:]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>3</td>\n",
       "      <td>The response accurately acknowledges that there is no information provided about the purpose of the BLIP-Diffusion model in the context given. This is a precise and honest evaluation, as the model correctly identifies that it cannot provide an answer based on the given context. However, this response does not include the reference answer's statement describing the actual purpose of the BLIP-Diffusion model.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n",
       "      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n",
       "      <td>huggingface/hub-docs/blob/main/docs/hub/paper-pages.md</td>\n",
       "      <td>A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page, clicking on their name in the corresponding Paper page, and then clicking \"claim authorship\". This will redirect them to their paper settings where they can confirm the request. The admin team will validate this request, and once confirmed, the Paper page will show as verified.</td>\n",
       "      <td>[* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email., &lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/&gt;\\n&lt;img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/&gt;\\n&lt;/div&gt;\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified., It also helps us if you spread the word: reference the library from blog posts\\non the awesome projects it made possible, shout out on Twitter every time it has\\nhelped you, or simply star the repo to say \"thank you\".\\n\\nWhichever way you choose to contribute, please be mindful to respect our\\n[code of conduct](https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md).\\n\\n&gt; Looking for a good first issue to work on?\\n&gt; Please check out our contributing guide below and then select an issue from our [curated list](https://github.com/huggingface/huggingface_hub/contribute).\\n&gt; Pick one and get started with it!\\n\\n### The client library, `huggingface_hub`, # Model `license:other` challenge\\n\\nRelated to https://github.com/huggingface/hub-docs/issues/985.\\n\\n## Context, ### MODEL CARDS ON THE HUGGING FACE HUB\\nSince 2018, new platforms and mediums for hosting and sharing model cards have also emerged. For example, particularly relevant to this project, Hugging Face hosts model cards on the Hugging Face Hub as README files in the repositories associated with ML models. As a result, model cards figure as a prominent form of documentation for users of models on the Hugging Face Hub. As part of our analysis of model cards, we developed and proposed model cards for several dozen ML models on the Hugging Face Hub, using the Hubâ€™s Pull Request (PR) and Discussion features to gather feedback on model cards, verify information included in model cards, and publish model cards for models on the Hugging Face Hub. At the time of writing of this guide book, all of Hugging Faceâ€™s models on the Hugging Face Hub have an associated model card on the Hub[^8]., --\\ntitle: Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\nthumbnail: /blog/assets/75_hugging_face_endpoints_on_azure/01.jpg\\nauthors:\\n- user: jeffboudier\\n- user: philschmid\\n- user: juliensimon\\n---\\n\\n# Hugging Face Collaborates with Microsoft to launch Hugging Face Model Catalog on Azure\\n\\n\\n![Hugging Face Endpoints on Azure](assets/75_hugging_face_endpoints_on_azure/01.jpg \"Hugging Face Endpoints on Azure\"), You can also revoke a user's membership or change their role on this page.\\n\\n## Organization domain name\\n\\nUnder the **Account** tab in the Organization settings, you can set an **Organization domain name**. Specifying a domain name will allow any user with a matching email address on the Hugging Face Hub to join your organization.]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>The response is correctly describing the steps to claim authorship of a paper on the Hugging Face Hub. It provides slightly more detail than the reference answer by specifying that the user will be redirected to their paper settings and that the admin team needs to validate the request. However, this does not affect the accuracy or correctness of the response, so it still aligns with the score rubric for a score 5.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of the /healthcheck endpoint in the Datasets server API?\\n</td>\n",
       "      <td>Ensure the app is running</td>\n",
       "      <td>huggingface/datasets-server/blob/main/services/api/README.md</td>\n",
       "      <td>The `/healthcheck` endpoint in the Datasets server API is used to ensure that the application is running. It's a basic health check for the server's status.</td>\n",
       "      <td>[Datasets server API - rows endpoint\\n\\n&gt; /rows endpoint\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: ensure the app is running\\n- /metrics: return a list of metrics in the Prometheus format\\n- /rows: get a slice of rows of a dataset split, The endpoint response is a JSON with the `dataset_info` key. Its structure and content correspond to [DatasetInfo](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetInfo) object of the `datasets` library., The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:, - `/healthcheck`\\n- `/metrics`: give info about the cache and the queue\\n- `/cache-reports{processing_step}`: give detailed reports on the content of the cache for a processing step\\n- `/cache-reports-with-content{processing_step}`: give detailed reports on the content of the cache for a processing step, including the content itself, which can be heavy\\n- `/pending-jobs`: give the pending jobs, classed by queue and status (waiting or started)\\n- `/force-refresh{processing_step}`: force refresh cache entries for the processing step. It's a POST endpoint. Pass the requested parameters, depending on the processing step's input type:\\n  - `dataset`: `?dataset={dataset}`\\n  - `config`: `?dataset={dataset}&amp;config={config}`\\n  - `split`: `?dataset={dataset}&amp;config={config}&amp;split={split}`, Datasets server - worker\\n\\n&gt; Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus, - /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset, Datasets server API\\n\\n&gt; API on ðŸ¤— datasets\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>The response accurately explains the purpose of the `/healthcheck` endpoint in the Datasets server API, stating that it is used to ensure that the application is running and functions as a basic health check for the server's status. This aligns with the reference answer and meets the criteria for a score of 5.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the default context window size for Local Attention in the LongT5 model?\\n</td>\n",
       "      <td>127 tokens</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md</td>\n",
       "      <td>The default context window size for Local Attention in the LongT5 model is defined by `config.attention_window`. It can be of type `List` to define a different window size for each layer. For more specifics on the default value, it would be best to refer to the actual implementation or documentation as it could vary depending on the specific configuration of the model.</td>\n",
       "      <td>[Longformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\\nconventionally done for all tokens in `BertSelfAttention`., - [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`, [Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\\nrepresentation of the whole sentence.\\n\\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:, Note that \"locally\" and \"globally\" attending tokens are projected by different query, key and value matrices. Also note\\nthat every \"locally\" attending token not only attends to tokens within its window \\\\(w\\\\), but also to all \"globally\"\\nattending tokens so that global attention is *symmetric*.\\n\\nThe user can define which tokens attend \"locally\" and which tokens attend \"globally\" by setting the tensor\\n`global_attention_mask` at run-time appropriately. All Longformer models employ the following logic for\\n`global_attention_mask`:\\n\\n- 0: the token attends \"locally\",\\n- 1: the token attends \"globally\".\\n\\nFor more information please also refer to [`~LongformerModel.forward`] method., are constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token's embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below., However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\\n\\n### Decoder[[nlp-decoder]], The authors use 512 latents for all image models, and set the dimensionality of the latents to 1024. Hence, the latents are a tensor of shape (batch_size, 512, 1024) - assuming we add a batch dimension. The cross-attention layer takes the queries of shape (batch_size, 512, 1024) and keys + values of shape (batch_size, 50176, 512) as input, and produces a tensor that has the same shape as the queries, so outputs a new tensor of shape (batch_size, 512, 1024). Next, a block of 6 self-attention layers is applied repeatedly (8 times), to produce final hidden states of the latents of shape (batch_size, 512, 1024)]</td>\n",
       "      <td>langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>3</td>\n",
       "      <td>While the response correctly identifies where to find the attention window size in the LongT5 model configuration, it does not provide a specific default value as stated in the rubric. The reference answer clearly indicates that the default context window size for Local Attention in the LongT5 model is 127 tokens. Therefore, the response lacks the specificity required to be considered fully accurate.\\n\\nFeedback: The response does not provide a specific default value for the context window size of Local Attention in the LongT5 model.</td>\n",
       "      <td>./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\\n</td>\n",
       "      <td>10GB</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/big_models.md</td>\n",
       "      <td>The maximum size of a model checkpoint at which automatic sharding occurs in Transformers version 4.18.0 is 10GB.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>5</td>\n",
       "      <td>The response is entirely consistent with the reference answer, providing the same information about the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0. This demonstrates accuracy and adherence to the given fact.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What is the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists?\\n</td>\n",
       "      <td>To track their machine learning experiments at every stage, from training to production.</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md</td>\n",
       "      <td>Weights and Biases (W&amp;B) is a tool that helps data scientists and machine learning scientists track, visualize, and compare experiments and model performance. It allows users to log metrics, hyperparameters, code versions, and other relevant information about their models and datasets. W&amp;B also provides tools for comparing different runs, visualizing the training process, and collaborating with team members. By using W&amp;amp;B, data scientists and machine learning scientists can more easily manage and optimize their models, improve reproducibility, and share their results with others.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>The response accurately captures the purpose of Weights and Biases (W&amp;B) for data scientists and machine learning scientists. It highlights the main functionalities of W&amp;B such as tracking, visualizing, and comparing experiments, logging metrics, hyperparameters, code versions, and other relevant information. The response also correctly mentions that W&amp;B facilitates collaboration among team members, improves reproducibility, and helps optimize models. However, the response does not explicitly mention that W&amp;B is used to track machine learning experiments \"at every stage, from training to production\" as stated in the reference answer.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n</td>\n",
       "      <td>Optimum</td>\n",
       "      <td>huggingface/blog/blob/main/intel.md</td>\n",
       "      <td>The name of the open-source library created by Hugging Face for Transformer acceleration is `Transformers.js`.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>1</td>\n",
       "      <td>The response is incorrect as it provides the name `Transformers.js` which is not the correct name of the open-source library created by Hugging Face for Transformer acceleration. The reference answer indicates that the correct name is `Optimum`. Therefore, the response does not meet the criteria of being correct, accurate, and factual.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What parameter is used to ensure that elements in a row have the same height in Gradio?\\n</td>\n",
       "      <td>equal_height</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md</td>\n",
       "      <td>The `elem_id` parameter can be used in Gradio to set the same height for all elements in a row. This parameter allows you to specify the ID of an HTML element, and any other elements with the same ID will have their height adjusted to match. For example:\\n```\\ngr.Interface(my_function, inputs=\"text\", outputs=\"text\").launch(elem_id=\"same-height\")\\n```\\nThis will set the height of all text input elements in the interface to be the same. Note that this parameter is only available in Gradio version 3.0 and later.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>3</td>\n",
       "      <td>While the response correctly explains how to use the `elem_id` parameter in Gradio to set a uniform height for elements in a row, it unfortunately does not provide the correct parameter name as per the reference answer. The parameter used to ensure that elements in a row have the same height in Gradio is actually called \"equal_height\", not \"elem_id\". Therefore, the response is not accurate in this regard. However, the explanation of how to use the parameter is correct and detailed.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What is the command to install the latest version of Optimum with OpenVINO support?\\n</td>\n",
       "      <td>pip install --upgrade-strategy eager optimum[\"openvino\"]</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md</td>\n",
       "      <td>To install the most recent version of Optimum that includes OpenVINO support, you can use the following pip command:\\n```\\npip install optimum[openvino]\\n```\\nThis will install the `optimum` package along with the necessary dependencies for OpenVINO support.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm</td>\n",
       "      <td>4</td>\n",
       "      <td>While the provided response is correct in specifying the command to install Optimum with OpenVINO support using pip, it does not mention the '--upgrade-strategy eager' flag that is included in the reference answer. This flag ensures that the installation uses an eager strategy for upgrades, which may be important in specific use cases or environments. Since the response is missing this detail, it does not meet the full criteria of the score rubric.</td>\n",
       "      <td>./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows Ã— 9 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "609601e4b9513078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T11:43:16.513342Z",
     "start_time": "2024-04-17T11:43:16.280480Z"
    }
   },
   "source": [
    "# Get diffs\n",
    "import duckdb\n",
    "DIFF_SQL = \"SELECT tbl1.question, tbl1.true_answer, tbl1.generated_answer as langchain_answer, tbl1.score as langchain_score, tbl2.generated_answer as doc_agent_answer, tbl2.score as doc_agent_score \"\\\n",
    "           \"FROM \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'langchain%') AS tbl1 \"\\\n",
    "           \"JOIN \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'doc_agent%') AS tbl2 \"\\\n",
    "           \"ON tbl1.question = tbl2.question \" \\\n",
    "           f\"WHERE tbl1.score > tbl2.score\"\n",
    "\n",
    "DIFFS = duckdb.query(DIFF_SQL).to_df()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(DIFFS)\n",
    "\n",
    "DIFFS.to_excel(\"./output/diffs.xlsx\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                                                   question  \\\n",
       "0                                                                     How can a user claim authorship of a paper on the Hugging Face Hub?\\n   \n",
       "1                                                        What is the default context window size for Local Attention in the LongT5 model?\\n   \n",
       "2                                What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\\n   \n",
       "3                                                                                         What is the purpose of the `gradio.Blocks` API?\\n   \n",
       "4                      What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\\n   \n",
       "5   What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\\n   \n",
       "6                                                      What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?\\n   \n",
       "7                                                                             What is the resolution of images used by the CLIPSeg model?\\n   \n",
       "8                                          What is the default repository type created by the `create_repo` function on Hugging Face Hub?\\n   \n",
       "9                                                      What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\\n   \n",
       "10                                                                                        What do RAG models combine to generate outputs?\\n   \n",
       "11                                                                             What is the title of the paper introducing the ByT5 model?\\n   \n",
       "12                                                What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\\n   \n",
       "13                                                                        Where can the full code for the Stable Diffusion demo be found?\\n   \n",
       "14                                       What transformation does the FNet model use to replace the self-attention layer in a BERT model?\\n   \n",
       "15                                          How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?\\n   \n",
       "16                                                                                 What is the purpose of tokenizers in the NLP pipeline?\\n   \n",
       "17                                                                    What is the purpose of the Safety Checker in the Diffusers library?\\n   \n",
       "18                                                                                              What is the purpose of Textual Inversion?\\n   \n",
       "19                                                                  How do you run a Gradio Blocks app in reload mode using a Python IDE?\\n   \n",
       "20                                                                     What is the pretraining objective of the Wav2Vec2 context network?\\n   \n",
       "21                                    What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\\n   \n",
       "22                What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\\n   \n",
       "23                                                        What command line module does PyTorch provide to run a script on multiple GPUs?\\n   \n",
       "24                                                            What is the command to upload an ESPnet model to a Hugging Face repository?\\n   \n",
       "25                                          How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n   \n",
       "26                                                What parameter is used to ensure that elements in a row have the same height in Gradio?\\n   \n",
       "27                                                    What is the command to install the latest version of Optimum with OpenVINO support?\\n   \n",
       "28                                                         What is the goal of the Named Entity Recognition task in token classification?\\n   \n",
       "29                              What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                     true_answer  \\\n",
       "0                                                               By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.   \n",
       "1                                                                                                                                                                                                                     127 tokens   \n",
       "2                                                                                                                                                                                                                        IDEFICS   \n",
       "3                                                         The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.   \n",
       "4                                                                                                                                                                                                           Grouped convolutions   \n",
       "5                                                                                                                        Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.   \n",
       "6                                                                                                         1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.   \n",
       "7                                                                                                                                                                                                               352 x 352 pixels   \n",
       "8                                                                                                                                                                                                                          model   \n",
       "9                                    FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.   \n",
       "10                                                                                                                                                             Pretrained dense retrieval (DPR) and sequence-to-sequence models.   \n",
       "11                                                                                                                                                        ByT5: Towards a token-free future with pre-trained byte-to-byte models   \n",
       "12                                                                                                                                                                                                                      \"manual\"   \n",
       "13                                                                                                                                                                   https://hf.co/spaces/stabilityai/stable-diffusion/tree/main   \n",
       "14                                                                                                                                                                                                             Fourier transform   \n",
       "15                                                                                                                                                                               By passing `fp16=True` to the Accelerator init.   \n",
       "16                                                                                                                                                               To translate text into data that can be processed by the model.   \n",
       "17  The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.   \n",
       "18                                                                                                    Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.   \n",
       "19                                                                                                                                                                                          Run `gradio run.py` in the terminal.   \n",
       "20                    The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.   \n",
       "21                                                                                                                                                                                distilbert base uncased finetuned sst2 english   \n",
       "22                                                                                                  To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.   \n",
       "23                                                                                                                                                                                                                      torchrun   \n",
       "24                                                                                                                                                      ./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo   \n",
       "25                                                                                                                                                                                                                    3-5 images   \n",
       "26                                                                                                                                                                                                                  equal_height   \n",
       "27                                                                                                                                                                      pip install --upgrade-strategy eager optimum[\"openvino\"]   \n",
       "28                                                                                          The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.   \n",
       "29                                                                                                                                                                                                                       Optimum   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     langchain_answer  \\\n",
       "0                                                                                                                                                   A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page, clicking on their name in the corresponding Paper page, and then clicking \"claim authorship\". This will redirect them to their paper settings where they can confirm the request. The admin team will validate this request, and once confirmed, the Paper page will show as verified.   \n",
       "1                                                                                                                                                 The default context window size for Local Attention in the LongT5 model is defined by `config.attention_window`. It can be of type `List` to define a different window size for each layer. For more specifics on the default value, it would be best to refer to the actual implementation or documentation as it could vary depending on the specific configuration of the model.   \n",
       "2                                                                                                                                                                                                             The name of the large multimodal model is Flamingo. It's mentioned in Document 2, which states that you can use this model to solve image-text tasks. However, it's not explicitly stated that Flamingo is based on Flamingo itself in the document. This information might be derived from other sources or documents.   \n",
       "3                                                                                                                                                                                              The `gradio.Blocks` API is a new, low-level interface in Gradio 3.0 that provides full control over the data flows and layout of an application. It allows for the creation of complex, multi-step applications. This is achieved by enabling developers to build their own blocks, define their behavior, and arrange them as needed.   \n",
       "4                                                                                                                                                                                                                                                                                   SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. This information can be found in the first document, \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\".   \n",
       "5                                                                                                                                                                                                                                                                                      The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are 1) splitting the embedding matrix into two smaller matrices, and 2) using repeating layers split among groups.   \n",
       "6                                                                                                                                                                                                                      The three main steps for fine-tuning a model with the ðŸ¤— Datasets library are:\\n\\n1. Loading a dataset from the Hugging Face Hub.\\n2. Preprocessing the data using `Dataset.map()`.\\n3. Loading and computing metrics.\\n\\nThis is mentioned in Chapter 3 of the course you're taking on the ðŸ¤— Datasets library.   \n",
       "7                                                                                                                                                                                                                                                                                                                                         The resolution of images used by the CLIPSeg model is 352 x 352 pixels. This information can be found in document 5, where it's mentioned that \"the model uses images of 352 x 352 pixels\".   \n",
       "8                                                                                                                                                                                                                                                                                                                   The default repository type created by the `create_repo` function on Hugging Face Hub is a 'model' repository. If you want to create a 'dataset' or a 'space', you can specify it using the `repo_type` argument.   \n",
       "9    The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard a model's parameters, gradients, and optimizer states across multiple GPUs. This reduces memory usage, improves GPU memory efficiency, and allows for the training of much larger models on fewer GPUs. FSDP also supports CPU offloading of sharded model parameters, enabling the training of large models that exceed the available GPU memory. It is designed for distributed training of large pretrained models up to 1T parameters.   \n",
       "10                                                                                                  RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks. This information is provided in Document 0 and Document 4.   \n",
       "11                                                                                                                                                                                                                                                                                                        The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\". This information can be found in Document 2, 3, 4, and 5, which all refer to the same paper.   \n",
       "12                                                                                                                                                                                                                                                                                               The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that the flag button is not automatically displayed; it has to be added explicitly using the **Flag** button or through code.   \n",
       "13                                                                                                                                                                                                                                                                                                                                    The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. This information is provided in Document 0 of your context.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                          The FNet model uses a fourier transform to replace the self-attention layer in a BERT model, specifically returning only the real parts of the transform.   \n",
       "15                                                                                                                                                  You can force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate by setting the `fp16` parameter to `True`. Here's an example:\\n\\n```py\\nfrom accelerate import Accelerator\\n\\naccelerator = Accelerator(fp16=True)\\n```\\n\\nThis will enable mixed precision training, which can help reduce memory usage and speed up computation during model training.   \n",
       "16                                                                                                                                                                                                    The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. This is crucial because raw text needs to be converted into a format that models can understand and process.   \n",
       "17                                         The Safety Checker in the Diffusers library is a component that checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. It is designed to flag inappropriate or harmful content, helping users interact with generative models responsibly and ethically. The Safety Checker is a crucial part of the Diffusers library, ensuring that the generated content adheres to ethical guidelines.   \n",
       "18                                                                                                                                                                                                                                                      The purpose of Textual Inversion, as described in the provided context, is a training method for personalizing models by learning new text embeddings from a few example images. The resulting file is very small and the new embeddings can be loaded into the text encoder.   \n",
       "19                                                                                                                              To run a Gradio Blocks app in reload mode using a Python IDE, you need to use the `gradio` command followed by the name of your Python script and the name of your demo. If your `run.py` file contained a demo named `my_demo`, you would launch it in reload mode like this: `gradio run.py my_demo`. This will allow Gradio to automatically reload the app whenever changes are made to the file.   \n",
       "20                                                                                                                                                                                                 The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones. This encourages the model to find the most similar context vector and quantized speech unit, which is the target label.   \n",
       "21                                                                                                                                                                                                                                                    The default checkpoint used by the sentiment analysis pipeline in the Transformers library is a particular pretrained model that has been fine-tuned for sentiment analysis in English. It's downloaded and cached when you create the `classifier` object (source document 0).   \n",
       "22                                                                                                                                                                                                                                                                          The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train/fine-tune a 1.6B-parameter GPT2-XL model for causal language modeling on Habana Gaudi.   \n",
       "23                                                                                                                                                                      The command line module that PyTorch provides to run a script on multiple GPUs is `torchrun`. It allows you to specify the number of GPUs to use and the script to run. Here is an example:\\n\\n```bash\\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\\n```\\n\\nThis command will run the training script on two GPUs that live on a single machine.   \n",
       "24                                                                                                                                                                                                                                                    The command to upload an ESPnet model to a Hugging Face repository using the `run.sh` script is:\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\nReplace `username` and `model_repo` with your desired username and repository name.   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                    According to Document 4, you need 3-5 images to teach new concepts to Stable Diffusion using Textual Inversion.   \n",
       "26                                                                                                                                                                                                                                                                                           The `equal_height` parameter is used to ensure that elements in a row have the same height in Gradio. This parameter should be passed to the `.style()` method of `gr.Row()`. This information was updated in PR 3125 by freddyaboulton.   \n",
       "27                                                                                                                                                                                                                                                                                                 The command to install the latest version of Optimum with OpenVINO support is:\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\nThis command will ensure that `optimum-intel` is using the latest version.   \n",
       "28                                                                        The goal of the Named Entity Recognition (NER) task in token classification is to find and classify entities in a piece of text, such as persons, locations, or organizations. Each token (word or subword) in the text is assigned a label from a predefined set of classes indicating the type of entity it belongs to. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.   \n",
       "29                                                                                                                                                                                                                                                                                                                        The name of the open-source library created by Hugging Face to simplify Transformer acceleration is [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate). This information can be found in Document 2.   \n",
       "\n",
       "   langchain_score  \\\n",
       "0                5   \n",
       "1                3   \n",
       "2                3   \n",
       "3                5   \n",
       "4                5   \n",
       "5                5   \n",
       "6                5   \n",
       "7                5   \n",
       "8                5   \n",
       "9                5   \n",
       "10               5   \n",
       "11               5   \n",
       "12               5   \n",
       "13               5   \n",
       "14               5   \n",
       "15               5   \n",
       "16               5   \n",
       "17               5   \n",
       "18               5   \n",
       "19               4   \n",
       "20               5   \n",
       "21               4   \n",
       "22               5   \n",
       "23               5   \n",
       "24               5   \n",
       "25               5   \n",
       "26               4   \n",
       "27               5   \n",
       "28               5   \n",
       "29               3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   doc_agent_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page and clicking on their name. They will then be redirected to the paper settings where they can confirm the request. The admin team will validate the request, and once confirmed, the Paper page will show as verified.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The default context window size for Local Attention in the LongT5 model is 512.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  AlignVisionModel   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The `gradio.Blocks` API is a high-level interface for creating user interfaces (UIs) for machine learning models in Python. It allows developers to create custom UIs with minimal code, and includes features such as support for different input and output types, real-time updates, and the ability to share UIs via links or embed them in other websites. The `Blocks` API is part of the Gradio library, which provides a range of tools for building and deploying machine learning applications.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                SqueezeBERT uses depthwise separable convolutions in place of fully-connected layers for the Q, K, V, and FNN layers. This reduces the number of parameters and computations required while maintaining similar performance to traditional fully-connected layers.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The two parameter-reduction techniques proposed in the ALBERT model are cross-layer parameter sharing and factorized embedding parameterization. Cross-layer parameter sharing reduces the number of unique parameters by sharing them across multiple layers, while factorized embedding parameterization reduces the embedding size by decomposing it into smaller matrices.   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The three main steps for fine-tuning a model using the ðŸ¤— (Hugging Face) Datasets library are:\\n\\n1. Preprocessing the data: This involves loading and preprocessing the dataset, which includes tokenization, padding, and batching.\\n2. Fine-tuning the model: This involves defining the training parameters, such as the learning rate, number of epochs, and batch size, and then fine-tuning the model on the preprocessed data.\\n3. Evaluating the model: This involves evaluating the performance of the fine-tuned model on a validation dataset.\\n\\nHere's an example notebook that demonstrates these steps using the ðŸ¤— Transformers and Datasets libraries to fine-tune a BERT model for text classification: [How to fine-tune BERT for text classification with ðŸ¤— Transformers and ðŸ¤— Datasets](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_bert.ipynb).   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The default resolution for images used in the CLIPSeg model is 256x256 pixels. However, this can be changed during initialization of the `CLIPSegModel` or `CLIPSegForImageSegmentation` class by providing a different value for the `image_resolution` argument.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The default repository type created by the `create_repo` function when used on the Hugging Face Hub is a model card.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Fully Sharded Data Parallel (FSDP) is a technique used in distributed training to shard the model parameters, optimizer and gradient states, and even offload them to the CPU when they're inactive. This reduces the high cost of large-scale training by allowing you to train really large models on multiple GPUs or TPUs. FSDP can be a powerful tool for reducing memory usage and increasing throughput in distributed training.   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     RAG models combine two main components: a retriever and a generator. The retriever is responsible for retrieving relevant documents from a knowledge source (such as a corpus of text) based on a given input query. The generator then uses the retrieved documents, along with the input query, to generate an output sequence. This allows RAG models to generate more accurate and informative outputs by leveraging external knowledge beyond what is contained in their training data.   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The title of the paper that introduces the ByT5 model is \"ByT5: Towards a Unified Text-to-Text Transformer\".   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The default value for the `allow_flagging` parameter in a Gradio `Interface` is `False`.   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The full code for the Stable Diffusion demo is available in this Colab notebook: <https://colab.research.google.com/github/huggingface/diffusers/blob/main/notebooks/StableDiffusion_Demo.ipynb>\\n\\nThis notebook includes examples of how to use the Stable Diffusion model for text-to-image generation, image editing, and more. It also covers various techniques for optimizing inference time and reducing memory usage.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The FNet model uses a Fourier Transform that returns only the real parts of the transform to replace the self-attention layer in a BERT model.   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           To force mixed precision training when initializing the Accelerator in Hugging Face's Accelerate library, you can pass `mixed_precision=\"fp16\"` as an argument to the `Accelerator` constructor. Here is an example:\\n```python\\nfrom accelerate import Accelerator\\n\\naccelerator = Accelerator(mixed_precision=\"fp16\")\\n```\\nThis will enable mixed precision training using the FP16 data type, which can help reduce memory usage and improve training speed. Note that you may need to make additional changes to your code to ensure it is compatible with mixed precision training. For more information, please refer to the [Accelerate documentation](https://huggingface.co/docs/accelerate/main/en/usage_guides/training#mixed-precision).   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                     Tokenizers are used to break up text into smaller units called tokens, which can be words, subwords, or even individual characters. This process is a crucial step in NLP pipelines because it allows models to work with discrete units of meaning that can be more easily processed and analyzed. By breaking text down into tokens, tokenizers help to simplify the input data for NLP models, making it easier for them to learn patterns and relationships between words and phrases.\\n\\nTokenization is also an important step in preparing text data for use in machine learning algorithms, as it allows for more efficient processing of large datasets. By breaking up text into smaller units, tokenizers can help reduce the dimensionality of the input data, making it easier to train models on large datasets without running out of memory or taking too long to process.\\n\\nOverall, tokenization is a critical step in NLP pipelines that helps to simplify and streamline the processing of text data for use in machine learning algorithms. By breaking up text into smaller units, tokenizers enable models to more easily learn patterns and relationships between words and phrases, making it possible to build powerful NLP applications that can understand and analyze human language with remarkable accuracy.   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The Safety Checker in the Diffusers library is used to ensure that the generated images are safe and appropriate for all audiences. It checks for any explicit or harmful content in the generated images and prevents them from being saved or displayed. This feature helps to maintain a safe and respectful environment for users of the library.   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Textual Inversion is a method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.   \n",
       "19   To run a Gradio Blocks app in reload mode using a Python IDE, you can use the `gradio_client` library to connect to your local Gradio server and then use the `reload` function from the `IPython` library to automatically reload your code when it changes. Here's an example:\\n\\n1. Start your Gradio Blocks app in reload mode by running the following command in a terminal:\\n```python\\nimport gradio as gr\\n\\ndef greet(name):\\n    return \"Hello \" + name + \"!\"\\n\\nblocks = gr.Blocks()\\nblocks.greet_button, blocks.greet_textbox = blocks.buttons(\"Greet\", \"Click to greet\"), blocks.textbox(\"\")\\nblocks.greet_button.click(greet, blocks.greet_textbox)\\nblocks.launch(reload=True)  # Start the server in reload mode\\n```\\n2. In your Python IDE, run the following code to connect to your local Gradio server and automatically reload your code when it changes:\\n```python\\nimport gradio_client as grc\\nimport ipykernel\\nfrom traitlets import TraitSet\\n\\n# Connect to the local Gradio server\\napp = grc.connect('http://localhost:7865')\\n\\n# Set up reloading of code when it changes\\ncode = \"\"\"\\nimport gradio_client as grc\\n\\ndef on_reload():\\n    app.reload()\\n\\nipykernel.getfigid().on_reload(on_reload)\\n\"\"\"\\n\\n# Run the code in a separate kernel to avoid conflicts with your main kernel\\nipykernel.connect(code=code, fname='gradio_reload.py')\\n```\\nThis will automatically reload your Gradio Blocks app whenever you make changes to your code. Note that this approach only works when running the Gradio server locally on your machine. If you're hosting your Gradio app on a remote server or using a different URL, you'll need to modify the `connect` function accordingly.   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The pretraining objective of the Wav2Vec2 context network is contrastive predictive coding (CPC). CPC involves training a neural network to predict future audio samples based on past audio samples. Specifically, the network is trained to distinguish between true future audio samples and negative samples that are generated by randomly sampling from other parts of the input audio. This helps the network learn meaningful representations of the audio data that can be used for downstream tasks such as automatic speech recognition (ASR).   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The default checkpoint for the sentiment analysis pipeline in the Transformers library is \"finiteautomata/bertweet-base-sentiment-analysis\".   \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The purpose of the \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" notebook is to demonstrate how to use DeepSpeed, a distributed deep learning training framework, to train large models with billions of parameters on Habana Gaudi. This notebook provides an example of how to use DeepSpeed with Hugging Face's Transformers library and Habana's SynapseAI software development kit (SDK) to fine-tune the T5-3B text summarization model on the CNN DailyMail dataset. The results show that Gaudi2 is 2.44x faster than A100 80GB for this task.   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       `torch.distributed.launch`   \n",
       "24                                                                                                                                                                      To upload an ESPnet model to a Hugging Face repository, you can follow these steps:\\n\\n1. First, create a new repository on the Hugging Face Hub for your model. You can do this by going to the Hugging Face Hub website and clicking on the \"New Model\" button. Give your repository a name and a short description of what it contains.\\n2. Next, clone the repository to your local machine using Git. This will allow you to make changes to the repository and push them back to the Hugging Face Hub.\\n3. Once you have cloned the repository, you can add your ESPnet model files to the repository. Make sure to include a `model.py` file that defines the model architecture, as well as any other necessary files such as tokenizer files or vocabulary files.\\n4. After adding your model files to the repository, you can commit the changes and push them back to the Hugging Face Hub using Git. This will make the model available for others to use and download from the Hugging Face Hub.\\n5. Finally, you can create a model card for your model by adding a `README.md` file to the repository. The model card should include information about the model, such as its architecture, training data, and performance metrics. You can use the [Hugging Face Model Card template](https://huggingface.co/docs/transformers/model_card) as a guide for what to include in your model card.\\n\\nOnce you have uploaded your ESPnet model and created a model card, others will be able to find and use your model on the Hugging Face Hub.   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The number of images required to teach new concepts to Stable Diffusion through the process of Textual Inversion can vary depending on the complexity and specificity of the concept. Generally, a few dozen to a few hundred images are used in the training process for each concept. However, this is not a strict rule and the number can be adjusted based on the desired results. It's important to note that more images do not always lead to better results, as the quality of the images and their relevance to the concept also play crucial roles in the learning process.   \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The `elem_id` parameter can be used in Gradio to set the same height for all elements in a row. This parameter allows you to specify the ID of an HTML element, and any other elements with the same ID will have their height adjusted to match. For example:\\n```\\ngr.Interface(my_function, inputs=\"text\", outputs=\"text\").launch(elem_id=\"same-height\")\\n```\\nThis will set the height of all text input elements in the interface to be the same. Note that this parameter is only available in Gradio version 3.0 and later.   \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               To install the most recent version of Optimum that includes OpenVINO support, you can use the following pip command:\\n```\\npip install optimum[openvino]\\n```\\nThis will install the `optimum` package along with the necessary dependencies for OpenVINO support.   \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The goal of the Named Entity Recognition (NER) task in token classification is to identify and categorize named entities present in text into predefined classes such as person names, organization names, location names, medical codes, time expressions, quantities, monetary values, percentages, etc. This information can be used for various natural language processing applications like question answering, machine translation, and information extraction.   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The name of the open-source library created by Hugging Face for Transformer acceleration is `Transformers.js`.   \n",
       "\n",
       "   doc_agent_score  \n",
       "0                4  \n",
       "1                1  \n",
       "2                1  \n",
       "3                4  \n",
       "4                3  \n",
       "5                4  \n",
       "6                3  \n",
       "7                3  \n",
       "8                2  \n",
       "9                4  \n",
       "10               4  \n",
       "11               3  \n",
       "12               1  \n",
       "13               3  \n",
       "14               4  \n",
       "15               4  \n",
       "16               4  \n",
       "17               3  \n",
       "18               4  \n",
       "19               2  \n",
       "20               3  \n",
       "21               1  \n",
       "22               4  \n",
       "23               3  \n",
       "24               3  \n",
       "25               2  \n",
       "26               3  \n",
       "27               4  \n",
       "28               4  \n",
       "29               1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>langchain_answer</th>\n",
       "      <th>langchain_score</th>\n",
       "      <th>doc_agent_answer</th>\n",
       "      <th>doc_agent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can a user claim authorship of a paper on the Hugging Face Hub?\\n</td>\n",
       "      <td>By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.</td>\n",
       "      <td>A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page, clicking on their name in the corresponding Paper page, and then clicking \"claim authorship\". This will redirect them to their paper settings where they can confirm the request. The admin team will validate this request, and once confirmed, the Paper page will show as verified.</td>\n",
       "      <td>5</td>\n",
       "      <td>A user can claim authorship of a paper on the Hugging Face Hub by visiting the Paper page and clicking on their name. They will then be redirected to the paper settings where they can confirm the request. The admin team will validate the request, and once confirmed, the Paper page will show as verified.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the default context window size for Local Attention in the LongT5 model?\\n</td>\n",
       "      <td>127 tokens</td>\n",
       "      <td>The default context window size for Local Attention in the LongT5 model is defined by `config.attention_window`. It can be of type `List` to define a different window size for each layer. For more specifics on the default value, it would be best to refer to the actual implementation or documentation as it could vary depending on the specific configuration of the model.</td>\n",
       "      <td>3</td>\n",
       "      <td>The default context window size for Local Attention in the LongT5 model is 512.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\\n</td>\n",
       "      <td>IDEFICS</td>\n",
       "      <td>The name of the large multimodal model is Flamingo. It's mentioned in Document 2, which states that you can use this model to solve image-text tasks. However, it's not explicitly stated that Flamingo is based on Flamingo itself in the document. This information might be derived from other sources or documents.</td>\n",
       "      <td>3</td>\n",
       "      <td>AlignVisionModel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of the `gradio.Blocks` API?\\n</td>\n",
       "      <td>The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.</td>\n",
       "      <td>The `gradio.Blocks` API is a new, low-level interface in Gradio 3.0 that provides full control over the data flows and layout of an application. It allows for the creation of complex, multi-step applications. This is achieved by enabling developers to build their own blocks, define their behavior, and arrange them as needed.</td>\n",
       "      <td>5</td>\n",
       "      <td>The `gradio.Blocks` API is a high-level interface for creating user interfaces (UIs) for machine learning models in Python. It allows developers to create custom UIs with minimal code, and includes features such as support for different input and output types, real-time updates, and the ability to share UIs via links or embed them in other websites. The `Blocks` API is part of the Gradio library, which provides a range of tools for building and deploying machine learning applications.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\\n</td>\n",
       "      <td>Grouped convolutions</td>\n",
       "      <td>SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. This information can be found in the first document, \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\".</td>\n",
       "      <td>5</td>\n",
       "      <td>SqueezeBERT uses depthwise separable convolutions in place of fully-connected layers for the Q, K, V, and FNN layers. This reduces the number of parameters and computations required while maintaining similar performance to traditional fully-connected layers.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\\n</td>\n",
       "      <td>Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.</td>\n",
       "      <td>The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are 1) splitting the embedding matrix into two smaller matrices, and 2) using repeating layers split among groups.</td>\n",
       "      <td>5</td>\n",
       "      <td>The two parameter-reduction techniques proposed in the ALBERT model are cross-layer parameter sharing and factorized embedding parameterization. Cross-layer parameter sharing reduces the number of unique parameters by sharing them across multiple layers, while factorized embedding parameterization reduces the embedding size by decomposing it into smaller matrices.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the three main steps for fine-tuning a model with the ðŸ¤— Datasets library?\\n</td>\n",
       "      <td>1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.</td>\n",
       "      <td>The three main steps for fine-tuning a model with the ðŸ¤— Datasets library are:\\n\\n1. Loading a dataset from the Hugging Face Hub.\\n2. Preprocessing the data using `Dataset.map()`.\\n3. Loading and computing metrics.\\n\\nThis is mentioned in Chapter 3 of the course you're taking on the ðŸ¤— Datasets library.</td>\n",
       "      <td>5</td>\n",
       "      <td>The three main steps for fine-tuning a model using the ðŸ¤— (Hugging Face) Datasets library are:\\n\\n1. Preprocessing the data: This involves loading and preprocessing the dataset, which includes tokenization, padding, and batching.\\n2. Fine-tuning the model: This involves defining the training parameters, such as the learning rate, number of epochs, and batch size, and then fine-tuning the model on the preprocessed data.\\n3. Evaluating the model: This involves evaluating the performance of the fine-tuned model on a validation dataset.\\n\\nHere's an example notebook that demonstrates these steps using the ðŸ¤— Transformers and Datasets libraries to fine-tune a BERT model for text classification: [How to fine-tune BERT for text classification with ðŸ¤— Transformers and ðŸ¤— Datasets](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_bert.ipynb).</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the resolution of images used by the CLIPSeg model?\\n</td>\n",
       "      <td>352 x 352 pixels</td>\n",
       "      <td>The resolution of images used by the CLIPSeg model is 352 x 352 pixels. This information can be found in document 5, where it's mentioned that \"the model uses images of 352 x 352 pixels\".</td>\n",
       "      <td>5</td>\n",
       "      <td>The default resolution for images used in the CLIPSeg model is 256x256 pixels. However, this can be changed during initialization of the `CLIPSegModel` or `CLIPSegForImageSegmentation` class by providing a different value for the `image_resolution` argument.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the default repository type created by the `create_repo` function on Hugging Face Hub?\\n</td>\n",
       "      <td>model</td>\n",
       "      <td>The default repository type created by the `create_repo` function on Hugging Face Hub is a 'model' repository. If you want to create a 'dataset' or a 'space', you can specify it using the `repo_type` argument.</td>\n",
       "      <td>5</td>\n",
       "      <td>The default repository type created by the `create_repo` function when used on the Hugging Face Hub is a model card.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\\n</td>\n",
       "      <td>FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.</td>\n",
       "      <td>The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to shard a model's parameters, gradients, and optimizer states across multiple GPUs. This reduces memory usage, improves GPU memory efficiency, and allows for the training of much larger models on fewer GPUs. FSDP also supports CPU offloading of sharded model parameters, enabling the training of large models that exceed the available GPU memory. It is designed for distributed training of large pretrained models up to 1T parameters.</td>\n",
       "      <td>5</td>\n",
       "      <td>Fully Sharded Data Parallel (FSDP) is a technique used in distributed training to shard the model parameters, optimizer and gradient states, and even offload them to the CPU when they're inactive. This reduces the high cost of large-scale training by allowing you to train really large models on multiple GPUs or TPUs. FSDP can be a powerful tool for reducing memory usage and increasing throughput in distributed training.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What do RAG models combine to generate outputs?\\n</td>\n",
       "      <td>Pretrained dense retrieval (DPR) and sequence-to-sequence models.</td>\n",
       "      <td>RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. They retrieve documents, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks. This information is provided in Document 0 and Document 4.</td>\n",
       "      <td>5</td>\n",
       "      <td>RAG models combine two main components: a retriever and a generator. The retriever is responsible for retrieving relevant documents from a knowledge source (such as a corpus of text) based on a given input query. The generator then uses the retrieved documents, along with the input query, to generate an output sequence. This allows RAG models to generate more accurate and informative outputs by leveraging external knowledge beyond what is contained in their training data.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the title of the paper introducing the ByT5 model?\\n</td>\n",
       "      <td>ByT5: Towards a token-free future with pre-trained byte-to-byte models</td>\n",
       "      <td>The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models\". This information can be found in Document 2, 3, 4, and 5, which all refer to the same paper.</td>\n",
       "      <td>5</td>\n",
       "      <td>The title of the paper that introduces the ByT5 model is \"ByT5: Towards a Unified Text-to-Text Transformer\".</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\\n</td>\n",
       "      <td>\"manual\"</td>\n",
       "      <td>The default setting for the `allow_flagging` parameter in Gradio's `Interface` is \"manual\". This means that the flag button is not automatically displayed; it has to be added explicitly using the **Flag** button or through code.</td>\n",
       "      <td>5</td>\n",
       "      <td>The default value for the `allow_flagging` parameter in a Gradio `Interface` is `False`.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Where can the full code for the Stable Diffusion demo be found?\\n</td>\n",
       "      <td>https://hf.co/spaces/stabilityai/stable-diffusion/tree/main</td>\n",
       "      <td>The full code for the Stable Diffusion demo can be found at this link: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. This information is provided in Document 0 of your context.</td>\n",
       "      <td>5</td>\n",
       "      <td>The full code for the Stable Diffusion demo is available in this Colab notebook: &lt;https://colab.research.google.com/github/huggingface/diffusers/blob/main/notebooks/StableDiffusion_Demo.ipynb&gt;\\n\\nThis notebook includes examples of how to use the Stable Diffusion model for text-to-image generation, image editing, and more. It also covers various techniques for optimizing inference time and reducing memory usage.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What transformation does the FNet model use to replace the self-attention layer in a BERT model?\\n</td>\n",
       "      <td>Fourier transform</td>\n",
       "      <td>The FNet model uses a fourier transform to replace the self-attention layer in a BERT model, specifically returning only the real parts of the transform.</td>\n",
       "      <td>5</td>\n",
       "      <td>The FNet model uses a Fourier Transform that returns only the real parts of the transform to replace the self-attention layer in a BERT model.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How can you force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate?\\n</td>\n",
       "      <td>By passing `fp16=True` to the Accelerator init.</td>\n",
       "      <td>You can force mixed precision training when initializing the Accelerator in ðŸ¤— Accelerate by setting the `fp16` parameter to `True`. Here's an example:\\n\\n```py\\nfrom accelerate import Accelerator\\n\\naccelerator = Accelerator(fp16=True)\\n```\\n\\nThis will enable mixed precision training, which can help reduce memory usage and speed up computation during model training.</td>\n",
       "      <td>5</td>\n",
       "      <td>To force mixed precision training when initializing the Accelerator in Hugging Face's Accelerate library, you can pass `mixed_precision=\"fp16\"` as an argument to the `Accelerator` constructor. Here is an example:\\n```python\\nfrom accelerate import Accelerator\\n\\naccelerator = Accelerator(mixed_precision=\"fp16\")\\n```\\nThis will enable mixed precision training using the FP16 data type, which can help reduce memory usage and improve training speed. Note that you may need to make additional changes to your code to ensure it is compatible with mixed precision training. For more information, please refer to the [Accelerate documentation](https://huggingface.co/docs/accelerate/main/en/usage_guides/training#mixed-precision).</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the purpose of tokenizers in the NLP pipeline?\\n</td>\n",
       "      <td>To translate text into data that can be processed by the model.</td>\n",
       "      <td>The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. This is crucial because raw text needs to be converted into a format that models can understand and process.</td>\n",
       "      <td>5</td>\n",
       "      <td>Tokenizers are used to break up text into smaller units called tokens, which can be words, subwords, or even individual characters. This process is a crucial step in NLP pipelines because it allows models to work with discrete units of meaning that can be more easily processed and analyzed. By breaking text down into tokens, tokenizers help to simplify the input data for NLP models, making it easier for them to learn patterns and relationships between words and phrases.\\n\\nTokenization is also an important step in preparing text data for use in machine learning algorithms, as it allows for more efficient processing of large datasets. By breaking up text into smaller units, tokenizers can help reduce the dimensionality of the input data, making it easier to train models on large datasets without running out of memory or taking too long to process.\\n\\nOverall, tokenization is a critical step in NLP pipelines that helps to simplify and streamline the processing of text data for use in machine learning algorithms. By breaking up text into smaller units, tokenizers enable models to more easily learn patterns and relationships between words and phrases, making it possible to build powerful NLP applications that can understand and analyze human language with remarkable accuracy.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the purpose of the Safety Checker in the Diffusers library?\\n</td>\n",
       "      <td>The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.</td>\n",
       "      <td>The Safety Checker in the Diffusers library is a component that checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. It is designed to flag inappropriate or harmful content, helping users interact with generative models responsibly and ethically. The Safety Checker is a crucial part of the Diffusers library, ensuring that the generated content adheres to ethical guidelines.</td>\n",
       "      <td>5</td>\n",
       "      <td>The Safety Checker in the Diffusers library is used to ensure that the generated images are safe and appropriate for all audiences. It checks for any explicit or harmful content in the generated images and prevents them from being saved or displayed. This feature helps to maintain a safe and respectful environment for users of the library.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the purpose of Textual Inversion?\\n</td>\n",
       "      <td>Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.</td>\n",
       "      <td>The purpose of Textual Inversion, as described in the provided context, is a training method for personalizing models by learning new text embeddings from a few example images. The resulting file is very small and the new embeddings can be loaded into the text encoder.</td>\n",
       "      <td>5</td>\n",
       "      <td>Textual Inversion is a method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How do you run a Gradio Blocks app in reload mode using a Python IDE?\\n</td>\n",
       "      <td>Run `gradio run.py` in the terminal.</td>\n",
       "      <td>To run a Gradio Blocks app in reload mode using a Python IDE, you need to use the `gradio` command followed by the name of your Python script and the name of your demo. If your `run.py` file contained a demo named `my_demo`, you would launch it in reload mode like this: `gradio run.py my_demo`. This will allow Gradio to automatically reload the app whenever changes are made to the file.</td>\n",
       "      <td>4</td>\n",
       "      <td>To run a Gradio Blocks app in reload mode using a Python IDE, you can use the `gradio_client` library to connect to your local Gradio server and then use the `reload` function from the `IPython` library to automatically reload your code when it changes. Here's an example:\\n\\n1. Start your Gradio Blocks app in reload mode by running the following command in a terminal:\\n```python\\nimport gradio as gr\\n\\ndef greet(name):\\n    return \"Hello \" + name + \"!\"\\n\\nblocks = gr.Blocks()\\nblocks.greet_button, blocks.greet_textbox = blocks.buttons(\"Greet\", \"Click to greet\"), blocks.textbox(\"\")\\nblocks.greet_button.click(greet, blocks.greet_textbox)\\nblocks.launch(reload=True)  # Start the server in reload mode\\n```\\n2. In your Python IDE, run the following code to connect to your local Gradio server and automatically reload your code when it changes:\\n```python\\nimport gradio_client as grc\\nimport ipykernel\\nfrom traitlets import TraitSet\\n\\n# Connect to the local Gradio server\\napp = grc.connect('http://localhost:7865')\\n\\n# Set up reloading of code when it changes\\ncode = \"\"\"\\nimport gradio_client as grc\\n\\ndef on_reload():\\n    app.reload()\\n\\nipykernel.getfigid().on_reload(on_reload)\\n\"\"\"\\n\\n# Run the code in a separate kernel to avoid conflicts with your main kernel\\nipykernel.connect(code=code, fname='gradio_reload.py')\\n```\\nThis will automatically reload your Gradio Blocks app whenever you make changes to your code. Note that this approach only works when running the Gradio server locally on your machine. If you're hosting your Gradio app on a remote server or using a different URL, you'll need to modify the `connect` function accordingly.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is the pretraining objective of the Wav2Vec2 context network?\\n</td>\n",
       "      <td>The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.</td>\n",
       "      <td>The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones. This encourages the model to find the most similar context vector and quantized speech unit, which is the target label.</td>\n",
       "      <td>5</td>\n",
       "      <td>The pretraining objective of the Wav2Vec2 context network is contrastive predictive coding (CPC). CPC involves training a neural network to predict future audio samples based on past audio samples. Specifically, the network is trained to distinguish between true future audio samples and negative samples that are generated by randomly sampling from other parts of the input audio. This helps the network learn meaningful representations of the audio data that can be used for downstream tasks such as automatic speech recognition (ASR).</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\\n</td>\n",
       "      <td>distilbert base uncased finetuned sst2 english</td>\n",
       "      <td>The default checkpoint used by the sentiment analysis pipeline in the Transformers library is a particular pretrained model that has been fine-tuned for sentiment analysis in English. It's downloaded and cached when you create the `classifier` object (source document 0).</td>\n",
       "      <td>4</td>\n",
       "      <td>The default checkpoint for the sentiment analysis pipeline in the Transformers library is \"finiteautomata/bertweet-base-sentiment-analysis\".</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\\n</td>\n",
       "      <td>To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.</td>\n",
       "      <td>The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train/fine-tune a 1.6B-parameter GPT2-XL model for causal language modeling on Habana Gaudi.</td>\n",
       "      <td>5</td>\n",
       "      <td>The purpose of the \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" notebook is to demonstrate how to use DeepSpeed, a distributed deep learning training framework, to train large models with billions of parameters on Habana Gaudi. This notebook provides an example of how to use DeepSpeed with Hugging Face's Transformers library and Habana's SynapseAI software development kit (SDK) to fine-tune the T5-3B text summarization model on the CNN DailyMail dataset. The results show that Gaudi2 is 2.44x faster than A100 80GB for this task.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What command line module does PyTorch provide to run a script on multiple GPUs?\\n</td>\n",
       "      <td>torchrun</td>\n",
       "      <td>The command line module that PyTorch provides to run a script on multiple GPUs is `torchrun`. It allows you to specify the number of GPUs to use and the script to run. Here is an example:\\n\\n```bash\\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\\n```\\n\\nThis command will run the training script on two GPUs that live on a single machine.</td>\n",
       "      <td>5</td>\n",
       "      <td>`torch.distributed.launch`</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What is the command to upload an ESPnet model to a Hugging Face repository?\\n</td>\n",
       "      <td>./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo</td>\n",
       "      <td>The command to upload an ESPnet model to a Hugging Face repository using the `run.sh` script is:\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\nReplace `username` and `model_repo` with your desired username and repository name.</td>\n",
       "      <td>5</td>\n",
       "      <td>To upload an ESPnet model to a Hugging Face repository, you can follow these steps:\\n\\n1. First, create a new repository on the Hugging Face Hub for your model. You can do this by going to the Hugging Face Hub website and clicking on the \"New Model\" button. Give your repository a name and a short description of what it contains.\\n2. Next, clone the repository to your local machine using Git. This will allow you to make changes to the repository and push them back to the Hugging Face Hub.\\n3. Once you have cloned the repository, you can add your ESPnet model files to the repository. Make sure to include a `model.py` file that defines the model architecture, as well as any other necessary files such as tokenizer files or vocabulary files.\\n4. After adding your model files to the repository, you can commit the changes and push them back to the Hugging Face Hub using Git. This will make the model available for others to use and download from the Hugging Face Hub.\\n5. Finally, you can create a model card for your model by adding a `README.md` file to the repository. The model card should include information about the model, such as its architecture, training data, and performance metrics. You can use the [Hugging Face Model Card template](https://huggingface.co/docs/transformers/model_card) as a guide for what to include in your model card.\\n\\nOnce you have uploaded your ESPnet model and created a model card, others will be able to find and use your model on the Hugging Face Hub.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\\n</td>\n",
       "      <td>3-5 images</td>\n",
       "      <td>According to Document 4, you need 3-5 images to teach new concepts to Stable Diffusion using Textual Inversion.</td>\n",
       "      <td>5</td>\n",
       "      <td>The number of images required to teach new concepts to Stable Diffusion through the process of Textual Inversion can vary depending on the complexity and specificity of the concept. Generally, a few dozen to a few hundred images are used in the training process for each concept. However, this is not a strict rule and the number can be adjusted based on the desired results. It's important to note that more images do not always lead to better results, as the quality of the images and their relevance to the concept also play crucial roles in the learning process.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What parameter is used to ensure that elements in a row have the same height in Gradio?\\n</td>\n",
       "      <td>equal_height</td>\n",
       "      <td>The `equal_height` parameter is used to ensure that elements in a row have the same height in Gradio. This parameter should be passed to the `.style()` method of `gr.Row()`. This information was updated in PR 3125 by freddyaboulton.</td>\n",
       "      <td>4</td>\n",
       "      <td>The `elem_id` parameter can be used in Gradio to set the same height for all elements in a row. This parameter allows you to specify the ID of an HTML element, and any other elements with the same ID will have their height adjusted to match. For example:\\n```\\ngr.Interface(my_function, inputs=\"text\", outputs=\"text\").launch(elem_id=\"same-height\")\\n```\\nThis will set the height of all text input elements in the interface to be the same. Note that this parameter is only available in Gradio version 3.0 and later.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What is the command to install the latest version of Optimum with OpenVINO support?\\n</td>\n",
       "      <td>pip install --upgrade-strategy eager optimum[\"openvino\"]</td>\n",
       "      <td>The command to install the latest version of Optimum with OpenVINO support is:\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\nThis command will ensure that `optimum-intel` is using the latest version.</td>\n",
       "      <td>5</td>\n",
       "      <td>To install the most recent version of Optimum that includes OpenVINO support, you can use the following pip command:\\n```\\npip install optimum[openvino]\\n```\\nThis will install the `optimum` package along with the necessary dependencies for OpenVINO support.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the goal of the Named Entity Recognition task in token classification?\\n</td>\n",
       "      <td>The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.</td>\n",
       "      <td>The goal of the Named Entity Recognition (NER) task in token classification is to find and classify entities in a piece of text, such as persons, locations, or organizations. Each token (word or subword) in the text is assigned a label from a predefined set of classes indicating the type of entity it belongs to. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.</td>\n",
       "      <td>5</td>\n",
       "      <td>The goal of the Named Entity Recognition (NER) task in token classification is to identify and categorize named entities present in text into predefined classes such as person names, organization names, location names, medical codes, time expressions, quantities, monetary values, percentages, etc. This information can be used for various natural language processing applications like question answering, machine translation, and information extraction.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\\n</td>\n",
       "      <td>Optimum</td>\n",
       "      <td>The name of the open-source library created by Hugging Face to simplify Transformer acceleration is [ðŸ¤— Accelerate](https://github.com/huggingface/accelerate). This information can be found in Document 2.</td>\n",
       "      <td>3</td>\n",
       "      <td>The name of the open-source library created by Hugging Face for Transformer acceleration is `Transformers.js`.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "ddfe8937f08d290a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Scoring evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9a50771a6f6daa9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-04-17T11:43:46.273765Z",
     "start_time": "2024-04-17T11:43:46.239769Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scoring_output(eval_result: pd.DataFrame, evaluator_name: str):\n",
    "    score_field = f\"eval_score_{evaluator_name}\"\n",
    "    result = eval_result.loc[:, [score_field, \"settings\"]].copy()\n",
    "    \n",
    "    result[score_field] = result[score_field].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "    \n",
    "    result[score_field] = (result[score_field] - 1) / 4    \n",
    "    average_scores = result.groupby(\"settings\")[score_field].mean()\n",
    "\n",
    "    average_scores.sort_values()\n",
    "    return average_scores\n",
    "\n",
    "scores = scoring_output(EVAL_RESULTS, EVALUATOR_NAME)\n",
    "display(scores)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/langchain_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json        0.824627\n",
       "./output/rag_doc_agent_chunk:200_rerank:False_reader-model:mixtral:instruct_embedding-model:all-minilm.json    0.712687\n",
       "Name: eval_score_mixtral:instruct, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8f2dcec8098f6f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
