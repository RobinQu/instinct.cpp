{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install pre-requisites",
   "id": "3a65285e868d7809"
  },
  {
   "cell_type": "code",
   "id": "3c7ba6dbea18e2ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets ragatouille ratelimit retry duckdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d2c4b6af65d58cdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Model preparations\n",
    "\n",
    "To go through the evaluation process, we need following models:\n",
    "\n",
    "1. Document model: Embedding model to generate document embeddings which will persisted in vector index. \n",
    "2. Reader model: A text completion model to answer the final question with augmented context.\n",
    "3. Evaluator model: A chat completion model that will give final verdict about RAG output. As this model will affect scoring considerably, stronger model should be used. \n",
    "\n",
    "As the choice of different models is not subject of this article and won't impact the comparison between RAG frameworks, we are determined to use completed local solution for this experiment for better speed and lower cost. \n",
    "\n",
    "To be more precise, following models that are already optimized in Ollama are used:\n",
    "\n",
    "* [all-minilm](https://huggingface.co/google/gemma-2b) for `Embedding model`;\n",
    "* [Mixtral-8x7B](https://ollama.com/library/mixtral) for `Evaluator model` and `Reader model`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "id": "b4dfdc109f0daffc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama, MiniMaxChat\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "import os\n",
    "\n",
    "# points to a vLLM server\n",
    "MIXTRAL_ENDPOINT = \"http://192.168.0.134:30253\"\n",
    "\n",
    "# points to a ollama server\n",
    "MINILM_ENDPOINT = \"http://192.168.0.29:11434\"\n",
    "\n",
    "READER_MODEL_NAME = \"mixtral:instruct\"\n",
    "EMBEDDING_NAME = \"all-minilm\"\n",
    "EVALUATOR_NAME = \"mixtral:instruct\"\n",
    "\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=EMBEDDING_NAME, base_url = MINILM_ENDPOINT)\n",
    "READER_LLM = Ollama(model=READER_MODEL_NAME, base_url = MIXTRAL_ENDPOINT)\n",
    "EVAL_MODEL = ChatOllama(model=EVALUATOR_NAME, base_url = MIXTRAL_ENDPOINT)\n",
    "\n",
    "LANGCHAIN_DATA_ROOT = \"./data/langchain\"\n",
    "INSTINCT_DOC_AGENT_DATA_ROOT = \"./data/doc_agent\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8088b64de81ed222",
   "metadata": {},
   "source": [
    "# Test all these models\n",
    "\n",
    "EMBEDDING_MODEL.embed_query(\"hello\")\n",
    "\n",
    "READER_LLM.invoke(\"hello\")\n",
    "\n",
    "EVAL_MODEL.invoke(\"hello\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bdb8cc674412799",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Build RAG pipeline using `langchain` \n",
    "\n",
    "1. transform training data in `m-ric/huggingface_doc` to `langchain`'s document objects\n",
    "2. Load into faiss index if index file is absent\n",
    "3. prompt with eval data `m-ric/huggingface_doc` using `READER_MODEL` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881763874245235f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge base preparations"
   ]
  },
  {
   "cell_type": "code",
   "id": "cdd427a5684551d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a855d46ab06e6ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e60ad11cd96e14c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "921990aa1cb355f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "EVAL_DATASET = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11ea5ea55c0da346",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\"))\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40aa9cb71de4a46a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument]\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=\"gpt-4\",\n",
    "        chunk_size=chunk_size,\n",
    "        # chunk_overlap=int(chunk_size / 10),\n",
    "        chunk_overlap=0,\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        disallowed_special=[],\n",
    "        allowed_special=\"all\"\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13dc9ec746ddadd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model: Embeddings,\n",
    "    embedding_model_name: str\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model: the embedding\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "         \n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name}\"\n",
    "    index_folder_path = os.path.join(LANGCHAIN_DATA_ROOT, index_name)\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs\n",
    "        )\n",
    "        print(f\"Index not found, generating it... {len(docs_processed)} docs in total\")\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74338ffcd9971f99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## QA chain"
   ]
  },
  {
   "cell_type": "code",
   "id": "851eb803b3f0a34f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "283a72f828eb8ec3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "    \n",
    "    print(\"final prompt size:\", len(final_prompt))\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eeda09db98f634b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generating answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110434a4f3c3b6d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test function with langchain"
   ]
  },
  {
   "cell_type": "code",
   "id": "913cbbdae2722e2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from langchain_core.language_models import BaseChatModel \n",
    "\n",
    "def run_langchain_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        \n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, knowledge_index, reranker=reranker)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78de170f745561d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def run_langchain_test_all() -> str:\n",
    "    \"\"\"\n",
    "    Build index and run langchain test with fixed parameter and model selections\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"langchain_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = os.path.join(\"./output\", f\"{settings_name}.json\")\n",
    "    \n",
    "\n",
    "    print(\"Loading knowledge base embeddings...\")\n",
    "    knowledge_index = load_embeddings(\n",
    "        RAW_KNOWLEDGE_BASE,\n",
    "        chunk_size=chunk_size,\n",
    "        embedding_model=EMBEDDING_MODEL,\n",
    "        embedding_model_name=EMBEDDING_NAME\n",
    "    )\n",
    "    \n",
    "    print(f\"Running RAG with {settings_name}\")\n",
    "    reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "    run_langchain_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        llm=READER_LLM,\n",
    "        knowledge_index=knowledge_index,\n",
    "        output_file=output_file_name,\n",
    "        reranker=reranker,\n",
    "        verbose=True,\n",
    "        test_settings=settings_name,\n",
    "    )\n",
    "    \n",
    "    return output_file_name "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96d9c29a10db3e84",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# execute test for langchain\n",
    "LANGCHAIN_TEST_OUTPUT = run_langchain_test_all()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4580fc87b583de8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test function with doc-agent in instinct.cpp\n",
    "\n",
    "You have to manually start `doc-agent` locally.\n",
    "\n",
    "To build knowledge index with same knowledge base data from HF:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=./data/instinct/index.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  build \\\n",
    "  --force \\\n",
    "  --file=https://huggingface.co/api/datasets/m-ric/huggingface_doc/parquet/default/train/0.parquet \\\n",
    "  --type=PARQUET \\\n",
    "  --parquet_mapping=0:txt,1:metadata:source:varchar\n",
    "```\n",
    "\n",
    "To start http server for query:\n",
    "\n",
    "```shell\n",
    "$DOC_AGENT_BIN --verbose \\\n",
    "  --parent_child_retriever \\\n",
    "  --child_chunk_size=200 \\\n",
    "  --chat_model_model_name=gemma:2b \\\n",
    "  --embedding_model_model_name=all-minilm:latest \\\n",
    "  --db_path=/tmp/rag_eval.db \\\n",
    "  --vector_table_dimension=384 \\\n",
    "  serve \\\n",
    "  --port=9090 \n",
    "```\n",
    "\n",
    "Next, we will begin QA tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b91ee34890b32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "4afb09c4ab37a98f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def answer_with_doc_agent(question: str):\n",
    "    import requests\n",
    "    res = requests.post(\"http://localhost:9090/v1/chat/completions\", json={\"messages\": [{\"content\": question, \"role\": \"human\"}], \"stream\": False})\n",
    "    assert res.status_code == 200\n",
    "    body = res.json()\n",
    "    return body[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "\n",
    "def run_doc_agent_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = answer_with_doc_agent(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cceb41a4f1cbe75",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def run_doc_agent_test_all():\n",
    "    if not os.path.exists(\"./output\"):\n",
    "        os.mkdir(\"./output\")\n",
    "    \n",
    "    chunk_size = 200\n",
    "    rerank = False\n",
    "    \n",
    "    settings_name = f\"doc_agent_chunk:{chunk_size}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}_embedding-model:{EMBEDDING_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "    \n",
    "    print(f\"Running RAG with settings {settings_name}\")\n",
    "    run_doc_agent_rag_tests(\n",
    "        eval_dataset=EVAL_DATASET,\n",
    "        output_file=output_file_name,\n",
    "        test_settings=settings_name\n",
    "    )\n",
    "    \n",
    "    return output_file_name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66a1467f5bfb50f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "DOC_AGENT_TEST_OUTPUT = run_doc_agent_test_all()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fdc11c2c8edc69d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Evaluation Runner"
   ]
  },
  {
   "cell_type": "code",
   "id": "31856cff24422023",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from ratelimit import limits,sleep_and_retry\n",
    "from retry import retry\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=6, period=60)\n",
    "def throttled_invoke(eval_chat_model, eval_prompt):\n",
    "    return eval_chat_model.invoke(eval_prompt)\n",
    "\n",
    "\n",
    "\n",
    "@retry(exceptions=Exception, tries=6)\n",
    "def evaluate_single_answer(\n",
    "        evaluation_prompt_template: ChatPromptTemplate,\n",
    "        experiment: dict,\n",
    "        throttled:bool,\n",
    "        eval_chat_model: BaseChatModel\n",
    "):\n",
    "    eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "    if throttled:\n",
    "        eval_result = throttled_invoke(eval_chat_model, eval_prompt)\n",
    "    else:\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "    splits = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "    if len(splits) != 2:\n",
    "        print(splits)\n",
    "        raise Exception(\"Evaluation did not complete successfully\")\n",
    "    assert 1 <= int(splits[1]) <= 5\n",
    "    return splits\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    "    throttled:bool = True\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment and experiment[f\"eval_score_{evaluator_name}\"]:\n",
    "            continue\n",
    "        \n",
    "        splits = evaluate_single_answer(evaluation_prompt_template, experiment, throttled, eval_chat_model)\n",
    "        \n",
    "        if len(splits) != 2:\n",
    "            print(splits)\n",
    "            # experiment[f\"eval_score_{evaluator_name}\"] = \"\"\n",
    "            # experiment[f\"eval_feedback_{evaluator_name}\"] = \"\"\n",
    "            continue\n",
    "        feedback, score = splits \n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c193a75198dabe56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "EVALUATION_PROMPT = \"\"\" You are a fair evaluator language model.\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "292b22f2978e2d2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run evaluations"
   ]
  },
  {
   "cell_type": "code",
   "id": "83e219af8e9a45c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def generate_eval_results():\n",
    "    import glob\n",
    "    for output_file_name in glob.glob(\"./output/*.json\"):\n",
    "        print(f\"Evaluating {output_file_name}\")\n",
    "        evaluate_answers(\n",
    "            output_file_name,\n",
    "            EVAL_MODEL,\n",
    "            EVALUATOR_NAME,\n",
    "            EVALUATION_PROMPT_TEMPLATE,\n",
    "            # throttling is not needed for local model\n",
    "            False\n",
    "        )\n",
    "\n",
    "generate_eval_results()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b157cef7583830b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_eval_results():\n",
    "    import glob\n",
    "    outputs = []\n",
    "    for file in glob.glob(\"./output/*.json\"):\n",
    "        output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "        output[\"settings\"] = file\n",
    "        outputs.append(output)\n",
    "    return pd.concat(outputs)\n",
    "\n",
    "EVAL_RESULTS = load_eval_results()\n",
    "display(EVAL_RESULTS)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "609601e4b9513078",
   "metadata": {},
   "source": [
    "# Get diffs\n",
    "import duckdb\n",
    "DIFF_SQL = \"SELECT tbl1.question, tbl1.true_answer, tbl1.generated_answer as langchain_answer, tbl1.score as langchain_score, tbl2.generated_answer as doc_agent_answer, tbl2.score as doc_agent_score \"\\\n",
    "           \"FROM \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'langchain%') AS tbl1 \"\\\n",
    "           \"JOIN \"\\\n",
    "           f\"(SELECT *, \\\"eval_score_{EVALUATOR_NAME}\\\" as score FROM EVAL_RESULTS where test_settings like 'doc_agent%') AS tbl2 \"\\\n",
    "           \"ON tbl1.question = tbl2.question \" \\\n",
    "           f\"WHERE tbl1.score > tbl2.score\"\n",
    "\n",
    "DIFFS = duckdb.query(DIFF_SQL).to_df()\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(DIFFS)\n",
    "\n",
    "DIFFS.to_excel(\"./output/diffs.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ddfe8937f08d290a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Scoring evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9a50771a6f6daa9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scoring_output(eval_result: pd.DataFrame, evaluator_name: str):\n",
    "    score_field = f\"eval_score_{evaluator_name}\"\n",
    "    result = eval_result.loc[:, [score_field, \"settings\"]].copy()\n",
    "    \n",
    "    result[score_field] = result[score_field].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "    \n",
    "    result[score_field] = (result[score_field] - 1) / 4    \n",
    "    average_scores = result.groupby(\"settings\")[score_field].mean()\n",
    "\n",
    "    average_scores.sort_values()\n",
    "    return average_scores\n",
    "\n",
    "scores = scoring_output(EVAL_RESULTS, EVALUATOR_NAME)\n",
    "display(scores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c8f2dcec8098f6f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
